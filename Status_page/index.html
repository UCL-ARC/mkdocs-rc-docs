
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Documentation for the centrally provided Research Computing HPC services at UCL.">
      
      
        <meta name="author" content="UCL RCAS Team">
      
      
        <link rel="canonical" href="https://www.rc.ucl.ac.uk/docs/Status_page/">
      
      
        <link rel="prev" href="../Remote_Access/">
      
      
        <link rel="next" href="../Terms_and_Conditions/">
      
      
      <link rel="icon" href="../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.7">
    
    
      
        <title>Status of machines - UCL Research Computing Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.8608ea7d.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
    
    
      <link rel="stylesheet" href="../stylesheets/tweaks.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#status-of-machines" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="UCL Research Computing Documentation" class="md-header__button md-logo" aria-label="UCL Research Computing Documentation" data-md-component="logo">
      
  <img src="../img/portico.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            UCL Research Computing Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Status of machines
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="deep-purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="deep-purple" data-md-color-accent="deep-purple"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/UCL-ARC/mkdocs-rc-docs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    UCL-ARC/mkdocs-rc-docs
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="UCL Research Computing Documentation" class="md-nav__button md-logo" aria-label="UCL Research Computing Documentation" data-md-component="logo">
      
  <img src="../img/portico.svg" alt="logo">

    </a>
    UCL Research Computing Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/UCL-ARC/mkdocs-rc-docs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    UCL-ARC/mkdocs-rc-docs
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Research Computing Services
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Account_Services/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Account Services
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Additional_Resource_Requests/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Additional Resource Requests
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Contact_Us/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contact and Support
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Data_Management/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Management
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Example_Jobscripts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Example Jobscripts
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Experienced_Users/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Quick Start Guide for Experienced HPC Users
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Interactive_Jobs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Interactive Job Sessions
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Job_Results/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Where do my results go?
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../New_Users/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Guide for New Users
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Planned_Outages/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Planned Outages
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Remote_Access/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Remote Access
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Status of machines
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Status of machines
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#myriad" class="md-nav__link">
    <span class="md-ellipsis">
      Myriad
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Myriad">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#check-lists-of-damaged-files-on-myriad" class="md-nav__link">
    <span class="md-ellipsis">
      Check lists of damaged files on Myriad
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#action-required-compression-or-removal-of-data" class="md-nav__link">
    <span class="md-ellipsis">
      Action required - compression or removal of data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latest-on-myriad" class="md-nav__link">
    <span class="md-ellipsis">
      Latest on Myriad
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kathleen" class="md-nav__link">
    <span class="md-ellipsis">
      Kathleen
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Kathleen">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#latest-on-kathleen" class="md-nav__link">
    <span class="md-ellipsis">
      Latest on Kathleen
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#young" class="md-nav__link">
    <span class="md-ellipsis">
      Young
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Young">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#young-new-filesystem" class="md-nav__link">
    <span class="md-ellipsis">
      Young new filesystem
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latest-on-young" class="md-nav__link">
    <span class="md-ellipsis">
      Latest on Young
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#michael" class="md-nav__link">
    <span class="md-ellipsis">
      Michael
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Michael">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#michael-new-filesystem" class="md-nav__link">
    <span class="md-ellipsis">
      Michael new filesystem
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thomas" class="md-nav__link">
    <span class="md-ellipsis">
      Thomas
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../Terms_and_Conditions/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Terms and Conditions
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../UCL_Service_For_Me/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    UCL Service For Me
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../howto/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How To
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_17" >
        
          
          <label class="md-nav__link" for="__nav_17" id="__nav_17_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Background
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_17">
            <span class="md-nav__icon md-icon"></span>
            Background
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Background/Cluster_Computing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Cluster Computing
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Background/Data_Storage/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data Storage
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Background/Parallel_Filesystems/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Parallel Filesystems
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_18" >
        
          
          <label class="md-nav__link" for="__nav_18" id="__nav_18_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Clusters
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_18_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_18">
            <span class="md-nav__icon md-icon"></span>
            Clusters
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Clusters/Myriad/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Myriad
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Clusters/Kathleen/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Kathleen
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Clusters/Michael/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MMM Michael
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Clusters/Young/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MMM Hub Young
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Clusters/Acknowledging_RC_Systems/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Acknowledging the Use of RC Systems
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_19" >
        
          
          <label class="md-nav__link" for="__nav_19" id="__nav_19_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Installed Software Lists
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_19_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_19">
            <span class="md-nav__icon md-icon"></span>
            Installed Software Lists
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Installed_Software_Lists/module-packages/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    General Software Lists
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Installed_Software_Lists/python-packages/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Python Packages
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Installed_Software_Lists/r-packages/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    R Packages
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_20" >
        
          
          <label class="md-nav__link" for="__nav_20" id="__nav_20_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Other Services
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_20_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_20">
            <span class="md-nav__icon md-icon"></span>
            Other Services
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Other_Services/Aristotle/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Aristotle
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Other_Services/JupyterHub/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    JupyterHub
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Other_Services/RStudio/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    RStudio
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Other_Services/UCL_UK_e-Science_Certificates/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    UCL UK e-Science Certificates
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_21" >
        
          
          <label class="md-nav__link" for="__nav_21" id="__nav_21_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Paid-For Resources
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_21_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_21">
            <span class="md-nav__icon md-icon"></span>
            Paid-For Resources
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Paid-For_Resources/How_to_Use/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    How to Use
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Paid-For_Resources/Purchasing_in_Myriad/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Purchasing in Myriad
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_22" >
        
          
          <label class="md-nav__link" for="__nav_22" id="__nav_22_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Software Guides
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_22_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_22">
            <span class="md-nav__icon md-icon"></span>
            Software Guides
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Software_Guides/ANSYS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ANSYS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Software_Guides/Matlab/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    MATLAB
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Software_Guides/Singularity/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Singularity/Apptainer
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Software_Guides/R/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    R
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Software_Guides/Other_Software/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Other Software
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Software_Guides/Installing_Software/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Installing Software
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_23" >
        
          
          <label class="md-nav__link" for="__nav_23" id="__nav_23_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Walkthroughs
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_23_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_23">
            <span class="md-nav__icon md-icon"></span>
            Walkthroughs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Walkthroughs/Giving_Files/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Giving Files to Another User
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Walkthroughs/Logging_In/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Logging In
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Walkthroughs/Machine_Learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Machine Learning
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#myriad" class="md-nav__link">
    <span class="md-ellipsis">
      Myriad
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Myriad">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#check-lists-of-damaged-files-on-myriad" class="md-nav__link">
    <span class="md-ellipsis">
      Check lists of damaged files on Myriad
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#action-required-compression-or-removal-of-data" class="md-nav__link">
    <span class="md-ellipsis">
      Action required - compression or removal of data
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latest-on-myriad" class="md-nav__link">
    <span class="md-ellipsis">
      Latest on Myriad
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kathleen" class="md-nav__link">
    <span class="md-ellipsis">
      Kathleen
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Kathleen">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#latest-on-kathleen" class="md-nav__link">
    <span class="md-ellipsis">
      Latest on Kathleen
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#young" class="md-nav__link">
    <span class="md-ellipsis">
      Young
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Young">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#young-new-filesystem" class="md-nav__link">
    <span class="md-ellipsis">
      Young new filesystem
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#latest-on-young" class="md-nav__link">
    <span class="md-ellipsis">
      Latest on Young
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#michael" class="md-nav__link">
    <span class="md-ellipsis">
      Michael
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Michael">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#michael-new-filesystem" class="md-nav__link">
    <span class="md-ellipsis">
      Michael new filesystem
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thomas" class="md-nav__link">
    <span class="md-ellipsis">
      Thomas
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="status-of-machines">Status of machines<a class="headerlink" href="#status-of-machines" title="Permanent link">§</a></h1>
<p>This page outlines that status of each of the machines managed by the Research Computing team at UCL. We endeavour to keep this page as up-to-date as possible but there might be some delay. Also there are spontaneous errors that we have to deal with (i.e. high load on login nodes) but feel free to report them to rc-support@ucl.ac.uk. Finally, details of our planned outages can be found <a href="https://www.rc.ucl.ac.uk/docs/Planned_Outages/">here</a>.  </p>
<h3 id="myriad">Myriad<a class="headerlink" href="#myriad" title="Permanent link">§</a></h3>
<ul>
<li>
<p>2023-03-06 - Myriad's filesystem is getting full again, which will impact performance. If you are 
    able, please consider backing up and deleting any files that you aren't actively using for your 
    research for the time being.</p>
<p>You can check your quota and see how much space you are using on Myriad with the <code>lquota</code> command, 
and you can see which directories are taking the most space using the <code>du</code> command, which can also 
be run in specific directories. You can tell <code>du</code> to only output details of the first level of 
directory sizes with the <code>--max-depth=1</code> option.</p>
<p>Jobs are still running for the moment, but if the filesystem keeps getting fuller we may need to 
stop new jobs running until usage is brought down again.</p>
</li>
<li>
<p>2023-07-31 12:30 - Myriad's filesystem is currently being very slow because we have two failed 
    drives in the Object Store Target storage. One drive has fully failed and the volume is under 
    reconstruction. The other has been detected as failing soon and is being copied to a spare, but
    this is happening slowly, likely because the disk is failing.</p>
<p>They are in separate volumes so there isn't a risk there, but it can take some time for 
reconstruction to complete and this will make the filesystem sluggish. This will affect you 
accessing files on the login nodes, and also your jobs reading or writing to files on Scratch 
or home, and accessing our centrally-installed software. (It won't affect writing to <code>$TMPDIR</code>,
the local hard disk on the compute node).</p>
<p>Once the reconstruction is complete, performance should return to normal. This could take most 
of the day and potentially continue into tomorrow.</p>
</li>
<li>
<p>2023-08-01 11:00 - We have had a report of another impending disk failure in the same volume as
    the first failed disk, which puts data at risk. The volume reconstruction is still in progress 
    and expected to take another 30-odd hours. (The second copy to spare has completed).</p>
<p>We have stopped Myriad from running jobs to reduce load on the filesystem while the 
reconstruction completes, so no new jobs will start for the time being. We'll keep you updated 
if anything changes.</p>
</li>
<li>
<p>2023-08-03 10:30 - We have another estimated 18 hours of volume reconstruction to go, so we are
    leaving jobs off today. On Friday morning we will check on the status of everything and 
    hopefully be able to re-enable jobs then if all is well.</p>
</li>
<li>
<p>2023-08-04 10:00 - The reconstruction is complete and we have re-enabled jobs on Myriad.</p>
</li>
<li>
<p>2023-11-23 14:00 - We need to stop new jobs running on Myriad while its filesystem recovers.</p>
<p>This means that jobs that are already running will keep running, but new jobs will not start 
until we enable them again. The filesystem is likely to be slower than usual. You can still log 
in and access your files.</p>
<p>This is because we've just had two disks fail in the same area of Myriad's filesystem and the 
draining of jobs is to reduce usage of the filesystem while it rebuilds onto spare disks.</p>
<p>The current estimated time for the disk rebuild is 35 hours (but these estimates can be 
inaccurate).</p>
<p>We will update you when new jobs can start running again.</p>
</li>
<li>
<p>2023-11-24 09:45 - <strong>Myriad filesystem is down.</strong> </p>
<p>I'm afraid Myriad's filesystem is currently down. You will not be able to log in and jobs are 
stopped.</p>
<p>A third disk failed in the same area as the last two and we started getting errors that some 
areas were unreadable and reconstruction failed. We need to get this area reconstructed so are 
in contact with our vendors.</p>
<p><strong>Detail</strong></p>
<p>Our parallel filesystem is Lustre. It has Object Storage Targets that run on the Object 
Storage Servers. Data gets striped across these so one file will have pieces in multiple 
different locations, and there is a metadata server that keeps track of what is where. One of 
the Object Storage Targets has lost three disks, is unreadable and is what needs to be 
reconstructed. While it is unavailable, the filesystem does not work. We will also have some 
amount of damaged files which have pieces that are unreadable.</p>
<p>If the reconstruction succeeds, we will then need to clear out the unreadable sectors. Then we 
will be able to start checking how much damage there is to the filesystem.</p>
<p>This is all going to take some time. We will update you again on Monday by midday, but there 
may be no new information by then.</p>
<p>I'm sorry about this, we don't know right now how long an interruption this will cause to 
service, or how much data we may have lost.</p>
<p>Please send any queries to rc-support@ucl.ac.uk </p>
</li>
<li>
<p>2023-11-27 12:10 - There is not much of an update on Myriad's filesystem yet - we've been in 
    contact with our vendors, have done some things they asked us to (reseating hardware) and are 
    waiting for their next update.</p>
<p>In the meantime, we are starting to copy the backup of home that we have from the backup system
where it is very slow to access to somewhere faster to access, should we need to recover files 
from it on to Myriad.</p>
<p>We received a question about backups - the last successful backup for Myriad home directories 
ran from Nov 20 23:46 until Nov 22 03:48. Data before that interval should be backed up. Data 
after that is not. Data created or changed during it may be backed up, it depends on which 
users it was doing when.</p>
<p>(Ideally we would be backing up every night, but as you can see the backup takes longer than a 
day to complete, and then the next one begins. The next backup had started on Nov 22 at 08:00 
but did not complete before the disk failures).</p>
</li>
<li>
<p>2023-11-29 11:30 Today's Myriad filesystem update:</p>
<ul>
<li>
<p>We are continuing to meet with our vendor and have sent them more data to analyse.</p>
</li>
<li>
<p>We are going to copy over the failed section of filesystem to a new area, with some data 
  loss, so we end up with a working volume that we can use to bring the whole filesystem back 
  up.</p>
</li>
<li>
<p>The copy of data from our backup location to easier access location is ongoing (we have 
  48TiB of home directories consisting of 219M items, currently copied ~~15TiB~~ 7.5TiB and 27M 
  items).</p>
</li>
</ul>
<p>Date of next update: Monday 4 Dec by midday.</p>
<p>Myriad will definitely not be back up this week, and likely to not be back up next week - but 
we'll update you on that. The data copying all takes a lot of time, and once it is there, we 
need to run filesystem checks on it which also take time.</p>
</li>
<li>
<p>2023-12-04 12:30 - The data copying is continuing.</p>
<ul>
<li>
<p>The copy of the failed section of filesystem to a new area is at 33% done, expected to take 
  4.5 days more.</p>
</li>
<li>
<p>The copy of home directories from our backup location to easier access location is at 
  ~~44TiB~~ 22TiB (GPFS was reporting twice the real disk usage) 
  and 86M items out of 48TiB and 219M. It is hard to predict how long that may take, as the 
  number of items is the bottleneck. It has been going through usernames in alphabetical order 
  and has reached those starting "ucbe".</p>
</li>
</ul>
<p>If the first of these copies completes successfully, we will then need to go through a 
filesystem check. This will also take possibly a week or more - it is difficult to predict 
right now. If this recovery continues as it has been and completes successfully, it looks like 
we may have lost a relatively small proportion of data - but we cannot guarantee that at this 
stage, and the copy has not completed.</p>
<p>So at present we expect to be down at least all of this week, and all of next week - and 
possibly into the week of the 18th before UCL's Christmas closing on the 22nd.</p>
<p>I'm sorry about this - with these amounts of data and numbers of files, it gets difficult to 
predict how long any of these operations will take.</p>
<p>We've had a couple of questions about whether there is any way people can get their data from 
Myriad - until the filesystem is reconstructed and brought back up, we cannot see separate 
files or directories on Myriad.</p>
</li>
<li>
<p>2023-12-12 15:00 - First data copy complete.</p>
<ul>
<li>
<p>The copy of the failed section of filesystem has completed and is showing at 99.92% rescued, 
  leaving around 75GB of data as not recoverable. (At present we don't know what this data is -
  it could be from all of home, Scratch, shared spaces and our application install area).</p>
</li>
<li>
<p>The copy of home directories from our backup location to easier access location is still 
  going, currently at 42.5TiB and 166M items out of 48TiB and 219M. My earlier reports had 
  doubled the amount we had successfully copied, so when I said 44TiB previously, that was only
  22TiB. (Corrected in this page). This is progressing slowly and has stalled a couple of times.</p>
</li>
</ul>
<p>We next need to bring the filesystem up so we can start running checks and see what files are 
damaged and missing. We will be discussing our next steps to do this with our vendor. </p>
<p>I intend to send another update this week, before the end of Friday. </p>
</li>
<li>
<p>2023-12-15 17:20 - No news, Christmas closing.</p>
<p>No news, I'm afraid. It is looking like Myriad's filesystem will definitely remain down over 
Christmas.</p>
<ul>
<li>
<p>Our vendors are investigating filesystem logs.</p>
</li>
<li>
<p>The copy of home directories from our backup location to easier access location appears to 
  have finished the copying stage but was still doing some cleanup earlier today.</p>
</li>
</ul>
<p>UCL is closed for Christmas from the afternoon of Friday 22 December until 9am on Tuesday 2 
January. Any tickets received during this time will be looked at on our return.</p>
<p>We will send an update next week before UCL closes.</p>
</li>
<li>
<p>2023-12-18 15:30 - Home directories from backup available</p>
<p>We have restored Myriad HOME directories only (no Scratch, Projects or
Apps) from the most recent back up which ran from:</p>
<p>Monday Nov 20 23:46 to Wednesday Nov 22 03:48</p>
<p>They are mounted READ ONLY on the Myriad login nodes so you can login
to check what files are missing or need updating and scp results etc
back to your local computer. We apologize for the delay in making this
data available, unfortunately the restore process was only finished during
the weekend.</p>
<p>Work on restoring more data (i.e. HOME from after the backup, as well
as Scratch and Projects) is still in progress.</p>
<p>It is currently not possible to run jobs.</p>
<p>We still don't expect Myriad to be restored to service before the
Christmas and New Year UCL closure.</p>
<p>UCL is closed for Christmas from the afternoon of Friday 22 December until 9am on Tuesday 2 
January. Any tickets received during this time will be looked at on our return.</p>
</li>
<li>
<p>2023-12-22 15:00 - A final Myriad update before UCL closes for the Christmas and New Year break.</p>
<p>The copy of rescued data back onto the re-initialised volume completed this morning (Friday 
22nd). We are now running filesystem checks. Myriad will remain down during the Christmas and 
New Year closure apart from the read only HOME directories as detailed previously.</p>
</li>
<li>
<p>2024-01-05 16:50 - An update on the status of Myriad before the weekend</p>
<p>We wanted to give you a quick update on the progress with Myriad before the weekend as we 
know several of you are asking for one.</p>
<p>We are meeting on Monday morning to consider options for returning the live filestore 
including Apps, HOME, Scratch and projects to service. We should have some rough timescale we 
can give you later on Monday.</p>
<p>Currently a scan is running to discover which files existed either wholly or in part on the 
failed volume. So far this has discovered around 60M files, and the scan is about halfway. This 
will carry on running over the weekend. Unfortunately, there is likely to be significant data 
loss from your Scratch directories.</p>
<p>We will send another update later on Monday.</p>
</li>
<li>
<p>2024-01-08 17:50 - Myriad update (Monday)</p>
<p>We met this morning to discuss options for returning the live filestore including Apps, HOME, 
Scratch and projects to service. Tentatively we hope to be able to allow you access to your 
HOME, Scratch and projects by the end of this week.</p>
<p>The scan to discover which files existed either wholly or in part on the failed volume has 
completed and found about 70M files which is around 9.1% of the files on Myriad. We are 
planning to put files in your HOME directory:</p>
<p>One listing the missing files from your HOME directory.</p>
<p>One listing the missing files from your Scratch directory.</p>
<p>There are also missing files in projects so if you own a project we will give you a list of 
these too.</p>
</li>
<li>
<p>2024-01-11 12:30 - Jobs on Myriad</p>
<p>We've had questions from you about when jobs will be able to restart. We were able to assess 
the damage to our software stack yesterday and most of the centrally installed applications 
are affected by missing files and need to be copied over from other clusters or reinstalled.</p>
<p>We're going to begin by copying over what we can from other systems. We'll be looking at the 
results of this first step and seeing how much is still missing after it and how fundamental 
those packages are. </p>
<p>It would be possible for us to enable jobs before the whole stack was reinstalled, but we need 
enough there for you to be able to carry out useful work. We should have a much better idea of 
what is still missing by Monday and our plans for reinstating it. I would rather lean towards 
giving you access sooner with only the most commonly-used software available rather than 
waiting for longer.</p>
<p>We're on schedule for giving you access to your files by the end of this week.</p>
<p>New user accounts will start being created again once jobs are re-enabled.</p>
<p>I'll also be sending an update in the next few days about our future filesystem plans and 
mitigations we were working on before this happened.</p>
</li>
</ul>
<h4 id="check-lists-of-damaged-files-on-myriad">Check lists of damaged files on Myriad<a class="headerlink" href="#check-lists-of-damaged-files-on-myriad" title="Permanent link">§</a></h4>
<ul>
<li>
<p>2024-01-12 14:00 Myriad: filesystem access restored, jobs tentatively expected for Monday</p>
<p>We've restored read-write access to Myriad's filesystem, and you will be able to log in and 
see all your directories again. Here's some detail about how you can identify which of your 
files were damaged.</p>
<p><strong>Your data on Myriad</strong></p>
<p>During the incident all files that resided at least partially on OST00 have been lost. 
In total these are 70M files (out of a filesystem total of 991M).</p>
<p>We restored files in <code>/lustre/home</code> from the latest backup where available. Data that was 
created while the backup was running or afterwards had no backup and could not be restored. 
For the time being, we will keep the latest backup available read-only in the directory 
<code>/lustre-backup</code>.</p>
<p>Files in <code>/lustre/scratch</code> and <code>/lustre/projects</code> were not backed up, as a matter of policy. 
All OST00 files from these directories have been lost.</p>
<p>Where files were lost but still showing up in directory listings, we have removed ("unlinked") 
them so it doesn't appear that they are still there when they are not.</p>
<p>For a tiny fraction of the lost files (0.03%), there is still some file data accessible, but 
most of these files are damaged (e.g. truncated or partially filled with zeroes). For some 
users these damaged files might still contain useful information, so we have left these files 
untouched.</p>
<p>The following files have been placed into your home directory:</p>
<ul>
<li>
<p>OST00-FILES-HOME-restored.txt</p>
<ul>
<li>A list of your home directory files that resided on OST00 and that were successfully 
 restored from backup.</li>
</ul>
</li>
<li>
<p>OST00-FILES-HOME-failed.txt</p>
<ul>
<li>A list of your home directory files that resided on OST00 and that could not be restored 
   from backup, including one of the following messages:</li>
<li>"no backup, stale directory entry, unlinked" - There was no backup for this file, and we 
   removed the stale directory entry.</li>
<li>"target file exists, potentially corrupt, leaving untouched" - Original file data is still 
   accessible, but likely damaged or corrupt. Feel free to delete these files if there's no 
   useful data in there.</li>
</ul>
</li>
<li>
<p>OST00-FILES-SCRATCH.txt</p>
<ul>
<li>A list of your Scratch directory files that resided on OST00, including one of the 
   following messages (similar to the above):</li>
<li>"stale directory entry, unlinked"</li>
<li>"file exists, potentially corrupt, leaving untouched"</li>
</ul>
</li>
</ul>
<p>For projects, the following file has been placed in the project root directory:</p>
<ul>
<li>OST00-FILES-PROJECTS.txt<ul>
<li>A list of project files that resided on OST00, including one of the following messages:</li>
<li>"stale directory entry, unlinked"</li>
<li>"file exists, potentially corrupt, leaving untouched"</li>
</ul>
</li>
</ul>
<p>A very few users had newline characters (<code>\n</code>) in their filenames: in this case in the above 
.txt files the <code>\n</code> has been replaced by the string <code>__NEWLINE__</code>, and an additional .bin file 
has been placed alongside the .txt file, containing the list of original filenames terminated 
by null bytes (and not including the messages).</p>
<p>These OST00-FILES-* files are owned by root, so that they don't use up any of your quota. 
You can still rename, move, or delete these files.</p>
<p><strong>Jobs</strong></p>
<p>We're currently running a Lustre filesystem check in the background. Provided it does not throw 
up any serious problems, we expect to be able to re-enable jobs during Monday. We'll be putting
user holds on all the jobs so you can check that the files and applications they are trying to 
use exist before allowing them to be scheduled. They will show in status <code>hqw</code> in qstat.</p>
<p>Once you have made sure they are ok, you will be able to use <code>qrls</code> followed by a job ID to 
release that job, or <code>qrls all</code> to release all your jobs. They will then be in status <code>qw</code> and 
queue as normal. (Array jobs will have the first task in status <code>qw</code> and the rest in <code>hqw</code> - 
this is normal). If you want to delete the jobs instead, use <code>qdel</code> followed by the job ID.</p>
<p><strong>Software</strong></p>
<p>We have successfully restored the vast majority of Myriad's software stack. I'll send a final 
update when we re-enable jobs, but at present I expect the missing applications to be:</p>
<ul>
<li>ABAQUS 2017</li>
<li>ANSYS (all versions)</li>
<li>STAR-CCM+ (all versions)</li>
<li>STAR-CD (all versions)</li>
</ul>
<p>These are all licensed applications that are best reinstalled from their original media, so 
we'll be working through those, starting with the most recent version we had.</p>
<p>Please send any queries to rc-support@ucl.ac.uk. If you've asked us for account deletions, we 
will be starting those next week, along with new user account creations.</p>
</li>
<li>
<p>2024-01-15 16:00 - Myriad jobs enabled</p>
<p>We have now allowed jobs to start on Myriad. </p>
<p>We have reinstalled ABAQUS 2017, ANSYS 2023.R1, STAR-CCM+ 14.06.013 and STAR-CD 4.28.050. The 
older versions of these applications are still missing at the moment.</p>
<p>Your jobs that were still in the queue from before have user holds on them and show in status 
<code>hqw</code> in qstat.</p>
<p>Once you have made sure the files and applications the jobs are using are present and correct, 
you will be able to use <code>qrls</code> followed by a job ID to release that job, or <code>qrls all</code> to 
release all your jobs. They will then be in status <code>qw</code> and queue as normal. (Array jobs will 
have the first task in status <code>qw</code> and the rest in <code>hqw</code> - this is normal). If you want to 
delete the jobs instead, use <code>qdel</code> followed by the job ID.</p>
<p>User deletions and new user creations are underway. We'll need to check that the 
synchronisation to the mailing list is working correctly and people are being added and removed
as appropriate.</p>
</li>
</ul>
<h4 id="action-required-compression-or-removal-of-data">Action required - compression or removal of data<a class="headerlink" href="#action-required-compression-or-removal-of-data" title="Permanent link">§</a></h4>
<ul>
<li>
<p>2024-10-18 13:55 - <strong>Action required: compression or removal of data on Myriad</strong></p>
<p>Myriad's filesystem is too full, so we need everyone to check what data they are keeping on the 
cluster and to remove data they are not currently using. To perform effectively, the filesystem 
needs a significant portion of empty space. As it gets fuller, performance begins to get worse 
and then stability also decreases. </p>
<p>The filesystem is at 70% full, and we will need to stop jobs running when it reaches 75%. (1% of 
the filesystem is 19.4 TiB).</p>
<p>Keeping data unnecessarily on the system affects everyone's ability to run jobs.</p>
<p>We will also be contacting those of you with large amounts of data separately.</p>
<p><strong>How to check usage and compress data</strong></p>
<p>You can check your quota and see how much space you are using on Myriad with the lquota command, 
and you can see which directories are taking the most space using <code>du -h</code> which can also be run 
in specific directories. You can tell du to only output details of the first level of directory 
sizes with the <code>--max-depth=1</code> option.</p>
<p>If you cannot remove your data from the cluster, please consider whether you can archive and 
compress any of it for the time being.</p>
<p>Example to tar up and gzip compress a directory:</p>
<ul>
<li><code>tar -czvf /home/username/Scratch/myarchive.tar.gz /home/username/Scratch/work_data</code> 
  will (c)reate a g(z)ipped archive (v)erbosely in the specified (f)ilename location. The 
  contents will be everything in this user's <code>work_data</code> directory.</li>
</ul>
<p>You can request bzip2 compression instead with <code>-j</code> or <code>--bzip2</code> and <code>xz</code> compression with <code>-J</code> 
or <code>--xz</code>. These will give you better compression than gzip, with xz generally being the most 
compressed.</p>
<p>If you are compressing an individual file rather than a directory, you can use the <code>gzip</code>, 
<code>bzip2</code> and <code>xz</code> commands on their own without tar.</p>
<p>(Have a look at <code>man tar</code> or gzip, bzip2 and xz for the manual page details - they also contain 
the names of the uncompress commands).</p>
<p>If you are transferring data to Windows and want to uncompress the data there, 7-zip can open 
all of these formats. Or you can create your archives using the <code>zip</code> command.</p>
<p><strong>Quota policies</strong></p>
<p>We are going to be adjusting the policies for granting Scratch quota increases - the CRAG will 
be granting them for shorter periods of time and will not be as easily re-granting increases at 
the end of that time period. We will have a process for what happens when a quota increase 
expires, including notifications to you. You will be asked to consider your plans for what to 
do with the data when the quota increase expires at the time of requesting one.</p>
<p>We are also going to be setting up annual account reapplications again, so we can identify 
users who are no longer using the system and activate our account deletion policies.</p>
<p>This is to prevent the current situation where new users to the system cannot get relatively 
small and short term quota increases necessary for their work because increases granted in the 
past are still using the space.</p>
<p><strong>ARC Cluster File System (ACFS)</strong></p>
<p>Timelines for access to the ARC Cluster Filesystem (ACFS) from Myriad are currently being 
considered. There is some info about the ACFS at 
<a href="https://www.rc.ucl.ac.uk/docs/Background/Data_Storage/#arc-cluster-file-system-acfs">ARC Cluster File System</a> </p>
<p>The ACFS once available from Myriad will give you 1T in total of backed-up data (and your home 
directories will no longer be backed up). For those of you with larger Scratch quota increases, 
you will still need to consider what you will do with the bulk of your data. </p>
<p><strong>Info</strong></p>
<p>We recently added some pages to our documentation which may be relevant:</p>
<ul>
<li><a href="https://www.rc.ucl.ac.uk/docs/Background/Parallel_Filesystems/">Parallel Filesystems</a> includes 
  sections on working effectively with parallel filesystems and tips for use.</li>
<li><a href="https://www.rc.ucl.ac.uk/docs/Background/Data_Storage/">Data Storage</a> - what storage locations exist.</li>
<li><a href="https://www.rc.ucl.ac.uk/docs/Data_Management/">Data Management</a> - checking quotas, transferring 
  data to other users, requesting data from those who have left.</li>
</ul>
<p>Please email rc-support@ucl.ac.uk with any queries or raise a request about Myriad via 
<a href="https://myservices.ucl.ac.uk/">UCL MyServices</a>. If you are no longer using the cluster and 
wish to be removed from this mailing list, please also contact us and say we can delete your account.</p>
</li>
<li>
<p>2024-10-25 18:20 - Filesystem issues on Myriad</p>
<p>We have been having some filesystem issues since around 16:50.</p>
<p>One of the servers that is part of the filesystem kept crashing, we had a failover and then the new 
active one crashed as well. This will be preventing logins and jobs will have failed.</p>
<p>Depending on what is happening, we may not be able to sort this out until Monday and if access is 
restored it may be unstable throughout the weekend.</p>
</li>
<li>
<p>2024-10-28 10:20 - Filesystem recovered</p>
<p>The filesystem recovered on Friday evening and was running ok over the weekend. </p>
<p>While it was down, it is likely that many running jobs failed with I/O errors. There will also be 
numbers of them in state <code>dr</code> which we need to reboot the nodes to clear.</p>
<p>At the moment we're still seeing some leftover activity in the logs but there don't appear to be any 
active problems.</p>
</li>
<li>
<p>2024-11-12 17:20 - <strong>Myriad filesystem and refresh news; ACFS available now</strong></p>
<p><strong>The ARC Cluster File System (ACFS) is now available on Myriad.</strong></p>
<p>The ARC Cluster File System (ACFS) is ARC's centralised storage system that will be available from 
multiple ARC systems. If you also have a Kathleen account and already put some data in the ACFS, you 
will now be able to see that data on Myriad too.</p>
<p>It is the backed-up location for data which you wish to keep.</p>
<p>The ACFS is available read-write on the login nodes but read-only on the compute nodes. This means 
that your jobs can read from it, but not write to it, and it is intended that you copy data onto it 
after deciding which outputs from your jobs are important to keep.</p>
<ul>
<li>Location: <code>/acfs/users/&lt;username&gt;</code></li>
<li>Also at: <code>/home/&lt;username&gt;/ACFS</code> (a shortcut or symbolic link to the first location).</li>
<li>Backed up daily.</li>
<li>1T quota - no quota increases available on ACFS at present.</li>
<li>Check your ACFS quota with <code>aquota</code>.</li>
</ul>
<p>The ACFS has dual locations for resilience, and as a result commands like <code>du -h</code> or <code>ls -alsh</code> will 
report filesizes on it as being twice what they really are. The <code>aquota</code> command will show you real 
usage and quota.</p>
<ul>
<li><a href="../Background/Data_Storage/#arc-cluster-file-system-acfs">Data Storage - ACFS</a></li>
<li><a href="../Data_Management/#acfs-quotas">Data Management - ACFS quotas</a></li>
</ul>
<p><strong>Until Myriad's filesystem is replaced, your home will continue to be backed up.</strong> After the new 
filesystem is available, only the ACFS will be backed up.</p>
<p>Please move some of your current data to the ACFS to reduce the usage on Myriad's Scratch.</p>
<p><strong>Myriad refresh and new filesystem</strong></p>
<p>The Myriad refresh is going ahead, and we will be adding some new compute nodes to Myriad as well 
as replacing the filesystem, networking and admin nodes.</p>
<p>We expect the new filesystem to go live during March 2025. Prior to that, we will be conducting 
setup, tuning and testing behind the scenes. (We expect the hardware to arrive in December or first 
week of January depending on delivery windows).</p>
<p>We expect the new compute nodes to also go live during March 2025 - they will be purchased and 
arrive later than the filesystem, but their testing period should coincide.</p>
<p>The new filesystem will be GPFS. The new compute nodes will be AMD EPYC 64 core, ~750G RAM, with 
~950G local disk for tmpfs. Some of the oldest nodes will be removed (eg types H, I, J which are 
nodes named node-h, node-i, node-j).</p>
<p>Timescales are based on our current knowledge and best estimates!</p>
<p>There will be many software changes as part of this refresh as well - we will be updating the 
operating system, changing the scheduler to Slurm and refreshing the software stack. More details 
nearer the time.</p>
<p><strong>Action required - compression or removal of data</strong></p>
<p>Myriad's filesystem is still at 70% full and we need to stop jobs at 75% full. Please continue to 
look at <a href="#action-required-compression-or-removal-of-data">our previous message with full info</a> and 
move data to the ACFS, off-cluster, or compress it if possible.</p>
</li>
<li>
<p>2025-02-10 - <strong>OS, software stack, scheduler and hardware update news</strong></p>
<p>This is to keep you informed about upcoming changes to Myriad.</p>
<p><strong>Live now:</strong></p>
<ul>
<li>Research Data Storage Service mount point on login nodes</li>
<li>Open OnDemand pilot access available on request until OS upgrade</li>
</ul>
<p>If you have an RDSS project, you can now access that read-write on the Myriad login nodes at <code>/rdss</code>. 
There are subdirectories <code>rd00</code>, <code>rd01</code> that contain projects with names beginning 
<code>ritd-ag-project-rd00</code> or <code>ritd-ag-project-rd01</code> and so on. You cannot access it from the compute 
nodes. You should <code>cp</code> or <code>rsync</code> data to your home, Scratch or the ACFS via the login nodes before 
doing any computation using the data.</p>
<p>If you want to access our <a href="https://www.rc.ucl.ac.uk/docs/Supplementary/OnDemand/">Open OnDemand pilot</a> 
for a remote desktop, Jupyter Notebooks or RStudio, contact rc-support@ucl.ac.uk and we will give you 
access. Please fill in the feedback survey if you use it. This will be used to prioritise whether we 
redo the work to set it up after the OS upgrade or concentrate on other improvements.</p>
<p><strong>Incoming:</strong></p>
<ul>
<li>OS upgrade to RHEL 9.5</li>
<li>Scheduler move to Slurm</li>
<li>New software via Spack</li>
<li>Alteration to hardware refresh schedule</li>
</ul>
<p>We will be moving to RHEL 9.5 as our operating system and Slurm as our new scheduler. The new 
filesystem below will be available before this is rolled out on Myriad.</p>
<p>If you have access to paid priority time on Myriad, we will stop using Gold for this and use Slurm 
mechanisms for priority allocations instead. You will receive the equivalent resource.</p>
<p>Our user documentation will be updated with examples of Slurm jobscripts instead of the SGE ones we 
currently have. Slurm uses <code>srun</code> and <code>sbatch</code> commands instead of <code>qsub</code>, if you have come across 
those in other software documentation.</p>
<p>The OS upgrade should allow you to run more recent binaries that currently give errors about GLIBC 
being too old. System tools will be newer and may look a little different.</p>
<p>We are also creating a new small software stack built with Spack. This will available to you to test 
before the OS upgrade, then rebuilt again after it, so it will change slightly in the meantime. Do 
let us know if applications you use in it are missing options we currently have or not working as 
you expect. </p>
<p>Most of the existing software stack will be reinstalled for the new OS. We are looking to remove 
some of the oldest installs and modules that are not being used in jobs. We then intend to prune 
this further over time and add newer versions into the Spack stack.</p>
<p><strong>Alteration to hardware refresh schedule</strong></p>
<p>The oldest nodetypes (H,I,J) in Myriad have been drained of jobs as planned and are being removed 
to make space in the racks.</p>
<p>The new Myriad filesystem is in place, undergoing its final testing period and we should have 
information on 24 Feb for timescales when you can expect to begin using it.</p>
<p>We are still adding new hardware to Myriad for general use, but it will not be added in March/April 
as previously stated. Instead, we will be replacing all Myriad's network hardware first, and then 
getting the new standard compute nodes in the next UCL financial year after August 2025. The specs 
for the new compute nodes are:</p>
<ul>
<li>AMD EPYC 9554P 64C 360W 3.1GHz (64 cores)</li>
<li>768G RAM</li>
<li>2x 480GB M.2 7450 PRO NVMe SSD (960G local disk)</li>
</ul>
<p>Note, this does not affect timelines for any paid nodes and those can still go in before this. It 
is to split the purchase over financial years rather than any issue with hardware supply times.</p>
<p>There is some additional hardware that we may be able to make available to increase general use 
Myriad capacity in the intervening period. The details need working out and that will take a few 
months. The OS and scheduler upgrade will need to happen first.</p>
<p><strong>Maintenance day Tues 11 Feb</strong></p>
<p>The nodes are being drained for a system config change this maintenance day - they'll be rebooted 
and jobs restarted after each is updated. There will also be a reboot of switches in the ACFS that 
will cause the ACFS mount on Myriad to hang for a period during the day. This is listed at 
<a href="https://www.rc.ucl.ac.uk/docs/Planned_Outages/">Planned Outages</a></p>
<p><strong>Documentation links</strong></p>
<p>The SSL certificate for www.rc.ucl.ac.uk is due to expire at midnight on 12 Feb. We're getting a 
new one but there might be a gap if it can't be renewed in time. If that happens your browser may 
prevent you accessing that link because the certificate is expired.</p>
<ul>
<li><a href="https://github-pages.arc.ucl.ac.uk/mkdocs-rc-docs/">mkdocs-rc-docs on Github</a> will work</li>
</ul>
<p>If there is a gap when the certificate expires we'll update the links in the message you see when 
you log in to the cluster, but if you are using an existing link or bookmark for www.rc.ucl.ac.uk 
at that point you will get an error or warning about the expired certificate.</p>
<p>(This did not happen, the certificate was renewed in time).</p>
</li>
</ul>
<h4 id="latest-on-myriad">Latest on Myriad<a class="headerlink" href="#latest-on-myriad" title="Permanent link">§</a></h4>
<div class="codehilite"><pre><span></span><code>2025-03-05 - **Myriad new filesystem update**

The Myriad new filesystem is going to have a system update before we put it into production - the 
new version fixes a number of security vulnerabilities, and it will be less disruptive to your 
jobs if we do this before you have access to it. We also have some minor hardware issues that have 
failed deployment checks that we are getting resolved with our vendors.

We&#39;re currently expecting to be able to give you access to the new filesystem around **31 March**, 
assuming all goes well with the above updates and the remaining snagging issues.

**What will happen?**

When the new filesystem goes live, you&#39;ll log in and will see a new empty home and Scratch. Your 
old home and Scratch will be available read-only at `/old_lustre/home/username` and 
`/old_lustre/scratch/username`. You&#39;ll be able to copy or rsync data out of it to the new 
filesystem, to the ACFS or archive it elsewhere. You will not be able to modify or delete the data 
in `/old_lustre`.

Your new home directory will not be backed up. The ACFS will remain as your backed-up location.

`/old_lustre` will remain available for three months after the new filesystem go-live date and will 
then be removed.

All jobs will be held in the queue and you&#39;ll be able to remove the holds yourself when the data 
they need is in the right place on the new filesystem.

Information on how best to do the data moving will be sent nearer the time and added to the documentation.

**Quota expiry on the new filesystem**

For the new filesystem we will be updating our policy on what happens when increased quotas expire 
and are not renewed. This will involve moving your user data off Myriad&#39;s filesystem to another 
location temporarily, and then deletion of it on specified timescales. This is to avoid the new 
filesystem filling up with data which is no longer being worked on and to allow those of you who are 
actively using an increased quota to be able to have the space you need.

Myriad&#39;s filesystem is not a long-term data store - if you are using the data in your jobs, that is 
fine. If you are no longer using Myriad to do computations on the data, it shouldn&#39;t be left on 
Myriad&#39;s filesystem.

You will receive multiple notifications before and after your quota expires if this is happening.

Further details on this to come. A similar process will take place when your Myriad user account 
expires.
</code></pre></div>

<h3 id="kathleen">Kathleen<a class="headerlink" href="#kathleen" title="Permanent link">§</a></h3>
<ul>
<li>
<p>2024-09-05 - Kathleen outage for new filesystems on Tues 10 September - action required </p>
<p>The previously-announced Kathleen outage for new filesystems will now take place on maintenance 
day next week, <strong>Tuesday 10 September</strong>. </p>
<p>The Kathleen cluster will go into maintenance on Tuesday 10 Sept 2024 from 9:00. Logins to 
Kathleen will not be possible until the maintenance is finished. Any jobs that won’t finish by 
the start of the maintenance window will stay queued. We aim to finish the maintenance within 
one day, so that you can access Kathleen again on Weds 11 Sept.</p>
<p><strong>We are implementing a number of changes to how data is stored on Kathleen:</strong></p>
<ul>
<li>The current Lustre filesystem will be replaced with a new Lustre filesystem. The old Lustre is 
  running on aging and error-prone hardware, and suffers from performance issues, especially for 
  interactive work on the login nodes. The new Lustre should provide a vastly better experience.</li>
<li>The Kathleen nodes will mount the ACFS (ARC Cluster File System) which is ARC’s new centralised 
  storage system that will (eventually) be available on other ARC systems (e.g. Myriad) too. 
  ACFS will be available read-write on the login nodes but read-only on the compute nodes.</li>
<li>Going forward, only the data on ACFS will be backed up. <strong>Please note that the data on the new 
  Lustre will not be backed up, not even data under <code>/home</code></strong>.</li>
</ul>
<p><strong>After the maintenance, you have the following storage locations:</strong></p>
<ul>
<li><code>/home/username</code>: your new home directory on the new Lustre; not backed up (this is a change 
  to the current situation)</li>
<li><code>/scratch/username</code>: your new scratch directory on the new Lustre; not backed up</li>
<li><code>/acfs/users/username</code>: your ACFS directory; backed up daily; read-only on the compute nodes</li>
<li><code>/old_lustre/home/username</code>: your old home directory on the old Lustre; read-only</li>
<li><code>/old_lustre/scratch/username</code>: your old scratch directory on the old Lustre; read-only</li>
</ul>
<p>You will also have a <code>~/ACFS</code> shortcut/symbolic link in your home that points to <code>/acfs/users/username</code>.</p>
<p>If you are looking in your <code>/old_lustre/home/username/Scratch</code> symbolic link, that will direct you 
back to your <strong>new</strong> Scratch not your old Scratch because it uses an absolute path. Please make sure 
to access old Scratch using <code>/old_lustre/scratch/username</code>.</p>
<p><strong>What you will need to do (after the maintenance):</strong></p>
<ul>
<li>After login, you will notice that your new home and scratch directories are mostly empty. 
  Please copy any data you need from your old home and scratch directories under <code>/old_lustre</code> to 
  the appropriate new locations.<ul>
<li>E.g. <code>cp -rp /old_lustre/home/username/data /home/username</code> will recursively copy your old 
  <code>data</code> directory and everything in it into your new home, while preserving the permissions.</li>
</ul>
</li>
<li>Any data that you consider important enough to be backed up should be copied into your ACFS 
  directory instead.</li>
<li>You have <strong>three months</strong> to copy your data. After this, the <code>/old_lustre</code> will become unavailable.</li>
<li>Your queued jobs will be held (showing status <code>hqw</code> in <code>qstat</code>) and won’t start running 
  automatically, as their job scripts will likely refer to locations on <code>/lustre</code> which won’t exist 
  until you have copied over the data. After you have copied the data that your jobs need to the new 
  Lustre, you can release the hold on your queued jobs.<ul>
<li>E.g. <code>qrls $JOB_ID</code> will release a specific job ID, and <code>qrls all</code> will release all your jobs.</li>
<li>Released array jobs will have the first task in status <code>qw</code> and the rest in <code>hqw</code> - this is normal.</li>
</ul>
</li>
<li>Depending on the amount of data, the copying may take some time, especially if you have many small 
  files. If you are copying data to ACFS and you don’t need immediate access to each file individually, 
  consider creating tar archives instead of copying data recursively.<ul>
<li>E.g. <code>tar -czvf /acfs/users/username/myarchive.tar.gz /old_lustre/home/username/data</code> will 
  (c)reate a g(z)ipped archive (v)erbosely in the specified (f)ilename location. The contents will be 
  everything in this user's old <code>data</code> directory. </li>
</ul>
</li>
</ul>
<p>Further reminders will be sent before <code>/old_lustre</code> is removed on 11 December.</p>
<p><strong>Kathleen quotas</strong></p>
<p>You will continue to have one quota on Kathleen, with a default value of 250G that includes your 
home and Scratch. If you have an active quota increase request that has not reached its requested 
expiry date on your old space, we will be recreating these on the new space. As stated above, 
<code>/home</code> will no longer be backed up.</p>
<p><strong>ACFS quotas</strong></p>
<p>You will have 1T of quota on the ACFS. You will be able to check this with the <code>aquota</code> command.</p>
<p>The ACFS has dual locations for resilience, and as a result standard commands like <code>du</code> or <code>ls -al</code> 
will report filesizes on it as being twice what they really are. The <code>aquota</code> command will show you 
real usage and quota. One of the reasons for the previous delay is that we tried to get filesizes to 
report correctly in all circumstances, but that was not possible so we decided it was less confusing 
if everything other than <code>aquota</code> always showed double. </p>
<p>For those interested, the ACFS is a GPFS filesystem.</p>
<p>This outage will show shortly on <a href="https://www.rc.ucl.ac.uk/docs/Planned_Outages/">Planned Outages</a> 
and the ACFS information will be added to our documentation.</p>
<p>We apologise for the inconvenience, but we believe these changes will help to provide a more performant 
and robust service in the future.</p>
<p>Please email rc-support@ucl.ac.uk with any queries or raise a request about Kathleen via 
<a href="https://myservices.ucl.ac.uk/">UCL MyServices</a>.</p>
</li>
<li>
<p>2024-09-11 09:35 Kathleen filesystem outage complete</p>
<p>The outage is complete and you can log in and access your new home, Scratch and ACFS directories 
on Kathleen.</p>
<p>In addition to the information given previously:</p>
<ul>
<li>You will have a <code>~/ACFS</code> shortcut/symbolic link in your home that points to <code>/acfs/users/username</code></li>
<li>If you want to copy data and preserve the permissions, use <code>cp -rp</code> rather than <code>cp -r</code></li>
<li>If you are looking in your <code>/old_lustre/home/username/Scratch</code> symbolic link, that will direct you 
  back to your new Scratch not your old Scratch because it uses an absolute path. Please make sure to 
  access old Scratch using <code>/old_lustre/scratch/username</code></li>
</ul>
<p>This will be added to https://www.rc.ucl.ac.uk/docs/Status_page/#kathleen and has been updated in the 
original message there.</p>
<p>The message you see when first logging in to Kathleen has been updated with the change to backed-up 
locations and ACFS information.</p>
<p>We have these additional pages in our documentation:</p>
<ul>
<li>https://www.rc.ucl.ac.uk/docs/Background/Parallel_Filesystems/ (parallel filesystem concepts)</li>
<li>https://www.rc.ucl.ac.uk/docs/Background/Data_Storage/ (data storage locations we provide)</li>
<li>https://www.rc.ucl.ac.uk/docs/Data_Management/ (how to check quotas, transfer ownership of files)</li>
</ul>
<p><strong>Terms &amp; Conditions update</strong></p>
<p>We have updated our Terms and Conditions (https://www.rc.ucl.ac.uk/docs/Terms_and_Conditions/) - 
please take a look. It now defines our data retention policies and when we can access your data, 
among other things. </p>
</li>
<li>
<p>2024-11-28 14:00 - <strong>Reminder: Kathleen /old_lustre removal on 11 Dec; later upcoming changes</strong></p>
<p>This is a reminder that access to <code>/old_lustre</code> will be removed on <strong>Monday 11 December</strong>, so if 
you still have files in your old home and scratch directories, they will no longer be accessible 
after this time.</p>
<p>Here's the previous information sent out on how to check that and how to copy your data: 
https://www.rc.ucl.ac.uk/docs/Status_page/#kathleen</p>
<p>Note: do check for hidden files starting with a <code>.</code> as well, such as customisations you may have 
added to your <code>.bashrc</code>, config files for programs like <code>.vimrc</code> and other directories like <code>.conda</code>, 
<code>.python3local</code>, <code>.julia</code>, <code>.cpanm</code> where you may have installed packages or have other 
environments or configuration.</p>
<p>These are visible to <code>ls -a</code> but not to <code>ls</code>.</p>
<p><strong>Later upcoming changes</strong></p>
<p>You may be aware that we are working to update the operating system on all our clusters to RHEL 9. 
Kathleen is likely to be the first deployed. There will be more details nearer the time, but this will 
involve an outage and after it the operating system will be updated, the software will be rebuilt and we 
will have Slurm as our scheduler instead of SGE.</p>
<p>We've had questions from some of you about VSCode, since it will stop connecting to unsupported 
operating systems in February 2025 - once we have updated the operating system it will be able to connect 
again. </p>
<p>Right now we don't have a timescale for this but will be letting you know when we do. Development work 
is ongoing. Documentation will be updated for what you will need to do with jobscripts and job submission 
commands. This is just to let you know that these changes are coming.</p>
</li>
<li>
<p>2024-12-11 09:10 - Kathleen /old_lustre removal</p>
<p>11 Dec is not a Monday, contrary to my previous email. As a result, we will leave <code>/old_lustre</code> 
mounted on Kathleen until <strong>Monday 16 December</strong>. If you still have files in your old home and 
scratch directories, they will no longer be accessible after this time. It will be unmounted at
or shortly after 9am.</p>
</li>
<li>
<p>2025-02-10 - <strong>OS, software stack and scheduler update news</strong></p>
<p>This is to keep you informed about upcoming changes to Kathleen.</p>
<p><strong>Live now:</strong></p>
<ul>
<li>Research Data Storage Service mount point on login nodes</li>
</ul>
<p>If you have an RDSS project, you can now access that read-write on the Kathleen login nodes at <code>/rdss</code>.
There are subdirectories <code>rd00</code>, <code>rd01</code> that contain projects with names beginning
<code>ritd-ag-project-rd00</code> or <code>ritd-ag-project-rd01</code> and so on. You cannot access it from the compute
nodes. You should <code>cp</code> or <code>rsync</code> data to your home, Scratch or the ACFS via the login nodes before
doing any computation using the data.</p>
<p><strong>Incoming:</strong></p>
<ul>
<li>OS upgrade to RHEL 9.5</li>
<li>Scheduler move to Slurm</li>
<li>New software via Spack</li>
</ul>
<p>We will be moving to RHEL 9.5 as our operating system and Slurm as our new scheduler.</p>
<p>We will send more updates when we are ready to roll it out - more info should be coming in the next 
couple of weeks. It will be available on Kathleen first.</p>
<p>Our user documentation will be updated with examples of Slurm jobscripts instead of the SGE ones we
currently have. Slurm uses <code>srun</code> and <code>sbatch</code> commands instead of <code>qsub</code>, if you have come across
those in other software documentation.</p>
<p>The OS upgrade should allow you to run more recent binaries that currently give errors about GLIBC
being too old. System tools will be newer and may look a little different.</p>
<p>We are also creating a new small software stack built with Spack. This will available to you to test
before the OS upgrade, then rebuilt again after it, so it will change slightly in the meantime. Do
let us know if applications you use in it are missing options we currently have or not working as
you expect.</p>
<p>We are aware there is a new CASTEP to add, and we can't install the recommended newer Intel OneAPI 
compilers to build the newest VASP until after the OS upgrade.</p>
<p>Most of the existing software stack will be reinstalled for the new OS. We are looking to remove
some of the oldest installs and modules that are not being used in jobs. We then intend to prune
this further over time and add newer versions into the Spack stack.</p>
</li>
</ul>
<p><strong>Documentation links</strong></p>
<div class="codehilite"><pre><span></span><code>The SSL certificate for www.rc.ucl.ac.uk is due to expire at midnight on 12 Feb. We&#39;re getting a
new one but there might be a gap if it can&#39;t be renewed in time. If that happens your browser may
prevent you accessing that link because the certificate is expired.

* [mkdocs-rc-docs on Github](https://github-pages.arc.ucl.ac.uk/mkdocs-rc-docs/) will work

If there is a gap when the certificate expires we&#39;ll update the links in the message you see when
you log in to the cluster, but if you are using an existing link or bookmark for www.rc.ucl.ac.uk
at that point you will get an error or warning about the expired certificate.

(This did not happen, the certificate was renewed in time).
</code></pre></div>

<h4 id="latest-on-kathleen">Latest on Kathleen<a class="headerlink" href="#latest-on-kathleen" title="Permanent link">§</a></h4>
<ul>
<li>
<p>2025-02-17 - <strong>New test software stack available</strong></p>
<p>There is a test version of our next software stack available now on Kathleen. This has a small 
number of packages at present. What is in it and the names of modules are liable to change over 
time, so please do not rely on it for production work. Instead, please test whether the 
applications you intend to use work the way you would expect.</p>
<p>This stack is built using <a href="https://spack.readthedocs.io/en/latest/">Spack</a>.</p>
<p>To use:</p>
</li>
</ul>
<p>```
module load beta-modules
module load test-stack/2025-02</p>
<div class="codehilite"><pre><span></span><code>    After that, when you type `module avail` there will be several sections of additional modules at 
    the top of the output.

    Not everything contained in the stack is visible by default - we have made the applications that 
    we expect people to use directly visible and lots of their dependencies are hidden. These will 
    show up if you search for that package specifically, for example:
</code></pre></div>

<p>module avail libpng
-------------------------- /shared/ucl/apps/spack/0.23/deploy/2025-02/modules/applications/linux-rhel7-cascadelake --------------------------
libpng/1.6.39/gcc-12.3.0-iopfrab
````</p>
<div class="codehilite"><pre><span></span><code>This module does not show up in the full list but is still installed. It has a hash at the end 
of its name `-iopfrab` and this will change over time with different builds.

If you find you are needing one of these modules often, let us know and we&#39;ll make it one that 
is not hidden in the next release of this stack.

The stack will be rebuilt for the operating system upgrade and moved out from beta-modules.

This information is available at 
[Kathleen test software stack](https://www.rc.ucl.ac.uk/docs/Clusters/Kathleen/#test-software-stack)
</code></pre></div>

<h3 id="young">Young<a class="headerlink" href="#young" title="Permanent link">§</a></h3>
<ul>
<li>
<p>2023-10-26 14:50 - We seem to have a dying OmniPath switch in Young. The 32 nodes with names 
    beginning <code>node-c12b</code> lost their connection to the filesystem earlier. Powercycling the switch 
    only helped temporarily before it went down again. Those nodes are all currently out of service 
    so new jobs will not start on them, but if your job was running on one of those when the two 
    failures happened those jobs will have failed.</p>
<p>(You can see in <code>jobhist</code> what the head node of a job was, and the .po file will show all the 
nodes that an MPI job ran on).</p>
</li>
<li>
<p>2024-01 Parallel filesystem soon to be replaced.</p>
</li>
<li>
<p>2024-01-03 12:30 Filesystem issues on Young: temporary hangs, running but degraded </p>
<p>We've just had two periods today where Young's filesystem would have hung - hopefully briefly 
enough that operations in progress will have continued after it recovered.</p>
<p>We failed over from one server to the other and back again. </p>
<p>Young's filesystem is more at risk than usual right now since we have some failed disks and 
one area (one Object Store Target) is degraded. We have stopped new data from being written 
there and are migrating the existing data to the rest of the filesystem. </p>
<p>The filesystem is still working and Young is still running jobs. We do not need you to take 
any action at present, but things may be running more slowly while this completes.</p>
</li>
<li>
<p>2024-09 Young's new filesystem is being readied for service.</p>
</li>
</ul>
<h4 id="young-new-filesystem">Young new filesystem<a class="headerlink" href="#young-new-filesystem" title="Permanent link">§</a></h4>
<ul>
<li>
<p>2024-09-27 - Young and Michael outage for new filesystem on Mon 7 Oct - action required</p>
<p>We will be replacing the two filesystems on Young and Michael with one new filesystem on 
<strong>Monday 7 October 2024</strong>. </p>
<p>Both clusters will go into maintenance on Monday 7 Oct 2024 from 09:00am. Logins will not be 
possible until the maintenance is finished. Any jobs that won’t finish by the start of the 
maintenance window will stay queued. We aim to finish the maintenance within one day, so that 
you can access the clusters again on Tues 8 Oct.</p>
<p><strong>Single login node outages between 2-4 Oct</strong></p>
<p>From 2-4 October, Young <code>login02</code> and Michael <code>login11</code> will each be out of service for a day 
during this period for testing updates before the filesystem migration. There will be no 
interruption to jobs or logins to the general addresses <code>young.rc.ucl.ac.uk</code> and 
<code>michael.rc.ucl.ac.uk</code>. If you are on <code>login02</code> or <code>login11</code> at the time, you may see a message 
that it is about to go down for reboot, and if you have a tmux or screen session on that login 
node then it will be terminated. You will be able to log back in and be assigned to the other 
login node, Young <code>login01</code> or Michael <code>login10</code>.</p>
<p><strong>Why the change:</strong></p>
<ul>
<li>Young's filesystem is running on aging and error-prone hardware, and suffers from performance 
  issues, especially for interactive work on the login nodes. The new Lustre should provide a 
  vastly better experience.</li>
<li>Michael's filesystem is old and replacement parts are no longer available.</li>
<li>The new filesystem is a HPE ClusterStor Lustre filesystem and will enable both machines to keep 
  running in a supported and maintainable manner.</li>
</ul>
<p><strong>After the maintenance, you have the following storage locations:</strong></p>
<ul>
<li><code>/home/username</code>: your new home directory on the new Lustre; backed up</li>
<li><code>/scratch/username</code>: your new scratch directory on the new Lustre; not backed up</li>
<li><code>/old_lustre/home/username</code>: your old home directory on the old Lustre; read-only</li>
<li><code>/old_lustre/scratch/username</code>: your old scratch directory on the old Lustre; read-only</li>
</ul>
<p>If you currently have accounts on both Young and Michael, you will need to log into Young to 
see Young's <code>old_lustre</code> and into Michael to see Michael's <code>old_lustre</code>, but your home and 
Scratch will be the same on both, and the data you copy into it will be visible on both.</p>
<p><strong>Quotas</strong></p>
<p>On the new filesystem we are able to set separate home and Scratch quotas. </p>
<ul>
<li>Home: 100G, backed up</li>
<li>Scratch: 250G by default</li>
</ul>
<p>Previously the default quota was 250G total.</p>
<p>If you have an existing non-expired quota increase, we will increase your Scratch quota on 
the new filesystem to this amount. If you find you need an increased Scratch quota, you can 
run the <code>request_quota</code> command on either cluster and it will ask you for some information and 
send us a support ticket.</p>
<p><strong>What you will need to do (after the maintenance):</strong></p>
<ul>
<li>After login, you will notice that your new home and scratch directories are mostly empty. 
  Please copy any data you need from your old home and scratch directories under <code>/old_lustre</code> to 
  the appropriate new locations. Your existing SSH keys will all have been copied in so that you 
  can log in. You can do this copy on the login nodes.<ul>
<li>E.g. <code>cp -rp /old_lustre/home/username/data /home/username</code> will recursively copy with 
  preserved permissions your old <code>data</code> directory and everything in it into your new home.</li>
</ul>
</li>
<li>You have <strong>three months and one week</strong> to copy your data. After this, the <code>/old_lustre</code> will 
  become unavailable.</li>
<li>Your queued jobs will be held (showing status <code>hqw</code> in qstat) and won’t start running 
  automatically, as their job scripts will likely refer to locations on <code>/lustre</code> which won’t 
  exist until you have copied over the data. After you have copied the data that your jobs 
  need to the new Lustre, you can release the hold on your queued jobs.<ul>
<li>E.g. <code>qrls $JOB_ID</code> will release a specific job ID, and <code>qrls all</code> will release all your jobs.</li>
<li>Released array jobs will have the first task in status <code>qw</code> and the rest in <code>hqw</code> - this is normal.</li>
</ul>
</li>
<li>Depending on the amount of data, the copying may take some time, especially if you have many 
  small files. If you wish to archive some of your data, consider creating tar archives straight 
  away instead of copying data recursively.<ul>
<li>E.g. <code>tar -czvf /home/username/Scratch/myarchive.tar.gz /old_lustre/home/username/data</code> will 
  (c)reate a g(z)ipped archive (v)erbosely in the specified (f)ilename location. The contents 
  will be everything in this user's old <code>data</code> directory. </li>
</ul>
</li>
</ul>
<p>Further reminders will be sent before the <code>/old_lustre</code> locations are removed on <strong>14 January 2025</strong>.</p>
<p><strong>Terms &amp; Conditions update</strong></p>
<p>We have updated our <a href="https://www.rc.ucl.ac.uk/docs/Terms_and_Conditions/">Terms and Conditions for all services</a> 
- please take a look. It now defines our data retention policies and when we can access your data, 
among other things.</p>
<p>These outages are listed on <a href="https://www.rc.ucl.ac.uk/docs/Planned_Outages/">Planned Outages</a>. 
The information above will also be copied into the https://www.rc.ucl.ac.uk/docs/Status_page/ 
sections for Young and Michael.</p>
<p>Please email rc-support@ucl.ac.uk with any queries. </p>
<p>If you are no longer using Young or Michael and wish to be removed from these mailing lists, 
email us confirming that we can delete your accounts and we will do so and remove you from the lists.</p>
</li>
<li>
<p>2024-10-07 11:00 - <strong>Delay in returning Young GPU nodes to service</strong></p>
<p>In addition to the above, there may be a delay in allowing the GPU nodes in Young to run jobs again - 
we need to rebuild the existing GPU software for those, to have it on an architecture-specific section 
of our software stack. This will result in GPU jobs remaining in the queue while we complete this. I 
would expect this to be complete by Wednesday or Thursday but will keep you updated.</p>
</li>
<li>
<p>2024-10-08 13:30 - <strong>Logins are enabled.</strong></p>
<p>Logins are enabled again.</p>
<p>We're continuing to rebuild the GPU applications on Young.</p>
</li>
<li>
<p>2024-10-09 17:00 - <strong>GPU nodes are enabled, plus FAQ</strong></p>
</li>
</ul>
<p>The GPU applications are rebuilt so we have re-enabled jobs on the GPU nodes. Please let us know if you 
  encounter any issues or if we have missed something.</p>
<p>We found that we hadn't synced keys across for users created after 26 September so they could not log in 
  - this was sorted out at 10:20am today. It affected mmm1457 to mmm1463 on Young.</p>
<p><strong>Some frequently-asked questions about the filesystem update:</strong></p>
<ol>
<li>
<p>Where is my old <code>.bashrc</code>? &amp; Why are my jobs failing with module errors now?</p>
<p>This is in <code>/old_lustre/home/username/.bashrc</code> </p>
<p>It begins with a dot and is a hidden file so will only show up with <code>ls -a</code> rather than <code>ls</code>. You can 
 copy this across into your current home again. You may have put module load and unload commands in it, 
 so are now getting module conflicts when your jobs run since the modules are the default ones.</p>
<p>Note that this also applies to other hidden files or directories you may have, like <code>.condarc</code> and <code>.python3local</code>.</p>
</li>
<li>
<p>Do I need to copy my data in a job?</p>
<p>No, you can do the copy on the login nodes. It can cause a lot of I/O usage on the login nodes so you 
 could also do it in a job if you wanted or if you noticed that it was going slowly, but using the login 
 nodes for this is allowed.</p>
</li>
<li>
<p>I need more than the 100G home quota and 250G Scratch quota for my data, what do I do?</p>
<p>Home is your backed-up area and is limited in size. Scratch is not backed up and also not deleted and we 
 can give you more space there. Run the <code>request_quota</code> command and it will ask you a few questions and send 
 us the request. We migrated all quota increases that were made within the last 12 months to Scratch 
 increases and did not migrate older ones. This allows us to make sure that the quota increases are current 
 and still needed. We are processing the new requests.</p>
</li>
<li>
<p>Where should I write large temporary data during my jobs?</p>
<p>Into Scratch. As above, you can request a larger quota if you need one. (If you are using the GPU nodes, 
 you can use <code>$TMPDIR</code> and request it with <code>#$ -l tmpfs=20G</code> for example, but the GPU nodes are the only 
 ones with local disks so everything else needs to be in Scratch).</p>
</li>
</ol>
<ul>
<li>
<p>2024-09-04 15:10 - UKRI survey, new HBM nodes and GPU node driver updates</p>
<p><strong>EPSRC Large Scale Facilities - User survey</strong></p>
<p><a href="https://engagementhub.ukri.org/epsrc-researchinfrastructure/lsfusersurvey/consultation/intro/">EPSRC Large Scale Facilities survey</a></p>
<p>Please can you fill in this survey that EPSRC are running on all their Large Scale Facilities - it 
includes events run by facilities as well as accessibility of the remote service itself, so if you 
attended the MMM Hub Conference last week and didn't fill it in at the end, please consider it now.</p>
<p><strong>New High Bandwidth Memory nodes</strong></p>
<p>We have 32 new high bandwidth memory nodes in Young, each with 64 cores, 503G usable memory and 3.5T 
available to be requested as tmpfs in your job.</p>
<p>They are currently configured in cache mode, where the HBM functions as a memory-side cache for the 
contents of the DDR memory. This should allow all applications to take advantage of it without needing 
to be specifically configured to use it. </p>
<p>Please try this out with your applications and let us know how it is working and whether you see an 
improvement. We have one VASP test case at present and would like to build up suitable test cases 
with other applications, including more specific ones with VASP. Especially if you already know you 
have been running up against memory bandwidth limitations, please contact us at rc-support@ucl.ac.uk 
- if you have test cases and tell us where on the filesystem they are, we can get them from you. 
After we upgrade the operating system on the cluster we will be testing other HBM modes as well, which 
can have a greater impact if the application is aware of it.</p>
<p>We will also be using this information to inform future hardware purchases for the MMM community so any 
feedback, positive or negative, you have on these nodes would be gratefully received.</p>
<p><a href="../Clusters/Young/#high-bandwidth-memory-nodes">Information about the nodes and how to use</a></p>
<p>You will need to request these nodes explicitly in your jobscripts with <code>#$ -ac allow=W</code></p>
<p>The <a href="../Clusters/Young/#maximum-job-resources">Maximum job resources</a> and 
<a href="../Clusters/Young/#node-types">Node types</a> tables are also updated.</p>
</li>
</ul>
<h4 id="latest-on-young">Latest on Young<a class="headerlink" href="#latest-on-young" title="Permanent link">§</a></h4>
<ul>
<li>
<p>2025-01-14 11:10 Extension to availability of <code>/old_lustre</code> on Young until <strong>9am on Tues 28 Jan</strong></p>
<p>We've had multiple requests from people who have been unable to retrieve their data from Young's 
old_lustre in time for the removal of the filesystem today, so we are extending the deadline for 
two weeks.</p>
<p>The old filesystem on Young will be unmounted shortly after <strong>9am on Tuesday 28 January</strong>.</p>
<p>There have been some questions about ways to transfer files in chunks if your transfers are being 
interrupted, without repeating copying the same files - you can use <code>rsync</code> for this as it can 
resume incomplete copies by running again, and if you use the <code>-a</code> option ("archive") it retains 
many of the file properties, including e.g. timestamps.</p>
<blockquote>
<p>Use archive mode (recursively copy directories, copy symlinks without resolving, and preserve 
permissions, ownership and modification times):</p>
<p><code>rsync -a|--archive path/to/source path/to/destination</code></p>
</blockquote>
<p>(This shows that you can use <code>-a</code> or <code>--archive</code> as the option passed in).</p>
<p>Alejandro Santana-Bonilla (KCL) also created a script for copying your first 10 (or other number) 
directories, then second 10 and so on, if that is useful to you: </p>
<ul>
<li><a href="https://github.com/kcl-tscm/Young_instructions/tree/main/old_lustre_folders">old_lustre_folders script</a></li>
</ul>
</li>
</ul>
<h3 id="michael">Michael<a class="headerlink" href="#michael" title="Permanent link">§</a></h3>
<ul>
<li>
<p>2024-01-24 16:40 - Problem on Michael's admin nodes causing DNS failures - now solved</p>
<p>We have been having DNS problems on Michael today since around 14:40, meaning that scheduler 
commands were not working and running jobs may have errors, including failed username lookups. 
New jobs trying to start during this time are likely to have failed on start up. Running jobs 
may have been affected, so please check the outputs for any jobs that were running during 14:40 
to 16:30 today.</p>
<p>This was caused by a problem on the admin nodes that has now been sorted out.</p>
</li>
<li>
<p>2024-09 Michael's new filesystem (shared with Young) is being readied for service.</p>
</li>
</ul>
<h4 id="michael-new-filesystem">Michael new filesystem<a class="headerlink" href="#michael-new-filesystem" title="Permanent link">§</a></h4>
<ul>
<li>
<p>2024-09-27 - Young and Michael outage for new filesystem on Mon 7 Oct - action required</p>
<p>We will be replacing the two filesystems on Young and Michael with one new filesystem on
<strong>Monday 7 October 2024</strong>.</p>
<p>Both clusters will go into maintenance on Monday 7 Oct 2024 from 09:00am. Logins will not be
possible until the maintenance is finished. Any jobs that won’t finish by the start of the
maintenance window will stay queued. We aim to finish the maintenance within one day, so that
you can access the clusters again on Tues 8 Oct.</p>
<p><strong>Single login node outages between 2-4 Oct</strong></p>
<p>From 2-4 October, Young <code>login02</code> and Michael <code>login11</code> will each be out of service for a day
during this period for testing updates before the filesystem migration. There will be no
interruption to jobs or logins to the general addresses <code>young.rc.ucl.ac.uk</code> and
<code>michael.rc.ucl.ac.uk</code>. If you are on <code>login02</code> or <code>login11</code> at the time, you may see a message
that it is about to go down for reboot, and if you have a tmux or screen session on that login
node then it will be terminated. You will be able to log back in and be assigned to the other
login node, Young <code>login01</code> or Michael <code>login10</code>.</p>
<p><strong>Why the change:</strong></p>
<ul>
<li>Young's filesystem is running on aging and error-prone hardware, and suffers from performance
  issues, especially for interactive work on the login nodes. The new Lustre should provide a
  vastly better experience.</li>
<li>Michael's filesystem is old and replacement parts are no longer available.</li>
<li>The new filesystem is a HPE ClusterStor Lustre filesystem and will enable both machines to keep
  running in a supported and maintainable manner.</li>
</ul>
<p><strong>After the maintenance, you have the following storage locations:</strong></p>
<ul>
<li><code>/home/username</code>: your new home directory on the new Lustre; backed up</li>
<li><code>/scratch/username</code>: your new scratch directory on the new Lustre; not backed up</li>
<li><code>/old_lustre/home/username</code>: your old home directory on the old Lustre; read-only</li>
<li><code>/old_lustre/scratch/username</code>: your old scratch directory on the old Lustre; read-only</li>
</ul>
<p>If you currently have accounts on both Young and Michael, you will need to log into Young to
see Young's <code>old_lustre</code> and into Michael to see Michael's <code>old_lustre</code>, but your home and
Scratch will be the same on both, and the data you copy into it will be visible on both.</p>
<p><strong>Quotas</strong></p>
<p>On the new filesystem we are able to set separate home and Scratch quotas.</p>
<ul>
<li>Home: 100G, backed up</li>
<li>Scratch: 250G by default</li>
</ul>
<p>Previously the default quota was 250G total.</p>
<p>If you have an existing non-expired quota increase, we will increase your Scratch quota on
the new filesystem to this amount. If you find you need an increased Scratch quota, you can
run the <code>request_quota</code> command on either cluster and it will ask you for some information and
send us a support ticket.</p>
<p><strong>What you will need to do (after the maintenance):</strong></p>
<ul>
<li>After login, you will notice that your new home and scratch directories are mostly empty.
  Please copy any data you need from your old home and scratch directories under <code>/old_lustre</code> to
  the appropriate new locations. Your existing SSH keys will all have been copied in so that you
  can log in. You can do this copy on the login nodes.<ul>
<li>E.g. <code>cp -rp /old_lustre/home/username/data /home/username</code> will recursively copy with
  preserved permissions your old <code>data</code> directory and everything in it into your new home.</li>
</ul>
</li>
<li>You have <strong>three months and one week</strong> to copy your data. After this, the <code>/old_lustre</code> will
  become unavailable.</li>
<li>Your queued jobs will be held (showing status <code>hqw</code> in qstat) and won’t start running
  automatically, as their job scripts will likely refer to locations on <code>/lustre</code> which won’t
  exist until you have copied over the data. After you have copied the data that your jobs
  need to the new Lustre, you can release the hold on your queued jobs.<ul>
<li>E.g. <code>qrls $JOB_ID</code> will release a specific job ID, and <code>qrls all</code> will release all your jobs.</li>
<li>Released array jobs will have the first task in status <code>qw</code> and the rest in <code>hqw</code> - this is normal.</li>
</ul>
</li>
<li>Depending on the amount of data, the copying may take some time, especially if you have many
  small files. If you wish to archive some of your data, consider creating tar archives straight
  away instead of copying data recursively.<ul>
<li>E.g. <code>tar -czvf /home/username/Scratch/myarchive.tar.gz /old_lustre/home/username/data</code> will
  (c)reate a g(z)ipped archive (v)erbosely in the specified (f)ilename location. The contents
  will be everything in this user's old <code>data</code> directory.</li>
</ul>
</li>
</ul>
<p>Further reminders will be sent before the <code>/old_lustre</code> locations are removed on <strong>14 January 2025</strong>.</p>
<p><strong>Terms &amp; Conditions update</strong></p>
<p>We have updated our <a href="https://www.rc.ucl.ac.uk/docs/Terms_and_Conditions/">Terms and Conditions for all services</a>
- please take a look. It now defines our data retention policies and when we can access your data,
among other things.</p>
<p>These outages are listed on <a href="https://www.rc.ucl.ac.uk/docs/Planned_Outages/">Planned Outages</a>. 
The information above will also be copied into the https://www.rc.ucl.ac.uk/docs/Status_page/ 
sections for Young and Michael.</p>
<p>Please email rc-support@ucl.ac.uk with any queries.</p>
<p>If you are no longer using Young or Michael and wish to be removed from these mailing lists,
email us confirming that we can delete your accounts and we will do so and remove you from the lists.</p>
</li>
<li>
<p>2024-10-08 13:30 Logins are enabled.</p>
<p>Logins are enabled again.</p>
</li>
</ul>
<h3 id="thomas">Thomas<a class="headerlink" href="#thomas" title="Permanent link">§</a></h3>
<ul>
<li>Thomas is now retired.</li>
</ul>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2014 - 2024 UCL Research Computing Group
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/UCL-ARC" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["instant"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  </body>
</html>