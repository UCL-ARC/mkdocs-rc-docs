{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Research Computing Services","text":"<p>This documentation is maintained by the Research Computing team for the purpose of sharing information about our services, including user guides, service updates and account request and renewal support.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>General User Information:</p> <ul> <li>Getting an Account</li> <li>Guide for New Users</li> <li>Guide for Experienced Users</li> <li>Planned Outages</li> </ul> <p>Clusters:</p> <ul> <li>Myriad</li> <li>Kathleen</li> <li>Young</li> <li>Michael</li> </ul>"},{"location":"#email-support","title":"Email Support","text":"<p>For support for any of our services or to report a problem with any of our computing platforms, contact us at: rc-support@ucl.ac.uk</p> <p>We will endeavour to answer queries on any aspect of computing related to your research, whatever your skill level or requirements.</p>"},{"location":"#how-to","title":"How to...","text":"<ul> <li>Connect to the Research Data Storage service</li> <li>Access services from outside UCL</li> <li>Apply for access to national GPU clusters</li> <li>Get more resources (more storage, more   compute time, longer run time, etc)</li> <li>Acknowledge the use of RC Systems</li> </ul>"},{"location":"#training","title":"Training","text":"<p>We have an online Moodle course \"Introduction to the Unix Shell\" and infrequently provide a training course aimed at getting users up and running on one of our main clusters. \"Introduction to the Unix Shell (Moodle) (UCL users), GitHub-Pages (non-UCL users), additional resources can be found on UCL mediacentral.</p> <p>We have an online Moodle course \"Introduction to High Performance Computing at UCL\" aimed at  getting users comfortable with using HPC at UCL. \"Introduction to High Performance Computing at UCL\" (Moodle)  (UCL users), UCL mediacentral and search for \"HPC\" (non-UCL users). </p> <p>We run some instances of these courses between specified dates with scheduled  online tutorial sessions or they can be done self-service at any time.</p> <p>Please see our ARC Training page for details of when the taught versions of these courses are run.</p>"},{"location":"Account_Services/","title":"Account Services","text":""},{"location":"Account_Services/#cluster-access-application","title":"Cluster access application","text":"<p>Important</p> <p>Check our Status page and Planned outages to see  current status of clusters. Accounts cannot be created while clusters are down and will be  carried out once they are back up.</p> <p>There is a single online form for applying to the majority of our clusters. Use of these services is subject to a common set of terms and conditions. All granted applications give you a Myriad account while access to other systems depends on what types of work you tell us you will be doing.</p>"},{"location":"Account_Services/#account-sponsors","title":"Account sponsors","text":"<p>If you are a student or postdoctoral researcher, your application must be approved by a permanent member of staff (normally your supervisor or PI). This will be automatically determined when you begin your application, and when you submit it, an email will be sent to this person asking them to approve the application before the account can be created.</p> <p>Please note that the form requires your sponsor's UCL username and not their UPI.</p> <p>Permanent members of staff do not need a sponsor and their accounts will be automatically approved.</p>"},{"location":"Account_Services/#apply-for-an-account","title":"Apply for an account","text":"<p>The online application form can be found here:</p> <ul> <li>User account application form   (UCL login required via Shibboleth)</li> </ul> <p>You will need a UCL user name and password. These are the same credentials used to access core services like Portico and HR systems.</p>"},{"location":"Account_Services/#application-process","title":"Application process","text":"<p>The application process has these steps:</p> <ol> <li> <p>Enter your UCL username and password to access the application form.</p> </li> <li> <p>Complete the application form, reading the instructions carefully. Tip: hover    your mouse over text boxes for more information.</p> </li> <li> <p>When you successfully submit the form, your sponsor will be sent an email message    asking them to approve your application. If you do not require a sponsor,    your application will be automatically approved.</p> </li> <li> <p>Your sponsor should click on the link in the email, log in and approve the account.</p> </li> <li> <p>You will then receive an email when your account is approved, if you have a    sponsor. (This does not mean your account has been created yet).</p> </li> <li> <p>You should receive an email once we have created your account. Please note    that there may be a delay of up to one working day between an application    being approved and your account being created.</p> </li> </ol> <p>If your sponsor does not receive the email to approve your account, send them the link to your application directly (it will look like <code>dashboard.rc.ucl.ac.uk/computing/requests/xxxx</code>) and there will be a button at the top to approve it.</p> <p>Please note: most delays in the application process are caused by sponsors missing or not receiving the approval email, so check with your sponsor first if your application is stuck waiting for approval. Research Computing can only create your account after it is approved.</p>"},{"location":"Account_Services/#accounts-for-visitors","title":"Accounts for visitors","text":"<p>UCL visitors are welcome to apply for accounts on Research Computing Services. Please note that:</p> <ol> <li>Applicants must have a central UCL account. UCL Visitor accounts can be    arranged by your Departmental Administrator, so you should speak to them    first.</li> <li>Account applications should specify the UCL grant under which the work is    being carried out, if possible, as well as an associated UCL Group or    Researcher.</li> <li>Account applications may not be submitted on behalf of another, except to    cover accessibility requirements, as the account application process includes    agreeing to relevant legal terms and conditions.</li> </ol>"},{"location":"Account_Services/#accounts-for-honorary-staff","title":"Accounts for Honorary staff","text":"<p>UCL Staff Members may nominate Honorary members (named individuals) to be provided with access to Research Computing services where this is beneficial to UCL's research interests.</p> <p>Nomination should be made via the CRAG, explaining the benefit arising to UCL.</p> <p>Proposals will be reviewed by the CRAG at their monthly meeting and on a case by case basis.</p> <p>All accounts thus provided are subject to all other 'standard' T&amp;C's relating to their use of Research Computing Services.</p>"},{"location":"Account_Services/#charges-for-use-of-research-computing-services","title":"Charges for use of Research Computing services","text":"<p>Research Computing services are free at point of use by default. There are no direct charges for your usage under standard resource allocation policy as defined by the CRAG.</p> <p>Several methods are available to researchers who wish to gain access to additional resources, or obtain 'priority' use, including chargeable options. Details are available at Additional Resource_Requests and Purchasing in Myriad.</p>"},{"location":"Additional_Resource_Requests/","title":"Additional Resource Requests","text":"<p>We recognise that researchers may sometimes require a higher throughput of work than it is possible to achieve with free \u2018fair share\u2019 usage of Myriad and Kathleen. There a couple of ways of obtaining additional Myriad resources beyond this fair share:</p>"},{"location":"Additional_Resource_Requests/#make-a-special-request-to-the-crag-for-free-access-to-additional-resources","title":"Make a special request to the CRAG for free access to additional resources","text":"<p>Users who wish to request additional resources or reserve resources beyond those provided can complete the additional resource request form in collaboration with your supervisor or the project's principal investigator. This includes requests for increased storage quotas.</p> <p>The completed form should be sent to the Research Computing Platforms team at rc-support@ucl.ac.uk, for technical review. If successful, your case will be presented to the CRAG for consideration at the next meeting of the Group. The CRAG meets monthly, usually on the second Tuesday of the month, and users will be informed of the Group\u2019s decision as soon as possible after their next meeting.</p> <p>Note that an application to the CRAG for additional resources is only likely to be approved if the impact on other users is not deemed to be significant, or of long duration.</p> <ul> <li>Additional resource request form</li> <li>NB: Myriad's filesystem is still very full (replacement due March 2025) and so     we need more detail for requests, plus your plan for removal of the data once your      quota increase ends. We will be considering requests in batches and comparing to     space left. All granted requests will expire at the end of March 2025 or Myriad     refresh date, whichever is earliest, and will require reapplication for the new      filesystem.</li> </ul>"},{"location":"Additional_Resource_Requests/#examples-of-requests","title":"Examples of requests","text":"<ul> <li>Increased Scratch quota - tell us how much, for what purpose and how long you'll need it. Also    tell us in brief your plan for removal of the data once your quota increase ends.<ul> <li>Once implemented, <code>lquota</code> will show the new quota.</li> </ul> </li> <li>Longer wallclock limit<ul> <li>Consider whether you can checkpoint and restart your job: that is, write out everything that   you need for a second job to begin where the previous one finished. Running multiple shorter   jobs one after the other is more robust since if anything goes wrong you lose less work. If   this is not possible for your jobs, explain why. </li> <li>Please note that we cannot guarantee that longer jobs will not be interrupted by planned   outages or maintenance periods. We try not to, but sometimes it is unavoidable.</li> <li>Include details on what resources one job is likely to require, how many of those jobs you   expect to need to run, and over what time period you'll need access to longer jobs. </li> <li>If granted, you will be given access to the <code>crag5day</code> project for example, and by adding   <code>#$ -P crag5day</code> to your jobscript you will be able to request a longer wallclock time than    usual for that job.</li> </ul> </li> </ul>"},{"location":"Additional_Resource_Requests/#request-hosting-of-shared-datasets","title":"Request hosting of shared datasets","text":"<p>We have provision for hosting shared datasets for users on Myriad. These can be datasets that are freely accessible by all users, or ones limited to groups.</p> <p>Hosted datasets:</p> <ul> <li>Will not be backed up.</li> <li>Must have a named primary contact.</li> <li>Must be reapplied for every 12 months to make sure they are still     current and required.</li> <li>Will have an associated quota.</li> <li>Will be removed when renewal lapses (notice will be given).</li> </ul> <p>They are likely to be managed by a role account - access to the role account will be by ssh key.</p> <p>To apply for a hosted dataset, please send this form to rc-support@ucl.ac.uk.</p> <ul> <li>Hosted dataset request form</li> </ul>"},{"location":"Additional_Resource_Requests/#purchase-dedicated-compute-nodes-or-priority-cycles-on-myriad","title":"Purchase dedicated compute nodes or priority cycles on Myriad","text":"<p>Researchers may purchase additional resources to be used as part of the Myriad High Performance Computing cluster if the free service does not meet their needs. See Paid-for resources: Purchasing in Myriad for details.</p>"},{"location":"Additional_Resource_Requests/#further-information","title":"Further information","text":"<p>For further advice or information on future hardware options, please contact rc-support@ucl.ac.uk.</p>"},{"location":"Contact_Us/","title":"Contact and Support","text":"<p>Users should direct any queries relating to their use of Research Computing services to the Research Computing Support Team at  rc-support@ucl.ac.uk (see below). The team will respond to your question as quickly as possible, giving priority to requests that are deemed urgent on the basis of the information provided.</p> <p>Availability: 9:30am - 4:30pm, Monday - Friday, except on Bank Holidays and College Closures.</p> <p>We aim to provide to you with a useful response within 24 hours.</p> <p>Please do not email individuals unless you are explicitly asked to do so; always use the rc-support email address provided as this is the best way for your request to be processed.</p>"},{"location":"Contact_Us/#drop-in-sessions","title":"Drop-In Sessions","text":"<p>Research IT Services holds drop-in sessions roughly every two weeks which at least one member of the Research Computing team usually attends. More details and dates for these sessions are available on the the RITS pages.</p> <p>If you have a particularly complex problem, it may be useful to email the support address, rc-support@ucl.ac.uk, beforehand so that the person attending can prepare.</p>"},{"location":"Contact_Us/#location","title":"Location","text":"<p>The Research Computing Team are located at:</p> <p>38-50 Bidborough Street Floor 3 London WC1H 9BT</p> <p>We are keen to collaborate and welcome visitors to our offices to talk about all things research computing. However, we do not operate a walk-up service desk: if you are frustrated by slow response to a support ticket, we are sorry but please do send reminders as there is probably a good reason why your request is not being processed.</p>"},{"location":"Data_Management/","title":"Data Management","text":"<p>Our clusters have local parallel filesystems and may also have the ARC Cluster File System (ACFS) available.</p>"},{"location":"Data_Management/#checking-quotas","title":"Checking quotas","text":"<p>The amount of storage you can use on the cluster is limited by a quota.</p>"},{"location":"Data_Management/#local-filesystem-quota","title":"Local filesystem quota","text":"<p>Check your quota and usage for home and Scratch with the <code>lquota</code> command.</p>"},{"location":"Data_Management/#acfs-quotas","title":"ACFS quotas","text":"<p>Check your ACFS quota and usage with the <code>aquota</code> command.</p> <p>The ACFS has dual locations for resilience, and as a result commands like <code>du -h</code> or <code>ls -alsh</code>  will report filesizes on it as being twice what they really are. The <code>aquota</code> command will show you  real usage and quota.  </p>"},{"location":"Data_Management/#giving-files-to-another-user","title":"Giving files to another user","text":"<p>If both users are active and on the same cluster, you can give files to them.  Walkthrough: Giving Files to Another User</p>"},{"location":"Data_Management/#transferring-data-ownership","title":"Transferring data ownership","text":""},{"location":"Data_Management/#requesting-transfer-of-your-data-to-another-user","title":"Requesting transfer of your data to another user","text":"<p>If you want to transfer ownership of all your data on a service to another user, with their consent,  you can contact us at rc-support@ucl.ac.uk and ask us to do this. Please arrange this while you still  have access to the institutional credentials associated with the account. Without this, we cannot  identify you as the owner of the account. </p> <p>You will need to tell us what data to transfer, on what cluster, and the username of the recipient.</p>"},{"location":"Data_Management/#requesting-data-belonging-to-a-user-who-has-left","title":"Requesting data belonging to a user who has left","text":"<p>If a researcher you were working with has left and has not transferred their data to you before  leaving there is a general UCL Data Protection process to gain access to that data.</p> <p>At UCL Information Security Policy  go to Monitoring Forms and take a copy of Form MO2 \"Form MO2 - Request for Access to Stored Documents  and Email - long-term absence or staff have left\". (Note, it is also applicable to students). </p> <p>Follow the guidance on that page for how to encrypt the form when sending it to them. The form needs  to be signed by the head of department/division and the UCL data protection officer  (data-protection@ucl.ac.uk).</p>"},{"location":"Example_Jobscripts/","title":"Example Jobscripts","text":"<p>On this page we describe some basic example scripts to submit jobs to our clusters.</p> <p>After creating your script, submit it to the scheduler with:</p> <p><code>qsub my_script.sh</code></p>"},{"location":"Example_Jobscripts/#service-differences","title":"Service Differences","text":"<p>These scripts are applicable to all our clusters, but node sizes (core count, memory, and temporary storage sizes) differ between machines, so please check those details on the cluster-specific pages. Some clusters are diskless and have no temporary space that can be requested.</p>"},{"location":"Example_Jobscripts/#working-directories-and-output","title":"Working Directories and Output","text":"<p>The parallel filesystems we use to provide the home and scratch filesystems perform best when reading or writing single large files, and worst when operating on many different small files. To avoid causing problems, many of the scripts below are written to create all their files in the temporary <code>$TMPDIR</code> storage, and compress and copy them to the scratch area at the end of the job.</p> <p>This can be a problem if your job is not finishing and you need to see the output, or if your job is crashing or failing to produce what you expected. Feel free to modify the scripts to read from or write to Scratch directly, however, your performance will generally not be as good as writing to <code>$TMPDIR</code>, and you may impact the general performance of the machine if you do this with many jobs simultaneously. This is particularly the case with single-core jobs, because that core is guaranteed to be writing out data.</p> <p>Please be aware that some clusters are diskless (eg Kathleen) and have no <code>$TMPDIR</code> available  for use - in those you must remove the request for <code>tmpfs</code> in your script. Check the  cluster-specific pages.</p> <p>Note that there is also the option of using the <code>Local2Scratch</code> process,  which takes place after the job has finished, in the clean-up step. This gives you the option of always getting the contents of <code>$TMPDIR</code> back, at the cost of possibly getting incomplete files and not having any control over where the files go.</p>"},{"location":"Example_Jobscripts/#note-about-projects","title":"Note about Projects","text":"<p>Projects are a system used in the scheduler and the accounting system  to track budgets and access controls.</p> <p>Most users of UCL's internal clusters will not need to specify a project and will default to the AllUsers  project. Users of the Michael and Young services should refer to the specific  pages for those machines, and the information they were given when they registered.</p> <p>To specify a project ID in a job script, use the <code>-P</code> object as below:</p> <pre><code>#$\u00a0-P\u00a0&lt;your_project_id&gt;\n</code></pre>"},{"location":"Example_Jobscripts/#resources","title":"Resources","text":"<p>The lines starting with <code>#$ -l</code> are where you are requesting resources like wallclock  time (how long your job is allowed to run), memory, and possibly tmpfs (local hard  disk space on the node, if it is not diskless). In the case for array jobs, each job in the array is treated independently by the scheduler and are each allocated the same resources as are requested. For example, in a job array of 40 jobs requesting for 24 hours wallclock time and 3GB ram, each job in the array will be allocated 24 hours wallclock time and 3GB ram. Wallclock time does not include the time spent waiting in the queue. More details on how to request resources can be found here.</p> <p>If you have no notion of how much you should request for any of these, have a look at How do I estimate what resources to request in my jobscript?</p> <p>Useful resources:</p> <ul> <li>Resource requests (Moodle) (UCL users)</li> <li>Resource requests pt.2 (Moodle) (UCL users)</li> <li>Resource requests (mediacentral) (non-UCL users)</li> <li>Resource requests pt.2 (mediacentral) (non-UCL users)</li> </ul>"},{"location":"Example_Jobscripts/#serial-job-script-example","title":"Serial Job Script Example","text":"<p>The most basic type of job a user can submit is a serial job. These jobs run on a single processor (core) with a single thread. </p> <p>Shown below is a simple job script that runs /bin/date (which prints the current date) on the compute node, and puts the output into a file.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run a serial job under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Request 15 gigabyte of TMPDIR space (default is 10 GB - remove if cluster is diskless)\n#$ -l tmpfs=15G\n\n# Set the name of the job.\n#$ -N Serial_Job\n\n# Set the working directory to somewhere in your scratch space.  \n#  This is a necessary step as compute nodes cannot write to $HOME.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID.\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/workspace\n\n# Your work should be done in $TMPDIR \ncd $TMPDIR\n\n# Run the application and put the output into a file called date.txt\n/bin/date &gt; date.txt\n\n# Preferably, tar-up (archive) all output files onto the shared scratch area\ntar -zcvf $HOME/Scratch/workspace/files_from_job_$JOB_ID.tar.gz $TMPDIR\n\n# Make sure you have given enough time for the copy to complete!\n</code></pre>"},{"location":"Example_Jobscripts/#multi-threaded-job-example","title":"Multi-threaded Job Example","text":"<p>For programs that can use multiple threads, you can request multiple processor cores using the <code>-pe smp &lt;number&gt;</code> option. One common method for using multiple threads in a program is OpenMP, and the <code>$OMP_NUM_THREADS</code> environment variable is set automatically in a job of this type to tell OpenMP how many threads it should use. Most methods for running multi-threaded applications should correctly detect how many cores have been allocated, though (via a mechanism called <code>cgroups</code>).</p> <p>Note that this job script works directly in scratch instead of in the temporary <code>$TMPDIR</code> storage.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run an OpenMP threaded job under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM for each core/thread \n# (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Request 15 gigabyte of TMPDIR space (default is 10 GB - remove if cluster is diskless)\n#$ -l tmpfs=15G\n\n# Set the name of the job.\n#$ -N Multi-threaded_Job\n\n# Request 16 cores.\n#$ -pe smp 16\n\n# Set the working directory to somewhere in your scratch space.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/output\n\n# 8. Run the application.\n$HOME/my_program/example\n</code></pre>"},{"location":"Example_Jobscripts/#mpi-job-script-example","title":"MPI Job Script Example","text":"<p>The default MPI implementation on our clusters is the Intel MPI stack. MPI programs don\u2019t use a shared memory model so they can be run across multiple nodes. This script differs considerably from the serial and OpenMP jobs in that MPI programs need to be invoked by a program called gerun. This is our wrapper for mpirun and takes care of passing the number of processors and a file called a machine file.</p> <p>Important: If you wish to pass a file or stream of data to the standard input (stdin) of an MPI program, there are specific command-line options you need to use to control which MPI tasks are able to receive it. (<code>-s</code> for Intel MPI, <code>--stdin</code> for OpenMPI.) Please consult the help output of the <code>mpirun</code> command for further information. The <code>gerun</code> launcher does not automatically handle this.</p> <p>If you use OpenMPI, you need to make sure the Intel MPI modules are removed and the OpenMPI  modules are loaded, either in your jobscript or in your shell start-up files (e.g. <code>~/.bashrc</code>).</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run an MPI parallel job under SGE with Intel MPI.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM per process (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Request 15 gigabyte of TMPDIR space per node \n# (default is 10 GB - remove if cluster is diskless)\n#$ -l tmpfs=15G\n\n# Set the name of the job.\n#$ -N MadScience_1_16\n\n# Select the MPI parallel environment and 16 processes.\n#$ -pe mpi 16\n\n# Set the working directory to somewhere in your scratch space.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID :\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/output\n\n# Run our MPI job.  GERun is a wrapper that launches MPI jobs on our clusters.\ngerun $HOME/src/science/simulate\n</code></pre>"},{"location":"Example_Jobscripts/#array-job-script-example","title":"Array Job Script Example","text":"<p>If you want to submit a large number of similar serial jobs then it may be easier to submit them as an array job. Array jobs are similar to serial jobs except we use the <code>-t</code> option to get Sun Grid Engine to run 10,000 copies of this job numbered 1 to 10,000. Each job in this array will have the same job ID but a different task ID. The task ID is stored in the <code>$SGE_TASK_ID</code> environment variable in each task. All the usual SGE output files have the task ID appended. MPI jobs and parallel shared memory jobs can also be submitted as arrays.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run a serial array job under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Request 15 gigabyte of TMPDIR space (default is 10 GB - remove if cluster is diskless)\n#$ -l tmpfs=15G\n\n# Set up the job array.  In this instance we have requested 10000 tasks\n# numbered 1 to 10000.\n#$ -t 1-10000\n\n# Set the name of the job.\n#$ -N MyArrayJob\n\n# Set the working directory to somewhere in your scratch space. \n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID :)\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/output\n\n# Run the application.\n\necho \"$JOB_NAME $SGE_TASK_ID\"\n</code></pre>"},{"location":"Example_Jobscripts/#array-job-script-example-using-parameter-file","title":"Array Job Script Example Using Parameter File","text":"<p>Often a user will want to submit a large number of similar jobs but their input parameters don't match easily on to an index from 1 to n. In these cases it's possible to use a parameter file. To use this script a user needs to construct a file with a line for each element in the job array, with parameters separated by spaces.</p> <p>For example: </p> <pre><code>0001 1.5 3 aardvark\n0002 1.1 13 guppy\n0003 1.23 5 elephant\n0004 1.112 23 panda\n0005 ...\n</code></pre> <p>Assuming that this file is stored in <code>~/Scratch/input/params.txt</code> (you can call this file anything you want) then the user can use awk/sed to get the appropriate variables out of the file. The script below does this and stores them in <code>$index</code>, <code>$variable1</code>, <code>$variable2</code> and <code>$variable3</code>.  So for example in task 4, <code>$index = 0004</code>, <code>$variable1 = 1.112</code>, <code>$variable2 = 23</code> and <code>$variable3 = panda</code>.</p> <p>Since the parameter file can be generated automatically from a user's datasets, this approach allows the simple automation, submission and management of thousands or tens of thousands of tasks.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run an array job.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Request 15 gigabyte of TMPDIR space (default is 10 GB - remove if cluster is diskless)\n#$ -l tmpfs=15G\n\n# Set up the job array.  In this instance we have requested 1000 tasks\n# numbered 1 to 1000.\n#$ -t 1-1000\n\n# Set the name of the job.\n#$ -N array-params\n\n# Set the working directory to somewhere in your scratch space.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID :)\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/output\n\n# Parse parameter file to get variables.\nnumber=$SGE_TASK_ID\nparamfile=/home/&lt;your_UCL_id&gt;/Scratch/input/params.txt\n\nindex=\"`sed -n ${number}p $paramfile | awk '{print $1}'`\"\nvariable1=\"`sed -n ${number}p $paramfile | awk '{print $2}'`\"\nvariable2=\"`sed -n ${number}p $paramfile | awk '{print $3}'`\"\nvariable3=\"`sed -n ${number}p $paramfile | awk '{print $4}'`\"\n\n# Run the program (replace echo with your binary and options).\n\necho \"$index\" \"$variable1\" \"$variable2\" \"$variable3\"\n</code></pre>"},{"location":"Example_Jobscripts/#example-array-job-using-local2scratch","title":"Example Array Job Using Local2Scratch","text":"<p>Users can automate the transfer of data from <code>$TMPDIR</code> to their scratch space by adding the text <code>#Local2Scratch</code> to their script on a line alone as a special comment. During the clean-up phase of the job, a tool checks whether the script contains that text, and if so, files are transferred from <code>$TMPDIR</code> to a directory in scratch with the structure <code>&lt;job id&gt;/&lt;job id&gt;.&lt;task id&gt;.&lt;queue&gt;/</code>.</p> <p>The example below does this for a job array, but this works for any job type.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run an array job under SGE and \n#  transfer the output to Scratch from local.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Request 15 gigabyte of TMPDIR space (default is 10 GB - remove if cluster is diskless)\n#$ -l tmpfs=15G\n\n# Set up the job array.  In this instance we have requested 10000 tasks\n# numbered 1 to 10000.\n#$ -t 1-10000\n\n# Set the name of the job.\n#$ -N local2scratcharray\n\n# Set the working directory to somewhere in your scratch space.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID :)\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/output\n\n# Automate transfer of output to Scratch from $TMPDIR.\n#Local2Scratch\n\n# Run the application in TMPDIR.\ncd $TMPDIR\nhostname &gt; hostname.txt\n</code></pre>"},{"location":"Example_Jobscripts/#array-job-script-with-a-stride","title":"Array Job Script with a Stride","text":"<p>If each task for your array job is very small, you will get better use of the cluster if you can combine a number of these so each has a couple of hours' worth of work to do. There is a startup cost associated with the amount of time it takes to set up a new job. If your job's runtime is very small, this cost is proportionately high, and you incur it with every array task.</p> <p>Using a stride will allow you to leave your input files numbered as before, and each array task will run N inputs.</p> <p>For example, a stride of 10 will give you these task IDs: 1, 11, 21...</p> <p>Your script can then have a loop that runs task IDs from <code>$SGE_TASK_ID</code> to <code>$SGE_TASK_ID + 9</code>, so each task is doing ten times as many runs as it was before.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run an array job with strided task IDs under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Request 15 gigabyte of TMPDIR space (default is 10 GB - remove if cluster is diskless)\n#$ -l tmpfs=15G\n\n# Set up the job array.  In this instance we have requested task IDs\n# numbered 1 to 10000 with a stride of 10.\n#$ -t 1-10000:10\n\n# Set the name of the job.\n#$ -N arraystride\n\n# Set the working directory to somewhere in your scratch space.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID :)\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/output\n\n# Automate transfer of output to Scratch from $TMPDIR.\n#Local2Scratch\n\n# Do your work in $TMPDIR \ncd $TMPDIR\n\n# 10. Loop through the IDs covered by this stride and run the application if \n# the input file exists. (This is because the last stride may not have that\n# many inputs available). Or you can leave out the check and get an error.\nfor (( i=$SGE_TASK_ID; i&lt;$SGE_TASK_ID+10; i++ ))\ndo\n  if [ -f \"input.$i\" ]\n  then\n    echo \"$JOB_NAME\" \"$SGE_TASK_ID\" \"input.$i\"\n  fi\ndone\n</code></pre>"},{"location":"Example_Jobscripts/#hybrid-mpi-plus-openmp-jobscript-example","title":"Hybrid MPI plus OpenMP jobscript example","text":"<p>This is a type of job where you have a small number of MPI processes, and each one of those launches a number of threads. One common form of this is to have only one MPI process per node which handles communication between nodes, and  the work on each node is done in a shared memory style by threads.</p> <p>When requesting resources for this type of job, what you are asking the scheduler for is the physical number of cores and amount of memory per core that you need. Whether you end up running MPI processes or threads on that core is up to your code. (The <code>-pe mpi xx</code> request is telling it you want an MPI parallel environment and  xx number of cores, not that you want xx MPI processes - this can be confusing).</p>"},{"location":"Example_Jobscripts/#setting-number-of-threads","title":"Setting number of threads","text":"<p>You can either set <code>$OMP_NUM_THREADS</code> for the number of OpenMP threads yourself,  or allow it to be worked out automatically by setting it to <code>OMP_NUM_THREADS=$(ppn)</code>.  That is a helper script on our clusters which will set <code>$OMP_NUM_THREADS</code> to  <code>$NSLOTS/$NHOSTS</code>, so you will use threads within a node and MPI between nodes  and don't need to know in advance what size of node you are running on. Gerun  will then run <code>$NSLOTS/$OMP_NUM_THREADS</code> processes, round-robin allocated (if  supported by the MPI).</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run a hybrid parallel job under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM per core (must be an integer)\n#$ -l mem=1G\n\n# Request 15 gigabytes of TMPDIR space per node (default is 10 GB - remove if cluster is diskless)\n#$ -l tmpfs=15G\n\n# Set the name of the job.\n#$ -N MadIntelHybrid\n\n# Select the MPI parallel environment and 80 cores.\n#$ -pe mpi 80\n\n# Set the working directory to somewhere in your scratch space. \n# This directory must exist.\n#$ -wd /home/&lt;your_UCL_id/scratch/output/\n\n# Automatically set threads using ppn. On a cluster with 40 cores\n# per node this will be 80/2 = 40 threads.\nexport OMP_NUM_THREADS=$(ppn)\n\n# Run our MPI job with the default modules. Gerun is a wrapper script for mpirun. \ngerun $HOME/src/madscience/madhybrid\n</code></pre> <p>If you want to specify a specific number of OMP threads yourself, you would alter  the relevant lines above to this:</p> <pre><code># Request 80 cores and run 4 MPI processes per 40-core node, each spawning 10 threads\n#$ -pe mpi 80\nexport OMP_NUM_THREADS=10\ngerun your_binary\n</code></pre>"},{"location":"Example_Jobscripts/#gpu-job-script-example","title":"GPU Job Script Example","text":"<p>To use NVIDIA GPUs with the CUDA libraries, you need to load the CUDA runtime libraries module or else set up the environment yourself. The script below shows what you'll need to unload and load the appropriate modules.</p> <p>You also need to use the <code>-l gpu=&lt;number&gt;</code> option to request the GPUs from the scheduler.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run a GPU job under SGE.\n\n# Request a number of GPU cards, in this case 2 (the maximum)\n#$ -l gpu=2\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)\n#$ -l mem=1G\n\n# Request 15 gigabyte of TMPDIR space (default is 10 GB)\n#$ -l tmpfs=15G\n\n# Set the name of the job.\n#$ -N GPUJob\n\n# Set the working directory to somewhere in your scratch space.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID :)\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/output\n\n# Change into temporary directory to run work\ncd $TMPDIR\n\n# load the cuda module (in case you are running a CUDA program)\nmodule unload compilers mpi\nmodule load compilers/gnu/4.9.2\nmodule load cuda/7.5.18/gnu-4.9.2\n\n# Run the application - the line below is just a random example.\nmygpucode\n\n# 10. Preferably, tar-up (archive) all output files onto the shared scratch area\ntar zcvf $HOME/Scratch/files_from_job_$JOB_ID.tar.gz $TMPDIR\n\n# Make sure you have given enough time for the copy to complete!\n</code></pre>"},{"location":"Example_Jobscripts/#job-using-mpi-and-gpus","title":"Job using MPI and GPUs","text":"<p>It is possible to run MPI programs that use GPUs but our clusters currently only support this within a single node. The script below shows how to run a program using 2 gpus and 12 cpus.</p> <pre><code>#!/bin/bash -l\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 12 cores, 2 GPUs, 1 gigabyte of RAM per CPU, 15 gigabyte of TMPDIR space\n#$ -l mem=1G\n#$ -l gpu=2\n#$ -pe mpi 12\n#$ -l tmpfs=15G\n\n# Set the name of the job.\n#$ -N GPUMPIrun\n\n# Set the working directory to somewhere in your scratch space.\n#$ -wd /home/&lt;your user id&gt;/Scratch/output/\n\n# Run our MPI job. You can choose OpenMPI or IntelMPI for GCC.\nmodule unload compilers mpi\nmodule load compilers/gnu/4.9.2\nmodule load mpi/openmpi/1.10.1/gnu-4.9.2\nmodule load cuda/7.5.18/gnu-4.9.2\n\ngerun myGPUapp\n</code></pre>"},{"location":"Experienced_Users/","title":"Quick Start Guide for Experienced HPC Users","text":""},{"location":"Experienced_Users/#what-services-are-available","title":"What Services are available?","text":""},{"location":"Experienced_Users/#ucl","title":"UCL","text":"<ul> <li>Kathleen - HPC, large parallel MPI jobs.</li> <li>Myriad - High Throughput, GPU or large memory jobs.</li> </ul>"},{"location":"Experienced_Users/#external","title":"External","text":"<ul> <li>Young - MMM Hub Tier 2</li> <li>Michael - Faraday Institution Tier 2</li> </ul>"},{"location":"Experienced_Users/#how-do-i-get-access","title":"How do I get access?","text":"<p>UCL services: Fill in the sign-up form</p> <p>Tier 2 services: Contact your point of contact.</p>"},{"location":"Experienced_Users/#how-do-i-connect","title":"How do I connect?","text":"<p>All connections are via SSH, and you use your UCL credentials to log in (external users should use the <code>mmmXXXX</code> account with the SSH key they have provided to their point of contact). UCL services can only be connected to by users inside the UCL network which may mean using the institutional VPN or \"bouncing\" off another UCL machine when accessing them from outside the UCL network. The Tier 2 services (Michael and Young) are externally accessible.</p>"},{"location":"Experienced_Users/#login-hosts","title":"Login hosts","text":"Service General alias Direct login node addresses Kathleen <code>kathleen.rc.ucl.ac.uk</code> <code>login01.kathleen.rc.ucl.ac.uk</code> <code>login02.kathleen.rc.ucl.ac.uk</code> Myriad <code>myriad.rc.ucl.ac.uk</code> <code>login12.myriad.rc.ucl.ac.uk</code> <code>login13.myriad.rc.ucl.ac.uk</code> Young <code>young.rc.ucl.ac.uk</code> <code>login01.young.rc.ucl.ac.uk</code> <code>login02.young.rc.ucl.ac.uk</code> Michael <code>michael.rc.ucl.ac.uk</code> <code>login10.michael.rc.ucl.ac.uk</code> <code>login11.michael.rc.ucl.ac.uk</code> <p>Generally you should connect to the general alias as this is load-balanced across the available login nodes, however if you use <code>screen</code> or <code>tmux</code> you will want to use the direct hostname so that you can reconnect to your session.</p> <p>Please be aware that login nodes are shared resources, so users should not run memory intensive jobs nor jobs with long runtimes in the login node. Doing so may negatively impact the performance of the login node for other users. Hence, identified culprit user processes are systematically killed.</p>"},{"location":"Experienced_Users/#software-stack","title":"Software stack","text":"<p>All UCL services use the same software stack based upon RHEL 7.x with a standardised set of packages, exposed to the user through environment modules (the <code>module</code> command). By default this has a set of useful tools loaded, as well as the Intel compilers and MPI but users are free to change their own environment.</p>"},{"location":"Experienced_Users/#batch-system","title":"Batch System","text":"<p>UCL services use Grid Engine to manage jobs. This install is somewhat customised and so scripts for non-UCL services may not work.</p> <p>We recommend that when launching MPI jobs you use our <code>gerun</code> parallel launcher instead of <code>mpirun</code> as it inherits settings from the job and launches the appropriate number of processes with the MPI implementation you have chosen. It abstracts away a lot of the complexity between different version of MPI.</p> <pre><code># using gerun\ngerun myMPIprogram\n\n# using mpirun when a machinefile is needed (eg Intel MPI)\nmpirun -np $NSLOTS -machinefile $PE_HOSTFILE myMPIprogram\n</code></pre> <p><code>$NSLOTS</code> is an environment variable containing the value you gave to <code>-pe mpi</code> so you do not need to re-specify it.</p>"},{"location":"Experienced_Users/#troubleshooting-gerun","title":"Troubleshooting gerun","text":"<p>If you need to see what <code>gerun</code> is doing because something is not working as expected, look at the error file for your job, default name <code>$JOBNAME.e$JOB_ID</code>. It will contain debug information from <code>gerun</code> about where it ran and the exact <code>mpirun</code> command it used. </p> <p>You may need to use <code>mpirun</code> directly with different options if your program has sufficiently complex process placement requirements, or is using something like GlobalArrays and requires a different process layout than it is being given.</p>"},{"location":"Experienced_Users/#script-sections","title":"Script sections","text":""},{"location":"Experienced_Users/#shebang","title":"Shebang","text":"<p>It's important that you add the <code>-l</code> option to bash in the shebang so that login scripts are parsed and the environment modules environment is set up. <code>#!</code> must be the first two characters in the file, no previous white space.</p> <pre><code>#!/bin/bash -l\n</code></pre>"},{"location":"Experienced_Users/#resources-you-can-request","title":"Resources you can request","text":""},{"location":"Experienced_Users/#number-of-cores","title":"Number of cores","text":"<p>For MPI:</p> <pre><code>#$ -pe mpi &lt;number of cores&gt;\n</code></pre> <p>For threads:</p> <pre><code>#$ -pe smp &lt;number of cores&gt;\n</code></pre> <p>For single core jobs you don't need to request a number of cores. For hybrid codes use the MPI example and set <code>OMP_NUM_THREADS</code> to control the number of threads per node. <code>gerun</code> will launch the right number of processes in the right place if you use it.</p>"},{"location":"Experienced_Users/#amount-of-ram-per-core","title":"Amount of RAM per core","text":"<pre><code>#$ -l mem=&lt;integer amount of RAM in G or M&gt;\n</code></pre> <p>e.g. <code>#$ -l mem=4G</code> requests 4 gigabytes of RAM per core.</p>"},{"location":"Experienced_Users/#run-time","title":"Run time","text":"<pre><code>#$ -l h_rt=&lt;hours:minutes:seconds&gt;\n</code></pre> <p>e.g. <code>#$ -l h_rt=48:00:00</code> requests 48 hours.</p>"},{"location":"Experienced_Users/#working-directory","title":"Working directory","text":"<p>Either a specific working directory:</p> <pre><code>#$ -wd /path/to/working/directory\n</code></pre> <p>or the directory the script was submitted from:</p> <pre><code>#$ -cwd\n</code></pre>"},{"location":"Experienced_Users/#gpus-myriad-only","title":"GPUs (Myriad only)","text":"<pre><code>#$ -l gpu=&lt;number of GPUs&gt;\n</code></pre>"},{"location":"Experienced_Users/#enable-hyperthreads-kathleen-only","title":"Enable Hyperthreads (Kathleen only)","text":"<pre><code>#$ -l threads=1\n</code></pre> <p>The <code>-l threads=</code> request is not a true/false setting, instead you are telling the scheduler you want one slot to block one virtual cpu instead of the normal situation where it blocks two. If you have a script with a threads request and want to override it on the command line or set it back to normal, the usual case is <code>-l threads=2</code>. (Setting threads to 0 does not disable hyperthreading!)</p> <p>With Hyperthreads enabled you need to request twice as many cores and then control threads vs MPI ranks with <code>OMP_NUM_THREADS</code>. E.g. </p> <pre><code>#$ -pe mpi 160\n#$ -l threads=1\nexport OMP_NUM_THREADS=2\n</code></pre> <p>Would use 80 cores, with two threads (on Hyperthreads) per core. If you use <code>gerun</code> to launch your MPI process, it will take care of the division for you, but if you're using <code>mpirun</code> or <code>mpiexec</code> directly, you'll have to take care to use the correct number of MPI ranks per node yourself.</p> <p>Note that memory requests are now per virtual core with hyperthreading enabled.  If you asked for <code>#$ -l mem=4G</code>on a node with 80 virtual cores and 192G RAM then  you are requiring 320G RAM in total which will not fit on that node and so you  would be given a sparse process layout across more nodes to meet this requirement.</p>"},{"location":"Experienced_Users/#temporary-local-disk-every-machine-except-kathleen","title":"Temporary local disk (every machine EXCEPT Kathleen)","text":"<pre><code>#$ -l tmpdir=&lt;size in G&gt;\n</code></pre> <p>e.g. <code>#$ -l tmpdir=10G</code> requests 10 gigabytes of temporary local disk.</p>"},{"location":"Experienced_Users/#the-rest-of-the-script","title":"The rest of the script","text":"<p>You need to load any module dependencies, set up any custom environment variables or paths you need and then run the rest of your workflow.</p>"},{"location":"Experienced_Users/#submitting-your-jobscript","title":"Submitting your jobscript","text":"<p>Job scripts can be submitted with <code>qsub</code>, jobs can be monitored with <code>qstat</code> and deleted with <code>qdel</code>.</p>"},{"location":"Experienced_Users/#interactive-jobs","title":"Interactive jobs","text":"<p>If you need to run an interactive job, possibly with X forwarding, you can do so using <code>qrsh</code>. Please see our page on interactive jobs for more details.</p>"},{"location":"Interactive_Jobs/","title":"Interactive Job Sessions","text":"<p>For an interactive session, you reserve some compute nodes via the scheduler and then are logged in live, just like on the login nodes. These can be used for live visualisation, software debugging, or to work up a script to run your program without having to submit each attempt separately to the queue and wait for it to complete.</p>"},{"location":"Interactive_Jobs/#requesting-access","title":"Requesting Access","text":"<p>You will be granted an interactive shell after running a command that checks with the scheduler whether the resources you wish to use in your tests/analysis are available. Interactive sessions are requested using the qrsh command.  It typically takes the form:</p> <pre><code>qrsh\u00a0-pe\u00a0mpi\u00a08\u00a0-l\u00a0mem=512M,h_rt=2:00:00\u00a0-now\u00a0no\n</code></pre> <p>In this example you are asking to run eight parallel processes within an MPI environment, 512MB RAM per process, for a period of two hours.</p> <p>All job types we support on the system are supported via an interactive session (see our examples section). Likewise, all qsub options are supported like regular job submission with the difference that with qrsh they must be given at the command line, and not with any job script (or via -@).</p> <p>In addition the <code>-now</code> option is useful when a cluster is busy.  By default qrsh and qlogin jobs will run on the next scheduling cycle or give up. The <code>-now no</code> option tells it to keep waiting until it gets scheduled. Pressing Ctrl+C (i.e. the control key and the C key at the same time) will safely cancel the request if it doesn't seem to be able to get you a session.</p> <p>More resources can be found here:</p> <ul> <li>Moodle (UCL users)</li> <li>Mediacentral (non-UCL users)</li> </ul>"},{"location":"Interactive_Jobs/#interactive-x-sessions","title":"Interactive X sessions","text":"<p>You can get an interactive X session from the head node of the job back to the login node. The way to do this is to run the <code>qrsh</code> command in the following generic fashion:</p> <pre><code>qrsh\u00a0&lt;options&gt; &lt;command&gt;\u00a0&lt;arguments\u00a0to\u00a0&lt;command&gt;&gt;\n</code></pre> <p>Where <code>&lt;command&gt;</code> is either a command to launch an X terminal like Xterm or Mrxvt or a GUI application like XMGrace or GaussView.</p> <p>To make effective use of the X forwarding you will need to have logged in to the login node with ssh -X or some equivalent method. Here is an example of how you can get a X terminal session with the qrsh command:</p> <pre><code>qrsh\u00a0-l\u00a0mem=512M,h_rt=0:30:0\u00a0\\\n\u00a0\u00a0\u00a0\"/shared/ucl/apps/mrxvt/0.5.4/bin/mrxvt\u00a0-title\u00a0'User\u00a0Test\u00a0Node'\"\n</code></pre>"},{"location":"Interactive_Jobs/#working-on-the-nodes","title":"Working on the nodes","text":"<p>If you want to run a command on one of your other allocated nodes, you can use a standard <code>ssh</code> command from the interactive session: </p> <pre><code>ssh\u00a0&lt;hostname&gt; &lt;command&gt;\u00a0[args]\n</code></pre> <p>to access other nodes within your allocation. Note that you are not able to <code>ssh</code> directly from the login node.</p> <p>In the above, <code>&lt;hostname&gt;</code> can be obtained by inspecting the file <code>$TMPDIR/machines</code>.</p>"},{"location":"Interactive_Jobs/#gpu-test-nodes","title":"GPU test nodes","text":"<p>You can also run GPU jobs interactively simply by adding the <code>-l gpu=1</code> or <code>-l gpu=2</code> options to the qrsh command as normal.</p> <p>For more information, please contact us on rc-support@ucl.ac.uk</p>"},{"location":"Job_Results/","title":"Where do my results go?","text":"<p>After submitting your job, you can use the command <code>qstat</code> to view the status of all the jobs you have submitted. Once you can no longer see your job on the list, this means your job has completed. To view details on jobs that have completed, you can run <code>jobhist</code>, part of the <code>userscripts</code> module. There are various ways of monitoring the output of your job.</p>"},{"location":"Job_Results/#output-and-error-files","title":"Output and error files","text":"<p>When writing your job script you can either tell it to start in the directory you submit it from (<code>-cwd</code>), or from a particular directory (<code>-wd &lt;dir&gt;</code>), or from your home directory (the default). When your job runs, it will create files in this directory for the job's output and errors:</p> File Name Contents <code>myscript.sh</code> Your job script. <code>myscript.sh.o12345</code> Output from the job. (<code>stdout</code>) <code>myscript.sh.e12345</code> Errors, warnings, and other messages from the job that aren't mixed into the output. (<code>stderr</code>) <code>myscript.sh.po12345</code> Output from the setup script run before a job. (\"prolog\") <code>myscript.sh.pe12345</code> Output from the clean-up script run after a job. (\"epilog\") <p>Normally there should be nothing in the <code>.po</code> and <code>.pe</code> files, and that's fine. If you change the name of the job in the queue, using the <code>-N</code> option, your output and error files will use that as the filename stem instead.</p> <p>Most programs will also produce separate output files, in a way that is particular to that program. Often these will be in the same directory, but that depends on the program and how you ran it.</p>"},{"location":"Job_Results/#grid-engine-commands","title":"Grid Engine commands","text":"<p>The following commands can be used to submit and monitor a job.</p>"},{"location":"Job_Results/#qsub","title":"Qsub","text":"<p>This command submits your job to the batch queue. You can also use options on the command-line to override options you have put in your job script.</p> Command Action <code>qsub myscript.sh</code> Submit the script as-is <code>qsub -N NewName myscript.sh</code> Submit the script but change the job's name <code>qsub -l h_rt=24:0:0 myscript.sh</code> Submit the script but change the maximum run-time <code>qsub -hold_jid 12345 myscript.sh</code> Submit the script but make it wait for job 12345 to finish <code>qsub -ac allow=XYZ myscript.sh</code> Submit the script but only let it run on node classes X, Y, and Z"},{"location":"Job_Results/#qstat","title":"Qstat","text":"<p>This command shows the status of your jobs. When you run <code>qstat</code> with no options, all of your jobs currently running will be displayed. By adding in the option <code>-f -j &lt;job-ID&gt;</code> you will get more detail on the specified job.</p>"},{"location":"Job_Results/#qdel","title":"Qdel","text":"<p>This command deletes your job from the queue. When deleting a job will need to run <code>qdel &lt;job-ID&gt;</code>, however <code>qdel '*'</code> can be used to delete all jobs. To delete a batch of jobs, creating a file with the list of job IDs that you would like to delete and placing it in the following commands will delete the following jobs: <code>cat &lt;filename&gt; | xargs qdel</code></p>"},{"location":"Job_Results/#qsub-emailing","title":"Qsub emailing","text":"<p>We also have a mailing system that can be implemented to send emails with reminders of the status of your job through <code>qsub</code>. In your jobscript, or when you use <code>qsub</code> to submit your job, you can use the option <code>-m</code>. You can specify when you want an email sent to you by using the below options after <code>qsub -m</code>:</p> <code>b</code> Mail is sent at the beginning of the job. <code>e</code> Mail is sent at the end of the job. <code>a</code> Mail is sent when the job is aborted or rescheduled. <code>s</code> Mail is sent when the job is suspended. <code>n</code> No mail is sent. (The default.) <p>You specify where the email should be sent with <code>-M</code>.</p> <p>You can use more than one of these options by putting them together after the <code>-m</code> option; for example, adding the following to your job script would mean you get an email when the job begins and when it ends:</p> <pre><code>#$ -m be\n#$ -M me@example.com\n</code></pre> <p>Further resources can be found here:</p> <ul> <li>Scheduler fundamentals (moodle) (UCL users)</li> <li>Scheduler fundamentals (mediacentral) (non-UCL users)</li> </ul>"},{"location":"New_Users/","title":"Guide for New Users","text":"<ul> <li>What is a cluster?<ul> <li>Also includes why you would want to use one and the typical workflow of how you use a cluster</li> </ul> </li> <li>Which service(s) at UCL are right for me?</li> <li>How do I connect to the clusters?</li> <li>How do I transfer my data?</li> <li>How do I run a job?</li> <li>What does a jobscript look like?</li> <li>Where do my results go?</li> </ul>"},{"location":"Planned_Outages/","title":"Planned Outages","text":"<p>The second Tuesday of every month is a maintenance day, when the following clusters should be considered at risk from 8:00AM: Myriad, Kathleen, Young, Michael, Aristotle and the Data Science Platform. We won\u2019t necessarily perform maintenance every month, and notice by email will not always be given about maintenance day work that only puts services at risk.</p> <p>Full details of outages are emailed to the cluster-specific user lists. </p> <p>Generally speaking, an outage will last from the morning of the first date listed until mid-morning of the end date listed. The nodes may need to be emptied of jobs in advance ('drained'), so jobs may remain in the queue for longer before an outage begins.</p> <p>If there is a notable delay in bringing the system back we will contact you after approximately midday - please don't email us at 9am on the listed end days!</p> <p>After an outage, the first day or two back should be considered 'at risk'; that is, things are more likely to go wrong without warning and we might need to make adjustments.</p> Date Service Status Reason 7 April 2025 Myriad Planned Outage for switchover to new filesystem. No login access from 9am. Once access is restored later that day or the next day, you will have an empty new home and Scratch with symbolic links to <code>oldhome</code> and <code>oldscratch</code>. All jobs will be held. You will need to copy your data to the new filesystem or archive it. Once done, you can release the hold on your jobs using <code>qrls $JOB_ID</code> or <code>qrls all</code>."},{"location":"Planned_Outages/#previous-outages","title":"Previous Outages","text":"Date Service Status Reason 18 March 2025 Myriad, Kathleen Completed ACFS will be unavailable and all jobs on Myriad and Kathleen will be drained for 8am. This is to do some work on the ACFS that will allow archiving in future. Had to schedule for 18th not 11th based on vendor availability. Jobs that will not be able to complete before the outage will stay in the queue and will not start until after the outage. The ACFS is at risk all day. We'll let you know if the work is completed earlier. 11 February 2025 Myriad Completed Maintenance day: nodes being drained for a system config change. Also a reboot of ACFS switches will cause the ACFS mount on Myriad to hang for a period during the day. 28 January 2025 Young, Michael Completed /old_lustre will be unmounted. If you still have files in your old home and scratch directories, they will no longer be accessible after this time. 16 December 2024 Kathleen Completed /old_lustre will be unmounted at or shortly after 9am. (Moved from 11 Dec). If you still have files in your old home and scratch directories, they will no longer be accessible after this time. 4-8 November 2024 Myriad, Young Completed New drivers being deployed on the GPU nodes in a rolling fashion. No outage of all nodes at once. 8 October 2024 Kathleen Completed There will be a brief ACFS outage while we switch the network gateway from one util node to another. If all goes well it will recover quickly, but reading and writing to the ACFS will hang while it is in progress. We will do this on 8 Oct maintenance day. 7 October 2024 Young, Michael Completed Migration to new filesystem. Jobs will be drained and logins prevented for 9am. Expected to be back up the following day. Once back up, all jobs will be in <code>hqw</code> status and users will need to migrate their data from <code>/old_lustre/home/username</code> and <code>/old_lustre/scratch/username</code> which will be read-only. Users with accounts on both systems will need to log into each to see their old data, but will be copying it to the same shared home and Scratch. SSH keys will be copied to the new filesystem so login is possible. The two <code>/old_lustre</code> will remain for three months + 7 days and be removed on Tues 14 Jan 2025. [Logins enabled 8 Oct 13:30] 2-4 October 2024 Young, Michael Completed Young login02 and Michael login11 will each be out of service for a day during this period for testing updates before filesystem migration. No interruption to jobs or logins to the general addresses <code>young.rc.ucl.ac.uk</code> and <code>michael.rc.ucl.ac.uk</code>. 10 September 2024 Kathleen Completed Migration to new Lustre filesystem and mounting of ACFS (ARC Cluster File System) as the new backed-up location. Jobs will be drained for 8am and logins will be prevented from 9am. Expected to be back up the following day. Once back up, all jobs will be in <code>hqw</code> status and users will need to migrate their data from <code>/old_lustre/home/username</code> and <code>/old_lustre/scratch/username</code> which will be read-only. Home will no longer be backed up. <code>/old_lustre</code> will remain for three months and be removed on 11 Dec 2024. 11 June 2024 Young Completed Drain of one rack for 8am to physically install new hardware. The rest of the cluster will be operating as usual. The new hardware won't be available for use for some time - will still need testing and configuring. 10 June 2024 Michael Completed Full shutdown of Michael for network config alterations in preparation for new filesystem. Jobs will be drained for 8am. No logins and file access during outage. Expected to be back up later that day. 05 April -&gt; 10 May 2024 Young Completed The cabinets in Young are being re-cabled in turn. We plan for an eighth of Young to be out of service at a time, so queues may be longer due to fewer available nodes. This maintenance is essential for the longevity of Young. The rest of Young will be running jobs as usual. 9 April 2024 Myriad Completed Update of module version. Jobs to be drained for 8am, modules update to be done and then jobs begun again later in the day. 5 April -&gt; 8 April 2024 Kathleen, Young, Michael Completed Annual electrical testing in datacentre. No access to cluster from 15:00 on Fri 5 April to later afternoon on Mon 8 April and no jobs will be running. We will update Singularity to Apptainer on 8th on all three clusters. On Michael we will also update the module version (brought forward from 9 April). 21 March 2024 Kathleen, Young Completed Update of module version. Should not cause outage. 12 March 2024 Myriad Completed Full outage to fix inaccurate quota reporting. Jobs will be drained for 8am. From 9am no one will be able to log in and no files can be accessed. We do not know until we begin quotachecks whether they will take multiple days. (Completed on Weds 13 March 13:15) 14 Nov 2023 Myriad Completed Work on leaf switches in datacentre. Jobs will be drained for 8am. During the outage we intend to upgrade Singularity to Apptainer. We intend to restart jobs on the Weds 15 Nov. 29 Aug -&gt; 1 Sept 2023 Kathleen Completed Our vendors need us to do some filesystem testing on Kathleen to prepare for the replacement of its Scratch filesystem with a new filesystem later this year. We will drain jobs for 8am on Tues 29th. Depending on how long it takes to gather this information, we may not be running jobs for the rest of the week. We will update you on Thursday around lunchtime. (Jobs were restarted on Mon 4 Sept 12:00) 11 July 2023 Myriad Completed Full shutdown for replacement of both Lustre controllers in the filesystem. There will be no login access, no jobs running. Jobs will be drained for 8am. Myriad will be returned to service during the day on Weds 12 July. 10 -&gt; 15 May 2023 Myriad Completed Power work taking place in the datacentre that Myriad is in which may result in some cabinets losing power. We will be draining jobs for 8am on Weds 10 May and re-enabling them again once the work is complete, likely to be during Tues 16 May. 21 -&gt; 23 Apr 2023 Young, Michael, Kathleen Completed Electrical works in the datacentre require us to switch off all three clusters. Jobs will be drained for 12:00pm on Friday 21 April. The clusters will be powered down shortly after 12:00pm. You won't be able to log in or access data from then or throughout the weekend. We will bring the clusters back up during Monday 24 April. This may take a few hours to most of the day. 31 Jan 2023 -&gt; 1st Feb 2023 Myriad Completed Full shutdown of the system to perform firmware updates to the storage system, including all controllers and drives. Jobs will be drained for 08:00 on 31st. It is intended to make the system available again in the evening of the 1st. 23 Dec 2022 -&gt; 3-4 Jan 2023 Young, Michael, Kathleen Complete Full shutdown during UCL Christmas closing to prevent risk of filesystem damage from instability or power issues. Jobs will be drained for 9am on 23rd, shutting down at 1pm. Will be brought back up throughout 3 Jan with intent to have jobs running at some point on 4 Jan. 13 Dec 2022 -&gt; 14 Dec 2022 Myriad Postponed Upgrade of firmware on all Lustre storage disks. Jobs will be drained for 8am on Tues 13 Dec and we expect to re-enable jobs on Weds 14 Dec. Discovered that we would need to update the storage controller firmware as well, and possibly the OSSs (object store servers) and Lustre itself. We don't want to do this right before the Christmas closure so are postponing the update until later. 1 Nov 2022 -&gt; 2 Nov 2022 Myriad Complete Upgrade of firmware on all Lustre storage disks. No reading or writing of files will be possible during the upgrade interval. Jobs will be drained for 8am on Tues 1 Nov and we expect to re-enable jobs on Weds 2 Nov. 27 Sep 2022 -&gt; 28 Sep 2022 Myriad Completed Revised dates Moving switches. Jobs will be drained for 8am on 27 Sept and access to the login nodes disabled. Access should be restored during the day on Thurs 29. Myriad should be considered at risk for the rest of the week. 12 Apr 2022 -&gt; 19 Apr 2022 Kathleen, Young, Michael Completed Datacentre work. Clusters will be drained of jobs and access to the login nodes disabled, for 4:00pm Tuesday 12th April. Clusters will be brought back into service during the day on Tuesday 19th April. They should be considered AT RISK that day. 1 Apr 2022 -&gt; 4 Apr 2022 Kathleen, Young, Michael Completed Datacentre work. Clusters will be will be drained of jobs and access to the login nodes disabled, for 4pm Friday 1st April. Clusters will be brought back into service on Monday the 4th April. They should be considered AT RISK that day. 7 Jan 2022 -&gt; 10 Jan 2022 Kathleen, Young, Michael Completed Datacentre power outage. Jobs will be drained for 2pm on Fri 7. Login nodes will be powered down. Clusters will be brought back online during Mon 10, should be considered at risk that day. 19 Nov 2021 Myriad Completed One of our suppliers will be onsite to replace a component. Queues are being drained. 9 Nov 2021 Myriad Completed Continued work to improve the file system. Myriad will be unavailable for the whole day. Queues are being drained for 8am. 12 Oct 2021 Myriad Completed Filesystem load tests before expansion. No jobs will be running and access to login nodes will be blocked during the test, expected to finish by end of day. 29 Sept 2021 -&gt; 30 Sept 2021, 5 Oct 2021 Young Completed Extended: expected back in service midday Tues 5 Oct. Queues drained so ZFS can be upgraded to patch a bug causing metadata server issues. No jobs will be running. 14 Sept 2021 Myriad Planned Queues drained for 8am so that the central software installs can be redistributed over more servers. This is to help mitigate the effects of current filesystem issues. 03 Aug 2021 Young, Michael Completed Shortly after 8am: ~10 mins Gold outage for security updates. New job submission and Gold commands will not work during this. 12 July 2021 Myriad Completed Drain of d97a (Economics) nodes for 8am to update network config. May require less than the full day. Jobs will remain in queue until drain is over or run on other nodes if they can. 15 Jun 2021 -&gt; 18 Jun 2021 Myriad Completed Datacentre network outage. No jobs will run on the 15th, and Myriad will be at risk for the rest of the week. 14 Jun 2021 Kathleen, Young and Data Science Platform Completed Datacentre network outage 8.30-9am. No login access and no jobs will run. 08 Jun 2021 Thomas Completed Storage will be at risk during an essential controller reboot. 22 May 2021 -&gt; 24 May 2021 Michael Completed Datacentre network outage. Queues to be drained. Full outage, no access. 22 May 2021 -&gt; 24 May 2021 Young Completed Gold and job submission expected to be unavailable while database uncontactable due to datacentre network outage. 22 May 2021 -&gt; 24 May 2021 Myriad and Kathleen Completed Gold (i.e. high priority) job submission expected to be unavailable while database uncontactable due to datacentre network outage. 29 Mar 2021 -&gt; 30 Mar 2021 Myriad Completed A number of GPU nodes will be reserved for the NVidia AI Bootcamp on the 29th and 30th March. Some users may experience longer than usual waiting times for GPUs, from Monday the 22nd until after the event, especially for long jobs. We apologise for any inconvenience this may cause. 30 Mar 2021 Kathleen Completed Essential Lustre filesystem maintenance. Queue draining starts 26th March, SSH access to login nodes disabled on the 30th. Full outage, no access. 23 Feb 2021 8-9:30am Young Completed Gold and job submission expected to be unavailable for two 10-min periods while network switches upgraded. 09 Feb 2021 8-1pm Young Completed Gold and job submission outage while we migrate our database server. 08 Jan 2021 -&gt; 12 Jan 2021 Kathleen Completed Electrical work in datacentre. Logins delayed until Tues morning while tests completed. 08 Jan 2021 -&gt; 12 Jan 2021 MMM Michael Completed Electrical work in datacentre. Logins delayed until Tues morning while tests completed. 08 Jan 2021 -&gt; 12 Jan 2021 MMM Young Completed Electrical work in datacentre. Logins delayed until Tues morning while tests completed. 02 Jul 2020 -&gt; 09 Jul 2020 Michael Rollback Lustre software upgrade to fix bug. Full outage, no access. Upgrade was unusable, downgrading to previous. 01 May 2020 -&gt; 11 May 2020 Myriad Completed Storage upgrade. 20 Mar 2020 -&gt; 30 Mar 2020 Myriad Stopped, Postponed Issues found with new storage during outage. Myriad returned to service 24th. Sync data; switch to new storage. (Important: new metadata servers). Begins midday. Full outage, no access. 16 Mar 2020 -&gt; (was 23) 26 Mar 2020 Michael Completed Firmware upgrades to bridge old and new networks. Extended to get jobs working on new nodes. 2 Mar 2020 -&gt; 9 Mar 2020 Michael Completed Installation of phase 2 hardware, network bridging 10 Feb 2020 -&gt; 17 Feb 2020 Myriad Cancelled Storage upgrade to 3PB"},{"location":"Planned_Outages/#retirements","title":"Retirements","text":"<p>These clusters or services are being retired. Notable dates are below.</p>"},{"location":"Planned_Outages/#grace","title":"Grace","text":"<ul> <li>1st February 2021: Job submission will be switched off. Jobs still in the queue will run. Access to the login nodes will remain for three months so you can recover your data.</li> <li>3rd May 2021: Access to the login nodes will be removed and all data will be deleted.</li> </ul>"},{"location":"Planned_Outages/#mmm-hub-thomas","title":"MMM Hub Thomas","text":"<p>This service has been retired as part of the MMM Hub. Some remains are still running temporarily for UCL users only.</p> <ul> <li>Monday 1 March 2021: Job submission for MMM Hub users will be switched off this morning.  Jobs already in the queue may still run. Access to the login nodes will remain for one month  so you can retrieve data.</li> <li>Friday 5 March 2021: Queues will be drained. Any jobs left will never run.</li> <li>Thursday 1 April 2021: Access to the login nodes for MMM Hub users will be removed and all data will be deleted.</li> </ul>"},{"location":"Planned_Outages/#rstudio-rstudiorcuclacuk","title":"RStudio (rstudio.rc.ucl.ac.uk)","text":"<ul> <li>16 May 2022: The old RStudio service that was at rstudio.rc.ucl.ac.uk  will be decommisioned  and users will no longer be able to login to it.</li> </ul>"},{"location":"Remote_Access/","title":"Remote Access to Research Computing Resources","text":"<p>UCL's Research Computing services are accessible from inside the UCL firewall.  If you wish to connect from outside, you need to either connect through a VPN or use SSH to log in to a machine accessible from outside and use that to \"jump\" through into the UCL network.</p>"},{"location":"Remote_Access/#connecting-to-the-jump-boxes","title":"Connecting to the jump boxes","text":"<p>You can connect to the jump boxes by connecting with your SSH client to:</p> <pre><code>ssh-gateway.ucl.ac.uk\n</code></pre> <p>Once connected you can then log on to the UCL RC service you are using as normal.</p> <p>You can configure your ssh client to automatically connect via these jump boxes so that you make the connection in one step.</p>"},{"location":"Remote_Access/#single-step-logins-using-tunnelling","title":"Single-step logins using tunnelling","text":""},{"location":"Remote_Access/#linux-unix-macos","title":"Linux / Unix / macOS","text":""},{"location":"Remote_Access/#on-the-command-line","title":"On the command line","text":"<pre><code># Log in to Kathleen, jumping via jump box\n# Replace ccxxxxx with your own username.\nssh -o ProxyJump=ccxxxxx@ssh-gateway.ucl.ac.uk ccxxxxx@kathleen.rc.ucl.ac.uk\n</code></pre> <p>or</p> <pre><code># Copy 'my_file', from the machine you are logged in to, into your Scratch on Kathleen\n# Replace ccxxxxx with your own username.\nscp -o ProxyJump=ccxxxxx@ssh-gateway.ucl.ac.uk my_file ccxxxxx@kathleen.rc.ucl.ac.uk:~/Scratch/\n</code></pre> <p>This tunnels through the jump box service in order to get you to your destination - you'll be asked for your password twice, once for each machine. You can use this to log in or to copy files.</p> <p>You may also need to do this if you are trying to reach one cluster from another and there is a firewall in the way.</p> <p>Useful resources can be found here:</p> <ul> <li>SSH key pair pt 1 (moodle) (UCL users)</li> <li>SSH key pair pt 2 (moodle) (UCL users)</li> <li>SSH key pair pt 1 (mediacentral) (non-UCL users)</li> <li>SSH key pair pt 2 (mediacentral) (non-UCL users)</li> </ul>"},{"location":"Remote_Access/#using-a-config-file","title":"Using a config file","text":"<p>You can create a config which does this without you needing to type it every time.</p> <p>Inside your <code>~/.ssh</code> directory on your local machine, add the below to your <code>config</code> file (or create a file called <code>config</code> if you don't already have one).</p> <p>Generically, it should be of this form where <code>&lt;name&gt;</code> can be anything you want to call this entry. You can use these as short-hand names when you run <code>ssh</code>.</p> <pre><code>Host &lt;name&gt;\n   User &lt;remote_user_id&gt;\n   HostName &lt;remote_hostname&gt;\n   proxyCommand ssh -W &lt;remote_hostname&gt;:22 &lt;remote_user_id&gt;@ssh-gateway.ucl.ac.uk\n</code></pre> <p>This <code>proxyCommand</code> option causes the commands you type in your client to be forwarded on over a secure channel to the specified remote host.</p> <p>On newer versions of OpenSSH, you can use <code>ProxyJump &lt;remote_user_id&gt;@ssh-gateway.ucl.ac.uk</code>  instead of this <code>proxyCommand</code> line.</p> <p>Here are some examples - you can have as many of these as you need in your config file.</p> <pre><code>Host myriad\n   User ccxxxxx\n   HostName myriad.rc.ucl.ac.uk\n   proxyCommand ssh -W myriad.rc.ucl.ac.uk:22 ccxxxxx@ssh-gateway.ucl.ac.uk\n\nHost kathleen01\n   User ccxxxxx\n   HostName login01.kathleen.rc.ucl.ac.uk\n   proxyCommand ssh -W login01.kathleen.rc.ucl.ac.uk:22 ccxxxxx@ssh-gateway.ucl.ac.uk\n\nHost aristotle\n   User ccxxxxx\n   HostName aristotle.rc.ucl.ac.uk\n   proxyCommand ssh -W aristotle.rc.ucl.ac.uk:22 ccxxxxx@ssh-gateway.ucl.ac.uk\n</code></pre> <p>You can now just type <code>ssh myriad</code> or <code>scp file1 aristotle:~</code> and you will go through the jump box. You'll be asked for login details twice since you're logging in to two machines, the jump box and your endpoint.  </p>"},{"location":"Remote_Access/#file-storage-on-the-gateway-servers","title":"File storage on the Gateway servers","text":"<p>The individual servers in the pool for the Gateway service have extremely limited file storage space, intentionally, and should not be used for storing files - if you need to transfer files you should use the two-step process above.  This storage should only be used for SSH configuration files.</p> <p>This storage is not mirrored across the jump boxes which means if you write a file to your home directory, you will not be able to read it if you are allocated to another jump box next time you log in.</p>"},{"location":"Remote_Access/#key-management","title":"Key management","text":"<p>Warning</p> <p>If you use SSH keys you absolutely MUST NOT STORE UNENCRYPTED PRIVATE KEYS ON THIS OR ANY OTHER MULTI-USER COMPUTER.  We will be running regular scans of the filesystem to identify and then block unencrypted key pairs across our services.</p> <p>There are currently two servers in the pool, internally named <code>ejp-gateway01</code> and <code>ejp-gateway02</code>. </p> <p>Because the <code>/home</code> filesystem is not shared across the jump boxes, you need to sync SSH configuration files like <code>~/.ssh/authorized_keys</code> across all the available jump boxes so that the change takes effect whichever jump box you are allocated to.</p> <p>You can see which machine you are logged into by the bash prompt.</p> <p>So for example, if on <code>ejp-gateway02</code> then do:</p> <pre><code>[ccaaxxx@ad.ucl.ac.uk@ejp-gateway02 ~]$ scp -r ~/.ssh ejp-gateway01:\n\nPassword:\nknown_hosts 100% 196 87.1KB/s 00:00\nauthorized_keys 100% 0 0.0KB/s 00:00\n[ccaaxxx@ad.ucl.ac.uk@ejp-gateway02 ~]$\n</code></pre> <p>and similarly if on <code>ejp-gateway01</code> do <code>scp -r ~/.ssh ejp-gateway02:</code></p>"},{"location":"Status_page/","title":"Status of machines","text":"<p>This page outlines that status of each of the machines managed by the Research Computing team at UCL. We endeavour to keep this page as up-to-date as possible but there might be some delay. Also there are spontaneous errors that we have to deal with (i.e. high load on login nodes) but feel free to report them to rc-support@ucl.ac.uk. Finally, details of our planned outages can be found here.  </p>"},{"location":"Status_page/#myriad","title":"Myriad","text":"<ul> <li> <p>2023-03-06 - Myriad's filesystem is getting full again, which will impact performance. If you are      able, please consider backing up and deleting any files that you aren't actively using for your      research for the time being.</p> <p>You can check your quota and see how much space you are using on Myriad with the <code>lquota</code> command,  and you can see which directories are taking the most space using the <code>du</code> command, which can also  be run in specific directories. You can tell <code>du</code> to only output details of the first level of  directory sizes with the <code>--max-depth=1</code> option.</p> <p>Jobs are still running for the moment, but if the filesystem keeps getting fuller we may need to  stop new jobs running until usage is brought down again.</p> </li> <li> <p>2023-07-31 12:30 - Myriad's filesystem is currently being very slow because we have two failed      drives in the Object Store Target storage. One drive has fully failed and the volume is under      reconstruction. The other has been detected as failing soon and is being copied to a spare, but     this is happening slowly, likely because the disk is failing.</p> <p>They are in separate volumes so there isn't a risk there, but it can take some time for  reconstruction to complete and this will make the filesystem sluggish. This will affect you  accessing files on the login nodes, and also your jobs reading or writing to files on Scratch  or home, and accessing our centrally-installed software. (It won't affect writing to <code>$TMPDIR</code>, the local hard disk on the compute node).</p> <p>Once the reconstruction is complete, performance should return to normal. This could take most  of the day and potentially continue into tomorrow.</p> </li> <li> <p>2023-08-01 11:00 - We have had a report of another impending disk failure in the same volume as     the first failed disk, which puts data at risk. The volume reconstruction is still in progress      and expected to take another 30-odd hours. (The second copy to spare has completed).</p> <p>We have stopped Myriad from running jobs to reduce load on the filesystem while the  reconstruction completes, so no new jobs will start for the time being. We'll keep you updated  if anything changes.</p> </li> <li> <p>2023-08-03 10:30 - We have another estimated 18 hours of volume reconstruction to go, so we are     leaving jobs off today. On Friday morning we will check on the status of everything and      hopefully be able to re-enable jobs then if all is well.</p> </li> <li> <p>2023-08-04 10:00 - The reconstruction is complete and we have re-enabled jobs on Myriad.</p> </li> <li> <p>2023-11-23 14:00 - We need to stop new jobs running on Myriad while its filesystem recovers.</p> <p>This means that jobs that are already running will keep running, but new jobs will not start  until we enable them again. The filesystem is likely to be slower than usual. You can still log  in and access your files.</p> <p>This is because we've just had two disks fail in the same area of Myriad's filesystem and the  draining of jobs is to reduce usage of the filesystem while it rebuilds onto spare disks.</p> <p>The current estimated time for the disk rebuild is 35 hours (but these estimates can be  inaccurate).</p> <p>We will update you when new jobs can start running again.</p> </li> <li> <p>2023-11-24 09:45 - Myriad filesystem is down. </p> <p>I'm afraid Myriad's filesystem is currently down. You will not be able to log in and jobs are  stopped.</p> <p>A third disk failed in the same area as the last two and we started getting errors that some  areas were unreadable and reconstruction failed. We need to get this area reconstructed so are  in contact with our vendors.</p> <p>Detail</p> <p>Our parallel filesystem is Lustre. It has Object Storage Targets that run on the Object  Storage Servers. Data gets striped across these so one file will have pieces in multiple  different locations, and there is a metadata server that keeps track of what is where. One of  the Object Storage Targets has lost three disks, is unreadable and is what needs to be  reconstructed. While it is unavailable, the filesystem does not work. We will also have some  amount of damaged files which have pieces that are unreadable.</p> <p>If the reconstruction succeeds, we will then need to clear out the unreadable sectors. Then we  will be able to start checking how much damage there is to the filesystem.</p> <p>This is all going to take some time. We will update you again on Monday by midday, but there  may be no new information by then.</p> <p>I'm sorry about this, we don't know right now how long an interruption this will cause to  service, or how much data we may have lost.</p> <p>Please send any queries to rc-support@ucl.ac.uk </p> </li> <li> <p>2023-11-27 12:10 - There is not much of an update on Myriad's filesystem yet - we've been in      contact with our vendors, have done some things they asked us to (reseating hardware) and are      waiting for their next update.</p> <p>In the meantime, we are starting to copy the backup of home that we have from the backup system where it is very slow to access to somewhere faster to access, should we need to recover files  from it on to Myriad.</p> <p>We received a question about backups - the last successful backup for Myriad home directories  ran from Nov 20 23:46 until Nov 22 03:48. Data before that interval should be backed up. Data  after that is not. Data created or changed during it may be backed up, it depends on which  users it was doing when.</p> <p>(Ideally we would be backing up every night, but as you can see the backup takes longer than a  day to complete, and then the next one begins. The next backup had started on Nov 22 at 08:00  but did not complete before the disk failures).</p> </li> <li> <p>2023-11-29 11:30 Today's Myriad filesystem update:</p> <ul> <li> <p>We are continuing to meet with our vendor and have sent them more data to analyse.</p> </li> <li> <p>We are going to copy over the failed section of filesystem to a new area, with some data    loss, so we end up with a working volume that we can use to bring the whole filesystem back    up.</p> </li> <li> <p>The copy of data from our backup location to easier access location is ongoing (we have    48TiB of home directories consisting of 219M items, currently copied 15TiB 7.5TiB and 27M    items).</p> </li> </ul> <p>Date of next update: Monday 4 Dec by midday.</p> <p>Myriad will definitely not be back up this week, and likely to not be back up next week - but  we'll update you on that. The data copying all takes a lot of time, and once it is there, we  need to run filesystem checks on it which also take time.</p> </li> <li> <p>2023-12-04 12:30 - The data copying is continuing.</p> <ul> <li> <p>The copy of the failed section of filesystem to a new area is at 33% done, expected to take    4.5 days more.</p> </li> <li> <p>The copy of home directories from our backup location to easier access location is at    44TiB 22TiB (GPFS was reporting twice the real disk usage)    and 86M items out of 48TiB and 219M. It is hard to predict how long that may take, as the    number of items is the bottleneck. It has been going through usernames in alphabetical order    and has reached those starting \"ucbe\".</p> </li> </ul> <p>If the first of these copies completes successfully, we will then need to go through a  filesystem check. This will also take possibly a week or more - it is difficult to predict  right now. If this recovery continues as it has been and completes successfully, it looks like  we may have lost a relatively small proportion of data - but we cannot guarantee that at this  stage, and the copy has not completed.</p> <p>So at present we expect to be down at least all of this week, and all of next week - and  possibly into the week of the 18th before UCL's Christmas closing on the 22nd.</p> <p>I'm sorry about this - with these amounts of data and numbers of files, it gets difficult to  predict how long any of these operations will take.</p> <p>We've had a couple of questions about whether there is any way people can get their data from  Myriad - until the filesystem is reconstructed and brought back up, we cannot see separate  files or directories on Myriad.</p> </li> <li> <p>2023-12-12 15:00 - First data copy complete.</p> <ul> <li> <p>The copy of the failed section of filesystem has completed and is showing at 99.92% rescued,    leaving around 75GB of data as not recoverable. (At present we don't know what this data is -   it could be from all of home, Scratch, shared spaces and our application install area).</p> </li> <li> <p>The copy of home directories from our backup location to easier access location is still    going, currently at 42.5TiB and 166M items out of 48TiB and 219M. My earlier reports had    doubled the amount we had successfully copied, so when I said 44TiB previously, that was only   22TiB. (Corrected in this page). This is progressing slowly and has stalled a couple of times.</p> </li> </ul> <p>We next need to bring the filesystem up so we can start running checks and see what files are  damaged and missing. We will be discussing our next steps to do this with our vendor. </p> <p>I intend to send another update this week, before the end of Friday. </p> </li> <li> <p>2023-12-15 17:20 - No news, Christmas closing.</p> <p>No news, I'm afraid. It is looking like Myriad's filesystem will definitely remain down over  Christmas.</p> <ul> <li> <p>Our vendors are investigating filesystem logs.</p> </li> <li> <p>The copy of home directories from our backup location to easier access location appears to    have finished the copying stage but was still doing some cleanup earlier today.</p> </li> </ul> <p>UCL is closed for Christmas from the afternoon of Friday 22 December until 9am on Tuesday 2  January. Any tickets received during this time will be looked at on our return.</p> <p>We will send an update next week before UCL closes.</p> </li> <li> <p>2023-12-18 15:30 - Home directories from backup available</p> <p>We have restored Myriad HOME directories only (no Scratch, Projects or Apps) from the most recent back up which ran from:</p> <p>Monday Nov 20 23:46 to Wednesday Nov 22 03:48</p> <p>They are mounted READ ONLY on the Myriad login nodes so you can login to check what files are missing or need updating and scp results etc back to your local computer. We apologize for the delay in making this data available, unfortunately the restore process was only finished during the weekend.</p> <p>Work on restoring more data (i.e. HOME from after the backup, as well as Scratch and Projects) is still in progress.</p> <p>It is currently not possible to run jobs.</p> <p>We still don't expect Myriad to be restored to service before the Christmas and New Year UCL closure.</p> <p>UCL is closed for Christmas from the afternoon of Friday 22 December until 9am on Tuesday 2  January. Any tickets received during this time will be looked at on our return.</p> </li> <li> <p>2023-12-22 15:00 - A final Myriad update before UCL closes for the Christmas and New Year break.</p> <p>The copy of rescued data back onto the re-initialised volume completed this morning (Friday  22nd). We are now running filesystem checks. Myriad will remain down during the Christmas and  New Year closure apart from the read only HOME directories as detailed previously.</p> </li> <li> <p>2024-01-05 16:50 - An update on the status of Myriad before the weekend</p> <p>We wanted to give you a quick update on the progress with Myriad before the weekend as we  know several of you are asking for one.</p> <p>We are meeting on Monday morning to consider options for returning the live filestore  including Apps, HOME, Scratch and projects to service. We should have some rough timescale we  can give you later on Monday.</p> <p>Currently a scan is running to discover which files existed either wholly or in part on the  failed volume. So far this has discovered around 60M files, and the scan is about halfway. This  will carry on running over the weekend. Unfortunately, there is likely to be significant data  loss from your Scratch directories.</p> <p>We will send another update later on Monday.</p> </li> <li> <p>2024-01-08 17:50 - Myriad update (Monday)</p> <p>We met this morning to discuss options for returning the live filestore including Apps, HOME,  Scratch and projects to service. Tentatively we hope to be able to allow you access to your  HOME, Scratch and projects by the end of this week.</p> <p>The scan to discover which files existed either wholly or in part on the failed volume has  completed and found about 70M files which is around 9.1% of the files on Myriad. We are  planning to put files in your HOME directory:</p> <p>One listing the missing files from your HOME directory.</p> <p>One listing the missing files from your Scratch directory.</p> <p>There are also missing files in projects so if you own a project we will give you a list of  these too.</p> </li> <li> <p>2024-01-11 12:30 - Jobs on Myriad</p> <p>We've had questions from you about when jobs will be able to restart. We were able to assess  the damage to our software stack yesterday and most of the centrally installed applications  are affected by missing files and need to be copied over from other clusters or reinstalled.</p> <p>We're going to begin by copying over what we can from other systems. We'll be looking at the  results of this first step and seeing how much is still missing after it and how fundamental  those packages are. </p> <p>It would be possible for us to enable jobs before the whole stack was reinstalled, but we need  enough there for you to be able to carry out useful work. We should have a much better idea of  what is still missing by Monday and our plans for reinstating it. I would rather lean towards  giving you access sooner with only the most commonly-used software available rather than  waiting for longer.</p> <p>We're on schedule for giving you access to your files by the end of this week.</p> <p>New user accounts will start being created again once jobs are re-enabled.</p> <p>I'll also be sending an update in the next few days about our future filesystem plans and  mitigations we were working on before this happened.</p> </li> </ul>"},{"location":"Status_page/#check-lists-of-damaged-files-on-myriad","title":"Check lists of damaged files on Myriad","text":"<ul> <li> <p>2024-01-12 14:00 Myriad: filesystem access restored, jobs tentatively expected for Monday</p> <p>We've restored read-write access to Myriad's filesystem, and you will be able to log in and  see all your directories again. Here's some detail about how you can identify which of your  files were damaged.</p> <p>Your data on Myriad</p> <p>During the incident all files that resided at least partially on OST00 have been lost.  In total these are 70M files (out of a filesystem total of 991M).</p> <p>We restored files in <code>/lustre/home</code> from the latest backup where available. Data that was  created while the backup was running or afterwards had no backup and could not be restored.  For the time being, we will keep the latest backup available read-only in the directory  <code>/lustre-backup</code>.</p> <p>Files in <code>/lustre/scratch</code> and <code>/lustre/projects</code> were not backed up, as a matter of policy.  All OST00 files from these directories have been lost.</p> <p>Where files were lost but still showing up in directory listings, we have removed (\"unlinked\")  them so it doesn't appear that they are still there when they are not.</p> <p>For a tiny fraction of the lost files (0.03%), there is still some file data accessible, but  most of these files are damaged (e.g. truncated or partially filled with zeroes). For some  users these damaged files might still contain useful information, so we have left these files  untouched.</p> <p>The following files have been placed into your home directory:</p> <ul> <li> <p>OST00-FILES-HOME-restored.txt</p> <ul> <li>A list of your home directory files that resided on OST00 and that were successfully   restored from backup.</li> </ul> </li> <li> <p>OST00-FILES-HOME-failed.txt</p> <ul> <li>A list of your home directory files that resided on OST00 and that could not be restored     from backup, including one of the following messages:</li> <li>\"no backup, stale directory entry, unlinked\" - There was no backup for this file, and we     removed the stale directory entry.</li> <li>\"target file exists, potentially corrupt, leaving untouched\" - Original file data is still     accessible, but likely damaged or corrupt. Feel free to delete these files if there's no     useful data in there.</li> </ul> </li> <li> <p>OST00-FILES-SCRATCH.txt</p> <ul> <li>A list of your Scratch directory files that resided on OST00, including one of the     following messages (similar to the above):</li> <li>\"stale directory entry, unlinked\"</li> <li>\"file exists, potentially corrupt, leaving untouched\"</li> </ul> </li> </ul> <p>For projects, the following file has been placed in the project root directory:</p> <ul> <li>OST00-FILES-PROJECTS.txt<ul> <li>A list of project files that resided on OST00, including one of the following messages:</li> <li>\"stale directory entry, unlinked\"</li> <li>\"file exists, potentially corrupt, leaving untouched\"</li> </ul> </li> </ul> <p>A very few users had newline characters (<code>\\n</code>) in their filenames: in this case in the above  .txt files the <code>\\n</code> has been replaced by the string <code>__NEWLINE__</code>, and an additional .bin file  has been placed alongside the .txt file, containing the list of original filenames terminated  by null bytes (and not including the messages).</p> <p>These OST00-FILES-* files are owned by root, so that they don't use up any of your quota.  You can still rename, move, or delete these files.</p> <p>Jobs</p> <p>We're currently running a Lustre filesystem check in the background. Provided it does not throw  up any serious problems, we expect to be able to re-enable jobs during Monday. We'll be putting user holds on all the jobs so you can check that the files and applications they are trying to  use exist before allowing them to be scheduled. They will show in status <code>hqw</code> in qstat.</p> <p>Once you have made sure they are ok, you will be able to use <code>qrls</code> followed by a job ID to  release that job, or <code>qrls all</code> to release all your jobs. They will then be in status <code>qw</code> and  queue as normal. (Array jobs will have the first task in status <code>qw</code> and the rest in <code>hqw</code> -  this is normal). If you want to delete the jobs instead, use <code>qdel</code> followed by the job ID.</p> <p>Software</p> <p>We have successfully restored the vast majority of Myriad's software stack. I'll send a final  update when we re-enable jobs, but at present I expect the missing applications to be:</p> <ul> <li>ABAQUS 2017</li> <li>ANSYS (all versions)</li> <li>STAR-CCM+ (all versions)</li> <li>STAR-CD (all versions)</li> </ul> <p>These are all licensed applications that are best reinstalled from their original media, so  we'll be working through those, starting with the most recent version we had.</p> <p>Please send any queries to rc-support@ucl.ac.uk. If you've asked us for account deletions, we  will be starting those next week, along with new user account creations.</p> </li> <li> <p>2024-01-15 16:00 - Myriad jobs enabled</p> <p>We have now allowed jobs to start on Myriad. </p> <p>We have reinstalled ABAQUS 2017, ANSYS 2023.R1, STAR-CCM+ 14.06.013 and STAR-CD 4.28.050. The  older versions of these applications are still missing at the moment.</p> <p>Your jobs that were still in the queue from before have user holds on them and show in status  <code>hqw</code> in qstat.</p> <p>Once you have made sure the files and applications the jobs are using are present and correct,  you will be able to use <code>qrls</code> followed by a job ID to release that job, or <code>qrls all</code> to  release all your jobs. They will then be in status <code>qw</code> and queue as normal. (Array jobs will  have the first task in status <code>qw</code> and the rest in <code>hqw</code> - this is normal). If you want to  delete the jobs instead, use <code>qdel</code> followed by the job ID.</p> <p>User deletions and new user creations are underway. We'll need to check that the  synchronisation to the mailing list is working correctly and people are being added and removed as appropriate.</p> </li> </ul>"},{"location":"Status_page/#action-required-compression-or-removal-of-data","title":"Action required - compression or removal of data","text":"<ul> <li> <p>2024-10-18 13:55 - Action required: compression or removal of data on Myriad</p> <p>Myriad's filesystem is too full, so we need everyone to check what data they are keeping on the  cluster and to remove data they are not currently using. To perform effectively, the filesystem  needs a significant portion of empty space. As it gets fuller, performance begins to get worse  and then stability also decreases. </p> <p>The filesystem is at 70% full, and we will need to stop jobs running when it reaches 75%. (1% of  the filesystem is 19.4 TiB).</p> <p>Keeping data unnecessarily on the system affects everyone's ability to run jobs.</p> <p>We will also be contacting those of you with large amounts of data separately.</p> <p>How to check usage and compress data</p> <p>You can check your quota and see how much space you are using on Myriad with the lquota command,  and you can see which directories are taking the most space using <code>du -h</code> which can also be run  in specific directories. You can tell du to only output details of the first level of directory  sizes with the <code>--max-depth=1</code> option.</p> <p>If you cannot remove your data from the cluster, please consider whether you can archive and  compress any of it for the time being.</p> <p>Example to tar up and gzip compress a directory:</p> <ul> <li><code>tar -czvf /home/username/Scratch/myarchive.tar.gz /home/username/Scratch/work_data</code>    will (c)reate a g(z)ipped archive (v)erbosely in the specified (f)ilename location. The    contents will be everything in this user's <code>work_data</code> directory.</li> </ul> <p>You can request bzip2 compression instead with <code>-j</code> or <code>--bzip2</code> and <code>xz</code> compression with <code>-J</code>  or <code>--xz</code>. These will give you better compression than gzip, with xz generally being the most  compressed.</p> <p>If you are compressing an individual file rather than a directory, you can use the <code>gzip</code>,  <code>bzip2</code> and <code>xz</code> commands on their own without tar.</p> <p>(Have a look at <code>man tar</code> or gzip, bzip2 and xz for the manual page details - they also contain  the names of the uncompress commands).</p> <p>If you are transferring data to Windows and want to uncompress the data there, 7-zip can open  all of these formats. Or you can create your archives using the <code>zip</code> command.</p> <p>Quota policies</p> <p>We are going to be adjusting the policies for granting Scratch quota increases - the CRAG will  be granting them for shorter periods of time and will not be as easily re-granting increases at  the end of that time period. We will have a process for what happens when a quota increase  expires, including notifications to you. You will be asked to consider your plans for what to  do with the data when the quota increase expires at the time of requesting one.</p> <p>We are also going to be setting up annual account reapplications again, so we can identify  users who are no longer using the system and activate our account deletion policies.</p> <p>This is to prevent the current situation where new users to the system cannot get relatively  small and short term quota increases necessary for their work because increases granted in the  past are still using the space.</p> <p>ARC Cluster File System (ACFS)</p> <p>Timelines for access to the ARC Cluster Filesystem (ACFS) from Myriad are currently being  considered. There is some info about the ACFS at  ARC Cluster File System </p> <p>The ACFS once available from Myriad will give you 1T in total of backed-up data (and your home  directories will no longer be backed up). For those of you with larger Scratch quota increases,  you will still need to consider what you will do with the bulk of your data. </p> <p>Info</p> <p>We recently added some pages to our documentation which may be relevant:</p> <ul> <li>Parallel Filesystems includes    sections on working effectively with parallel filesystems and tips for use.</li> <li>Data Storage - what storage locations exist.</li> <li>Data Management - checking quotas, transferring    data to other users, requesting data from those who have left.</li> </ul> <p>Please email rc-support@ucl.ac.uk with any queries or raise a request about Myriad via  UCL MyServices. If you are no longer using the cluster and  wish to be removed from this mailing list, please also contact us and say we can delete your account.</p> </li> <li> <p>2024-10-25 18:20 - Filesystem issues on Myriad</p> <p>We have been having some filesystem issues since around 16:50.</p> <p>One of the servers that is part of the filesystem kept crashing, we had a failover and then the new  active one crashed as well. This will be preventing logins and jobs will have failed.</p> <p>Depending on what is happening, we may not be able to sort this out until Monday and if access is  restored it may be unstable throughout the weekend.</p> </li> <li> <p>2024-10-28 10:20 - Filesystem recovered</p> <p>The filesystem recovered on Friday evening and was running ok over the weekend. </p> <p>While it was down, it is likely that many running jobs failed with I/O errors. There will also be  numbers of them in state <code>dr</code> which we need to reboot the nodes to clear.</p> <p>At the moment we're still seeing some leftover activity in the logs but there don't appear to be any  active problems.</p> </li> <li> <p>2024-11-12 17:20 - Myriad filesystem and refresh news; ACFS available now</p> <p>The ARC Cluster File System (ACFS) is now available on Myriad.</p> <p>The ARC Cluster File System (ACFS) is ARC's centralised storage system that will be available from  multiple ARC systems. If you also have a Kathleen account and already put some data in the ACFS, you  will now be able to see that data on Myriad too.</p> <p>It is the backed-up location for data which you wish to keep.</p> <p>The ACFS is available read-write on the login nodes but read-only on the compute nodes. This means  that your jobs can read from it, but not write to it, and it is intended that you copy data onto it  after deciding which outputs from your jobs are important to keep.</p> <ul> <li>Location: <code>/acfs/users/&lt;username&gt;</code></li> <li>Also at: <code>/home/&lt;username&gt;/ACFS</code> (a shortcut or symbolic link to the first location).</li> <li>Backed up daily.</li> <li>1T quota - no quota increases available on ACFS at present.</li> <li>Check your ACFS quota with <code>aquota</code>.</li> </ul> <p>The ACFS has dual locations for resilience, and as a result commands like <code>du -h</code> or <code>ls -alsh</code> will  report filesizes on it as being twice what they really are. The <code>aquota</code> command will show you real  usage and quota.</p> <ul> <li>Data Storage - ACFS</li> <li>Data Management - ACFS quotas</li> </ul> <p>Until Myriad's filesystem is replaced, your home will continue to be backed up. After the new  filesystem is available, only the ACFS will be backed up.</p> <p>Please move some of your current data to the ACFS to reduce the usage on Myriad's Scratch.</p> <p>Myriad refresh and new filesystem</p> <p>The Myriad refresh is going ahead, and we will be adding some new compute nodes to Myriad as well  as replacing the filesystem, networking and admin nodes.</p> <p>We expect the new filesystem to go live during March 2025. Prior to that, we will be conducting  setup, tuning and testing behind the scenes. (We expect the hardware to arrive in December or first  week of January depending on delivery windows).</p> <p>We expect the new compute nodes to also go live during March 2025 - they will be purchased and  arrive later than the filesystem, but their testing period should coincide.</p> <p>The new filesystem will be GPFS. The new compute nodes will be AMD EPYC 64 core, ~750G RAM, with  ~950G local disk for tmpfs. Some of the oldest nodes will be removed (eg types H, I, J which are  nodes named node-h, node-i, node-j).</p> <p>Timescales are based on our current knowledge and best estimates!</p> <p>There will be many software changes as part of this refresh as well - we will be updating the  operating system, changing the scheduler to Slurm and refreshing the software stack. More details  nearer the time.</p> <p>Action required - compression or removal of data</p> <p>Myriad's filesystem is still at 70% full and we need to stop jobs at 75% full. Please continue to  look at our previous message with full info and  move data to the ACFS, off-cluster, or compress it if possible.</p> </li> <li> <p>2025-02-10 - OS, software stack, scheduler and hardware update news</p> <p>This is to keep you informed about upcoming changes to Myriad.</p> <p>Live now:</p> <ul> <li>Research Data Storage Service mount point on login nodes</li> <li>Open OnDemand pilot access available on request until OS upgrade</li> </ul> <p>If you have an RDSS project, you can now access that read-write on the Myriad login nodes at <code>/rdss</code>.  There are subdirectories <code>rd00</code>, <code>rd01</code> that contain projects with names beginning  <code>ritd-ag-project-rd00</code> or <code>ritd-ag-project-rd01</code> and so on. You cannot access it from the compute  nodes. You should <code>cp</code> or <code>rsync</code> data to your home, Scratch or the ACFS via the login nodes before  doing any computation using the data.</p> <p>If you want to access our Open OnDemand pilot  for a remote desktop, Jupyter Notebooks or RStudio, contact rc-support@ucl.ac.uk and we will give you  access. Please fill in the feedback survey if you use it. This will be used to prioritise whether we  redo the work to set it up after the OS upgrade or concentrate on other improvements.</p> <p>Incoming:</p> <ul> <li>OS upgrade to RHEL 9.5</li> <li>Scheduler move to Slurm</li> <li>New software via Spack</li> <li>Alteration to hardware refresh schedule</li> </ul> <p>We will be moving to RHEL 9.5 as our operating system and Slurm as our new scheduler. The new  filesystem below will be available before this is rolled out on Myriad.</p> <p>If you have access to paid priority time on Myriad, we will stop using Gold for this and use Slurm  mechanisms for priority allocations instead. You will receive the equivalent resource.</p> <p>Our user documentation will be updated with examples of Slurm jobscripts instead of the SGE ones we  currently have. Slurm uses <code>srun</code> and <code>sbatch</code> commands instead of <code>qsub</code>, if you have come across  those in other software documentation.</p> <p>The OS upgrade should allow you to run more recent binaries that currently give errors about GLIBC  being too old. System tools will be newer and may look a little different.</p> <p>We are also creating a new small software stack built with Spack. This will available to you to test  before the OS upgrade, then rebuilt again after it, so it will change slightly in the meantime. Do  let us know if applications you use in it are missing options we currently have or not working as  you expect. </p> <p>Most of the existing software stack will be reinstalled for the new OS. We are looking to remove  some of the oldest installs and modules that are not being used in jobs. We then intend to prune  this further over time and add newer versions into the Spack stack.</p> <p>Alteration to hardware refresh schedule</p> <p>The oldest nodetypes (H,I,J) in Myriad have been drained of jobs as planned and are being removed  to make space in the racks.</p> <p>The new Myriad filesystem is in place, undergoing its final testing period and we should have  information on 24 Feb for timescales when you can expect to begin using it.</p> <p>We are still adding new hardware to Myriad for general use, but it will not be added in March/April  as previously stated. Instead, we will be replacing all Myriad's network hardware first, and then  getting the new standard compute nodes in the next UCL financial year after August 2025. The specs  for the new compute nodes are:</p> <ul> <li>AMD EPYC 9554P 64C 360W 3.1GHz (64 cores)</li> <li>768G RAM</li> <li>2x 480GB M.2 7450 PRO NVMe SSD (960G local disk)</li> </ul> <p>Note, this does not affect timelines for any paid nodes and those can still go in before this. It  is to split the purchase over financial years rather than any issue with hardware supply times.</p> <p>There is some additional hardware that we may be able to make available to increase general use  Myriad capacity in the intervening period. The details need working out and that will take a few  months. The OS and scheduler upgrade will need to happen first.</p> <p>Maintenance day Tues 11 Feb</p> <p>The nodes are being drained for a system config change this maintenance day - they'll be rebooted  and jobs restarted after each is updated. There will also be a reboot of switches in the ACFS that  will cause the ACFS mount on Myriad to hang for a period during the day. This is listed at  Planned Outages</p> <p>Documentation links</p> <p>The SSL certificate for www.rc.ucl.ac.uk is due to expire at midnight on 12 Feb. We're getting a  new one but there might be a gap if it can't be renewed in time. If that happens your browser may  prevent you accessing that link because the certificate is expired.</p> <ul> <li>mkdocs-rc-docs on Github will work</li> </ul> <p>If there is a gap when the certificate expires we'll update the links in the message you see when  you log in to the cluster, but if you are using an existing link or bookmark for www.rc.ucl.ac.uk  at that point you will get an error or warning about the expired certificate.</p> <p>(This did not happen, the certificate was renewed in time).</p> <p>2025-03-05 - Myriad new filesystem update</p> <p>The Myriad new filesystem is going to have a system update before we put it into production - the  new version fixes a number of security vulnerabilities, and it will be less disruptive to your  jobs if we do this before you have access to it. We also have some minor hardware issues that have  failed deployment checks that we are getting resolved with our vendors.</p> <p>We're currently expecting to be able to give you access to the new filesystem around 31 March,  assuming all goes well with the above updates and the remaining snagging issues.</p> <p>What will happen?</p> <p>When the new filesystem goes live, you'll log in and will see a new empty home and Scratch. Your  old home and Scratch will be available read-only at <code>/old_lustre/home/username</code> and  <code>/old_lustre/scratch/username</code>. You'll be able to copy or rsync data out of it to the new  filesystem, to the ACFS or archive it elsewhere. You will not be able to modify or delete the data  in <code>/old_lustre</code>.</p> <p>Your new home directory will not be backed up. The ACFS will remain as your backed-up location.</p> <p><code>/old_lustre</code> will remain available for three months after the new filesystem go-live date and will  then be removed. </p> <p>All jobs will be held in the queue and you'll be able to remove the holds yourself when the data  they need is in the right place on the new filesystem.</p> <p>Information on how best to do the data moving will be sent nearer the time and added to the documentation.</p> <p>Quota expiry on the new filesystem</p> <p>For the new filesystem we will be updating our policy on what happens when increased quotas expire  and are not renewed. This will involve moving your user data off Myriad's filesystem to another  location temporarily, and then deletion of it on specified timescales. This is to avoid the new  filesystem filling up with data which is no longer being worked on and to allow those of you who are  actively using an increased quota to be able to have the space you need.</p> <p>Myriad's filesystem is not a long-term data store - if you are using the data in your jobs, that is  fine. If you are no longer using Myriad to do computations on the data, it shouldn't be left on  Myriad's filesystem.</p> <p>You will receive multiple notifications before and after your quota expires if this is happening.</p> <p>Further details on this to come. A similar process will take place when your Myriad user account  expires.</p> </li> </ul>"},{"location":"Status_page/#latest-on-myriad","title":"Latest on Myriad","text":"<ul> <li> <p>2025-03-31 - Myriad new filesystem on Mon 7 April</p> <p>We are replacing Myriad's filesystem with the new one on Monday 7 April.</p> <p>From 9am there will be a maintenance period when you will not be able to log in while we switch over to the new filesystem and do final checks. We expect to give you access again later during the 7 April, but if it takes longer you may not have access until Tues 8 April.</p> <p>Only the data on ACFS will be backed up. Please note that the data on the new filesystem will not be backed up,  not even data under <code>/home</code>.</p> <p>After the maintenance, you will have the following storage locations:</p> <ul> <li><code>/home/username</code>: your new home directory on the new filesystem; not backed up</li> <li><code>/home/username/Scratch</code>: we are keeping this as a directory in your home for convenience. It is now part of the same space with the same rules and quota as the rest of your home directory, still not backed up.</li> <li><code>/home/username/oldhome</code>: a symbolic link to <code>/old_lustre/home/username</code>, your old home directory on the old filesystem; read-only (no changes to data possible)</li> <li><code>/home/username/oldscratch</code>: a symbolic link to <code>/old_lustre/scratch/username</code>, your old scratch directory on the old filesystem; read-only (no changes to data possible)</li> <li><code>/home/username/ACFS</code>: a symbolic link to your ACFS space where you can put data you want to be backed up; unaffected by this change</li> </ul> <p>Quotas</p> <p>You will have one quota of 1T on Myriad for your home by default. If you applied for a quota increase or renewal after 15 Feb 2025 we will set this quota on the new filesystem for you (also if you already applied for the increase specifically for the new filesystem). If you had a quota increase from before 15 Feb, it will not be recreated on the new filesystem straight away and you will need to apply for it again.</p> <p>New terms for quota increases and when data is deleted</p> <p>If you have a Myriad quota increase, it will have an expiry date, maximum of one year after application. You will be sent a reminder one month before your quota expires, and then reminders at two weeks, one week, expiry day, two weeks after, one month after. One month after a quota has expired, if you have not contacted us, we will move all the contents of your Myriad home and Scratch into another location. We will keep your data there for two months further to give you longer to contact us to reapply for quota or retrieve the data and will then delete all of it. Please consider this at the time of applying for your quota increase, especially if you expect to be away from UCL for a period of time.</p> <p>This is to prevent Myriad's filesystem from filling up from large quota increases that expire and are not removed.  If you are in contact with us and reapplying for your quota increases, this should allow us to keep granting them  to you. </p> <p>What you need to do</p> <p>Step 1: Move your data</p> <p>After we tell you the new filesystem is available and you can log in, you will need to log in using your UCL  password, as any ssh keys you might have set up will not be there on the new filesystem. You can then copy your  <code>.ssh</code> directory from <code>/old_lustre/home/username</code> to your new home.</p> <p>Commands you may wish to use for copying:</p> <ul> <li>Copy your old .ssh directory into your new home (~) recursively and preserve permissions:<ul> <li><code>cp -rp ~/oldhome/.ssh ~</code> </li> </ul> </li> <li>Use rsync archive mode (recursively copy directories, copy symlinks without resolving, and preserve permissions, ownership and modification times) to copy your old .ssh directory into your new home:<ul> <li><code>rsync -r -a ~/oldhome/.ssh ~</code> </li> </ul> </li> </ul> <p>Copy only works locally, so you can use it for any filesystems that are directly mounted on Myriad (old_lustre, ACFS, RDSS, new filesystem). Rsync can be used locally or between remote systems as well. It can also resume incomplete copies by running again and doesn't re-copy data that you have already copied if your transfer gets interrupted for any reason.</p> <p>You will need to copy your data off old_lustre onto the new filesystem, or the ACFS, or the RDSS if you have a project, or onto other external systems if you want to keep it for future reference.</p> <p>If you have large amounts of data (particularly many small files) that you are intending to archive elsewhere, consider creating tar archives straight away instead of copying data recursively first.</p> <ul> <li><code>tar -czvf /home/username/Scratch/myarchive.tar.gz /old_lustre/home/username/data</code> will (c)reate a g(z)ipped archive (v)erbosely in the specified (f)ilename location on the new filesystem. The contents will be everything in this user's old \"data\" directory.</li> </ul> <p>Step 2: release your jobs</p> <p>All your jobs will be in held status (<code>hqw</code>) so that they do not fail while your data is not there. After you have copied the data that your jobs need to the new filesystem, you can release the hold on your queued jobs.</p> <ul> <li><code>qrls $JOB_ID</code> will release a specific job ID, and <code>qrls all</code> will release all your jobs.</li> </ul> <p>Released array jobs will have the first task in status qw and the rest in hqw - this is normal.</p> <p>FAQ: .bashrc and hidden files</p> <p>Where is my old .bashrc? Why are my jobs failing with module errors now? Why are my python packages not there?</p> <p>Your <code>.bashrc</code> is in <code>/old_lustre/home/username/.bashrc</code></p> <p>It begins with a dot and is a hidden file so will only show up with <code>ls -a</code> rather than <code>ls</code>. You can copy this across into your current home again. You may have put module load and unload commands in it, so are now getting module conflicts when your jobs run since otherwise the modules are still the default ones.</p> <p>This also applies to other hidden files or directories you may have, like <code>.condarc</code> and <code>.python3local</code> where you may have environments defined or packages installed.</p> <p>After I move my files, will they still be read-only?</p> <p>No, it is the old filesystem itself that was set to be read-only. After you copy your files to the new filesystem,  you will be able to edit them in the same way as before.</p> <p>Project/shared spaces/hosted datasets</p> <p>If you have an existing project or shared space or hosted dataset (in <code>/lustre/projects</code> aka <code>/shared/ucl/depts</code>) you will need to reapply for this space and we will need to recreate it on the new filesystem. We'll be sending another email separately to people we have listed as the owners of spaces.</p> <p>The same terms for quota increases and data deletion set out above will apply to project spaces so the data will  be deleted if they expire and are not renewed, after reminders. Where an access group (eg <code>ag-archpc-groupname</code>,  formerly <code>lgsh0xx</code>) has access to the space, we will contact all members of the group so they are still aware if  the original owner of the space has left.</p> <p>If you currently have an access group named like <code>lgsh0xx</code> for your space, as part of reapplying we will be  transferring this to a new access group named <code>ag-archpc-groupname</code>. These groups can be updated within half  an hour rather than only overnight.</p> <p>Removal of old filesystem</p> <p><code>/old_lustre</code> will be available for three months, until 9am on Monday 7 July. It will then be unmounted and you will not be able to access it any longer.</p> <p>Myriad at risk for first week</p> <p>Myriad should be considered exposed to potential issues for the first week of running a full workload with the new filesystem, and so there might be interruptions to service if anything goes wrong or needs tuning differently.</p> <p>The new filesystem is GPFS (IBM Storage Scale) and not Lustre, for those who are interested.</p> <p>Additional FAQs will be added here based on questions we receive. </p> </li> </ul>"},{"location":"Status_page/#kathleen","title":"Kathleen","text":"<ul> <li> <p>2024-09-05 - Kathleen outage for new filesystems on Tues 10 September - action required </p> <p>The previously-announced Kathleen outage for new filesystems will now take place on maintenance  day next week, Tuesday 10 September. </p> <p>The Kathleen cluster will go into maintenance on Tuesday 10 Sept 2024 from 9:00. Logins to  Kathleen will not be possible until the maintenance is finished. Any jobs that won\u2019t finish by  the start of the maintenance window will stay queued. We aim to finish the maintenance within  one day, so that you can access Kathleen again on Weds 11 Sept.</p> <p>We are implementing a number of changes to how data is stored on Kathleen:</p> <ul> <li>The current Lustre filesystem will be replaced with a new Lustre filesystem. The old Lustre is    running on aging and error-prone hardware, and suffers from performance issues, especially for    interactive work on the login nodes. The new Lustre should provide a vastly better experience.</li> <li>The Kathleen nodes will mount the ACFS (ARC Cluster File System) which is ARC\u2019s new centralised    storage system that will (eventually) be available on other ARC systems (e.g. Myriad) too.    ACFS will be available read-write on the login nodes but read-only on the compute nodes.</li> <li>Going forward, only the data on ACFS will be backed up. Please note that the data on the new    Lustre will not be backed up, not even data under <code>/home</code>.</li> </ul> <p>After the maintenance, you have the following storage locations:</p> <ul> <li><code>/home/username</code>: your new home directory on the new Lustre; not backed up (this is a change    to the current situation)</li> <li><code>/scratch/username</code>: your new scratch directory on the new Lustre; not backed up</li> <li><code>/acfs/users/username</code>: your ACFS directory; backed up daily; read-only on the compute nodes</li> <li><code>/old_lustre/home/username</code>: your old home directory on the old Lustre; read-only</li> <li><code>/old_lustre/scratch/username</code>: your old scratch directory on the old Lustre; read-only</li> </ul> <p>You will also have a <code>~/ACFS</code> shortcut/symbolic link in your home that points to <code>/acfs/users/username</code>.</p> <p>If you are looking in your <code>/old_lustre/home/username/Scratch</code> symbolic link, that will direct you  back to your new Scratch not your old Scratch because it uses an absolute path. Please make sure  to access old Scratch using <code>/old_lustre/scratch/username</code>.</p> <p>What you will need to do (after the maintenance):</p> <ul> <li>After login, you will notice that your new home and scratch directories are mostly empty.    Please copy any data you need from your old home and scratch directories under <code>/old_lustre</code> to    the appropriate new locations.<ul> <li>E.g. <code>cp -rp /old_lustre/home/username/data /home/username</code> will recursively copy your old    <code>data</code> directory and everything in it into your new home, while preserving the permissions.</li> </ul> </li> <li>Any data that you consider important enough to be backed up should be copied into your ACFS    directory instead.</li> <li>You have three months to copy your data. After this, the <code>/old_lustre</code> will become unavailable.</li> <li>Your queued jobs will be held (showing status <code>hqw</code> in <code>qstat</code>) and won\u2019t start running    automatically, as their job scripts will likely refer to locations on <code>/lustre</code> which won\u2019t exist    until you have copied over the data. After you have copied the data that your jobs need to the new    Lustre, you can release the hold on your queued jobs.<ul> <li>E.g. <code>qrls $JOB_ID</code> will release a specific job ID, and <code>qrls all</code> will release all your jobs.</li> <li>Released array jobs will have the first task in status <code>qw</code> and the rest in <code>hqw</code> - this is normal.</li> </ul> </li> <li>Depending on the amount of data, the copying may take some time, especially if you have many small    files. If you are copying data to ACFS and you don\u2019t need immediate access to each file individually,    consider creating tar archives instead of copying data recursively.<ul> <li>E.g. <code>tar -czvf /acfs/users/username/myarchive.tar.gz /old_lustre/home/username/data</code> will    (c)reate a g(z)ipped archive (v)erbosely in the specified (f)ilename location. The contents will be    everything in this user's old <code>data</code> directory. </li> </ul> </li> </ul> <p>Further reminders will be sent before <code>/old_lustre</code> is removed on 11 December.</p> <p>Kathleen quotas</p> <p>You will continue to have one quota on Kathleen, with a default value of 250G that includes your  home and Scratch. If you have an active quota increase request that has not reached its requested  expiry date on your old space, we will be recreating these on the new space. As stated above,  <code>/home</code> will no longer be backed up.</p> <p>ACFS quotas</p> <p>You will have 1T of quota on the ACFS. You will be able to check this with the <code>aquota</code> command.</p> <p>The ACFS has dual locations for resilience, and as a result standard commands like <code>du</code> or <code>ls -al</code>  will report filesizes on it as being twice what they really are. The <code>aquota</code> command will show you  real usage and quota. One of the reasons for the previous delay is that we tried to get filesizes to  report correctly in all circumstances, but that was not possible so we decided it was less confusing  if everything other than <code>aquota</code> always showed double. </p> <p>For those interested, the ACFS is a GPFS filesystem.</p> <p>This outage will show shortly on Planned Outages  and the ACFS information will be added to our documentation.</p> <p>We apologise for the inconvenience, but we believe these changes will help to provide a more performant  and robust service in the future.</p> <p>Please email rc-support@ucl.ac.uk with any queries or raise a request about Kathleen via  UCL MyServices.</p> </li> <li> <p>2024-09-11 09:35 Kathleen filesystem outage complete</p> <p>The outage is complete and you can log in and access your new home, Scratch and ACFS directories  on Kathleen.</p> <p>In addition to the information given previously:</p> <ul> <li>You will have a <code>~/ACFS</code> shortcut/symbolic link in your home that points to <code>/acfs/users/username</code></li> <li>If you want to copy data and preserve the permissions, use <code>cp -rp</code> rather than <code>cp -r</code></li> <li>If you are looking in your <code>/old_lustre/home/username/Scratch</code> symbolic link, that will direct you    back to your new Scratch not your old Scratch because it uses an absolute path. Please make sure to    access old Scratch using <code>/old_lustre/scratch/username</code></li> </ul> <p>This will be added to https://www.rc.ucl.ac.uk/docs/Status_page/#kathleen and has been updated in the  original message there.</p> <p>The message you see when first logging in to Kathleen has been updated with the change to backed-up  locations and ACFS information.</p> <p>We have these additional pages in our documentation:</p> <ul> <li>https://www.rc.ucl.ac.uk/docs/Background/Parallel_Filesystems/ (parallel filesystem concepts)</li> <li>https://www.rc.ucl.ac.uk/docs/Background/Data_Storage/ (data storage locations we provide)</li> <li>https://www.rc.ucl.ac.uk/docs/Data_Management/ (how to check quotas, transfer ownership of files)</li> </ul> <p>Terms &amp; Conditions update</p> <p>We have updated our Terms and Conditions (https://www.rc.ucl.ac.uk/docs/Terms_and_Conditions/) -  please take a look. It now defines our data retention policies and when we can access your data,  among other things. </p> </li> <li> <p>2024-11-28 14:00 - Reminder: Kathleen /old_lustre removal on 11 Dec; later upcoming changes</p> <p>This is a reminder that access to <code>/old_lustre</code> will be removed on Monday 11 December, so if  you still have files in your old home and scratch directories, they will no longer be accessible  after this time.</p> <p>Here's the previous information sent out on how to check that and how to copy your data:  https://www.rc.ucl.ac.uk/docs/Status_page/#kathleen</p> <p>Note: do check for hidden files starting with a <code>.</code> as well, such as customisations you may have  added to your <code>.bashrc</code>, config files for programs like <code>.vimrc</code> and other directories like <code>.conda</code>,  <code>.python3local</code>, <code>.julia</code>, <code>.cpanm</code> where you may have installed packages or have other  environments or configuration.</p> <p>These are visible to <code>ls -a</code> but not to <code>ls</code>.</p> <p>Later upcoming changes</p> <p>You may be aware that we are working to update the operating system on all our clusters to RHEL 9.  Kathleen is likely to be the first deployed. There will be more details nearer the time, but this will  involve an outage and after it the operating system will be updated, the software will be rebuilt and we  will have Slurm as our scheduler instead of SGE.</p> <p>We've had questions from some of you about VSCode, since it will stop connecting to unsupported  operating systems in February 2025 - once we have updated the operating system it will be able to connect  again. </p> <p>Right now we don't have a timescale for this but will be letting you know when we do. Development work  is ongoing. Documentation will be updated for what you will need to do with jobscripts and job submission  commands. This is just to let you know that these changes are coming.</p> </li> <li> <p>2024-12-11 09:10 - Kathleen /old_lustre removal</p> <p>11 Dec is not a Monday, contrary to my previous email. As a result, we will leave <code>/old_lustre</code>  mounted on Kathleen until Monday 16 December. If you still have files in your old home and  scratch directories, they will no longer be accessible after this time. It will be unmounted at or shortly after 9am.</p> </li> <li> <p>2025-02-10 - OS, software stack and scheduler update news</p> <p>This is to keep you informed about upcoming changes to Kathleen.</p> <p>Live now:</p> <ul> <li>Research Data Storage Service mount point on login nodes</li> </ul> <p>If you have an RDSS project, you can now access that read-write on the Kathleen login nodes at <code>/rdss</code>. There are subdirectories <code>rd00</code>, <code>rd01</code> that contain projects with names beginning <code>ritd-ag-project-rd00</code> or <code>ritd-ag-project-rd01</code> and so on. You cannot access it from the compute nodes. You should <code>cp</code> or <code>rsync</code> data to your home, Scratch or the ACFS via the login nodes before doing any computation using the data.</p> <p>Incoming:</p> <ul> <li>OS upgrade to RHEL 9.5</li> <li>Scheduler move to Slurm</li> <li>New software via Spack</li> </ul> <p>We will be moving to RHEL 9.5 as our operating system and Slurm as our new scheduler.</p> <p>We will send more updates when we are ready to roll it out - more info should be coming in the next  couple of weeks. It will be available on Kathleen first.</p> <p>Our user documentation will be updated with examples of Slurm jobscripts instead of the SGE ones we currently have. Slurm uses <code>srun</code> and <code>sbatch</code> commands instead of <code>qsub</code>, if you have come across those in other software documentation.</p> <p>The OS upgrade should allow you to run more recent binaries that currently give errors about GLIBC being too old. System tools will be newer and may look a little different.</p> <p>We are also creating a new small software stack built with Spack. This will available to you to test before the OS upgrade, then rebuilt again after it, so it will change slightly in the meantime. Do let us know if applications you use in it are missing options we currently have or not working as you expect.</p> <p>We are aware there is a new CASTEP to add, and we can't install the recommended newer Intel OneAPI  compilers to build the newest VASP until after the OS upgrade.</p> <p>Most of the existing software stack will be reinstalled for the new OS. We are looking to remove some of the oldest installs and modules that are not being used in jobs. We then intend to prune this further over time and add newer versions into the Spack stack.</p> <p>Documentation links</p> <p>The SSL certificate for www.rc.ucl.ac.uk is due to expire at midnight on 12 Feb. We're getting a new one but there might be a gap if it can't be renewed in time. If that happens your browser may prevent you accessing that link because the certificate is expired.</p> <ul> <li>mkdocs-rc-docs on Github will work</li> </ul> <p>If there is a gap when the certificate expires we'll update the links in the message you see when you log in to the cluster, but if you are using an existing link or bookmark for www.rc.ucl.ac.uk at that point you will get an error or warning about the expired certificate.</p> <p>(This did not happen, the certificate was renewed in time).</p> </li> </ul>"},{"location":"Status_page/#latest-on-kathleen","title":"Latest on Kathleen","text":"<ul> <li> <p>2025-02-17 - New test software stack available</p> <p>There is a test version of our next software stack available now on Kathleen. This has a small  number of packages at present. What is in it and the names of modules are liable to change over  time, so please do not rely on it for production work. Instead, please test whether the  applications you intend to use work the way you would expect.</p> <p>This stack is built using Spack.</p> <p>To use:</p> </li> </ul> <pre><code>module load beta-modules\nmodule load test-stack/2025-02\n</code></pre> <pre><code>After that, when you type `module avail` there will be several sections of additional modules at \nthe top of the output.\n\nNot everything contained in the stack is visible by default - we have made the applications that \nwe expect people to use directly visible and lots of their dependencies are hidden. These will \nshow up if you search for that package specifically, for example:\n</code></pre> <pre><code>module avail libpng\n-------------------------- /shared/ucl/apps/spack/0.23/deploy/2025-02/modules/applications/linux-rhel7-cascadelake --------------------------\nlibpng/1.6.39/gcc-12.3.0-iopfrab\n</code></pre> <pre><code>This module does not show up in the full list but is still installed. It has a hash at the end \nof its name `-iopfrab` and this will change over time with different builds.\n\nIf you find you are needing one of these modules often, let us know and we'll make it one that \nis not hidden in the next release of this stack.\n\nThe stack will be rebuilt for the operating system upgrade and moved out from beta-modules.\n\nThis information is available at \n[Kathleen test software stack](https://www.rc.ucl.ac.uk/docs/Clusters/Kathleen/#test-software-stack)\n</code></pre>"},{"location":"Status_page/#young","title":"Young","text":"<ul> <li> <p>2023-10-26 14:50 - We seem to have a dying OmniPath switch in Young. The 32 nodes with names      beginning <code>node-c12b</code> lost their connection to the filesystem earlier. Powercycling the switch      only helped temporarily before it went down again. Those nodes are all currently out of service      so new jobs will not start on them, but if your job was running on one of those when the two      failures happened those jobs will have failed.</p> <p>(You can see in <code>jobhist</code> what the head node of a job was, and the .po file will show all the  nodes that an MPI job ran on).</p> </li> <li> <p>2024-01 Parallel filesystem soon to be replaced.</p> </li> <li> <p>2024-01-03 12:30 Filesystem issues on Young: temporary hangs, running but degraded </p> <p>We've just had two periods today where Young's filesystem would have hung - hopefully briefly  enough that operations in progress will have continued after it recovered.</p> <p>We failed over from one server to the other and back again. </p> <p>Young's filesystem is more at risk than usual right now since we have some failed disks and  one area (one Object Store Target) is degraded. We have stopped new data from being written  there and are migrating the existing data to the rest of the filesystem. </p> <p>The filesystem is still working and Young is still running jobs. We do not need you to take  any action at present, but things may be running more slowly while this completes.</p> </li> <li> <p>2024-09 Young's new filesystem is being readied for service.</p> </li> </ul>"},{"location":"Status_page/#young-new-filesystem","title":"Young new filesystem","text":"<ul> <li> <p>2024-09-27 - Young and Michael outage for new filesystem on Mon 7 Oct - action required</p> <p>We will be replacing the two filesystems on Young and Michael with one new filesystem on  Monday 7 October 2024. </p> <p>Both clusters will go into maintenance on Monday 7 Oct 2024 from 09:00am. Logins will not be  possible until the maintenance is finished. Any jobs that won\u2019t finish by the start of the  maintenance window will stay queued. We aim to finish the maintenance within one day, so that  you can access the clusters again on Tues 8 Oct.</p> <p>Single login node outages between 2-4 Oct</p> <p>From 2-4 October, Young <code>login02</code> and Michael <code>login11</code> will each be out of service for a day  during this period for testing updates before the filesystem migration. There will be no  interruption to jobs or logins to the general addresses <code>young.rc.ucl.ac.uk</code> and  <code>michael.rc.ucl.ac.uk</code>. If you are on <code>login02</code> or <code>login11</code> at the time, you may see a message  that it is about to go down for reboot, and if you have a tmux or screen session on that login  node then it will be terminated. You will be able to log back in and be assigned to the other  login node, Young <code>login01</code> or Michael <code>login10</code>.</p> <p>Why the change:</p> <ul> <li>Young's filesystem is running on aging and error-prone hardware, and suffers from performance    issues, especially for interactive work on the login nodes. The new Lustre should provide a    vastly better experience.</li> <li>Michael's filesystem is old and replacement parts are no longer available.</li> <li>The new filesystem is a HPE ClusterStor Lustre filesystem and will enable both machines to keep    running in a supported and maintainable manner.</li> </ul> <p>After the maintenance, you have the following storage locations:</p> <ul> <li><code>/home/username</code>: your new home directory on the new Lustre; backed up</li> <li><code>/scratch/username</code>: your new scratch directory on the new Lustre; not backed up</li> <li><code>/old_lustre/home/username</code>: your old home directory on the old Lustre; read-only</li> <li><code>/old_lustre/scratch/username</code>: your old scratch directory on the old Lustre; read-only</li> </ul> <p>If you currently have accounts on both Young and Michael, you will need to log into Young to  see Young's <code>old_lustre</code> and into Michael to see Michael's <code>old_lustre</code>, but your home and  Scratch will be the same on both, and the data you copy into it will be visible on both.</p> <p>Quotas</p> <p>On the new filesystem we are able to set separate home and Scratch quotas. </p> <ul> <li>Home: 100G, backed up</li> <li>Scratch: 250G by default</li> </ul> <p>Previously the default quota was 250G total.</p> <p>If you have an existing non-expired quota increase, we will increase your Scratch quota on  the new filesystem to this amount. If you find you need an increased Scratch quota, you can  run the <code>request_quota</code> command on either cluster and it will ask you for some information and  send us a support ticket.</p> <p>What you will need to do (after the maintenance):</p> <ul> <li>After login, you will notice that your new home and scratch directories are mostly empty.    Please copy any data you need from your old home and scratch directories under <code>/old_lustre</code> to    the appropriate new locations. Your existing SSH keys will all have been copied in so that you    can log in. You can do this copy on the login nodes.<ul> <li>E.g. <code>cp -rp /old_lustre/home/username/data /home/username</code> will recursively copy with    preserved permissions your old <code>data</code> directory and everything in it into your new home.</li> </ul> </li> <li>You have three months and one week to copy your data. After this, the <code>/old_lustre</code> will    become unavailable.</li> <li>Your queued jobs will be held (showing status <code>hqw</code> in qstat) and won\u2019t start running    automatically, as their job scripts will likely refer to locations on <code>/lustre</code> which won\u2019t    exist until you have copied over the data. After you have copied the data that your jobs    need to the new Lustre, you can release the hold on your queued jobs.<ul> <li>E.g. <code>qrls $JOB_ID</code> will release a specific job ID, and <code>qrls all</code> will release all your jobs.</li> <li>Released array jobs will have the first task in status <code>qw</code> and the rest in <code>hqw</code> - this is normal.</li> </ul> </li> <li>Depending on the amount of data, the copying may take some time, especially if you have many    small files. If you wish to archive some of your data, consider creating tar archives straight    away instead of copying data recursively.<ul> <li>E.g. <code>tar -czvf /home/username/Scratch/myarchive.tar.gz /old_lustre/home/username/data</code> will    (c)reate a g(z)ipped archive (v)erbosely in the specified (f)ilename location. The contents    will be everything in this user's old <code>data</code> directory. </li> </ul> </li> </ul> <p>Further reminders will be sent before the <code>/old_lustre</code> locations are removed on 14 January 2025.</p> <p>Terms &amp; Conditions update</p> <p>We have updated our Terms and Conditions for all services  - please take a look. It now defines our data retention policies and when we can access your data,  among other things.</p> <p>These outages are listed on Planned Outages.  The information above will also be copied into the https://www.rc.ucl.ac.uk/docs/Status_page/  sections for Young and Michael.</p> <p>Please email rc-support@ucl.ac.uk with any queries. </p> <p>If you are no longer using Young or Michael and wish to be removed from these mailing lists,  email us confirming that we can delete your accounts and we will do so and remove you from the lists.</p> </li> <li> <p>2024-10-07 11:00 - Delay in returning Young GPU nodes to service</p> <p>In addition to the above, there may be a delay in allowing the GPU nodes in Young to run jobs again -  we need to rebuild the existing GPU software for those, to have it on an architecture-specific section  of our software stack. This will result in GPU jobs remaining in the queue while we complete this. I  would expect this to be complete by Wednesday or Thursday but will keep you updated.</p> </li> <li> <p>2024-10-08 13:30 - Logins are enabled.</p> <p>Logins are enabled again.</p> <p>We're continuing to rebuild the GPU applications on Young.</p> </li> <li> <p>2024-10-09 17:00 - GPU nodes are enabled, plus FAQ</p> </li> </ul> <p>The GPU applications are rebuilt so we have re-enabled jobs on the GPU nodes. Please let us know if you    encounter any issues or if we have missed something.</p> <p>We found that we hadn't synced keys across for users created after 26 September so they could not log in    - this was sorted out at 10:20am today. It affected mmm1457 to mmm1463 on Young.</p> <p>Some frequently-asked questions about the filesystem update:</p> <ol> <li> <p>Where is my old <code>.bashrc</code>? &amp; Why are my jobs failing with module errors now?</p> <p>This is in <code>/old_lustre/home/username/.bashrc</code> </p> <p>It begins with a dot and is a hidden file so will only show up with <code>ls -a</code> rather than <code>ls</code>. You can   copy this across into your current home again. You may have put module load and unload commands in it,   so are now getting module conflicts when your jobs run since the modules are the default ones.</p> <p>Note that this also applies to other hidden files or directories you may have, like <code>.condarc</code> and <code>.python3local</code>.</p> </li> <li> <p>Do I need to copy my data in a job?</p> <p>No, you can do the copy on the login nodes. It can cause a lot of I/O usage on the login nodes so you   could also do it in a job if you wanted or if you noticed that it was going slowly, but using the login   nodes for this is allowed.</p> </li> <li> <p>I need more than the 100G home quota and 250G Scratch quota for my data, what do I do?</p> <p>Home is your backed-up area and is limited in size. Scratch is not backed up and also not deleted and we   can give you more space there. Run the <code>request_quota</code> command and it will ask you a few questions and send   us the request. We migrated all quota increases that were made within the last 12 months to Scratch   increases and did not migrate older ones. This allows us to make sure that the quota increases are current   and still needed. We are processing the new requests.</p> </li> <li> <p>Where should I write large temporary data during my jobs?</p> <p>Into Scratch. As above, you can request a larger quota if you need one. (If you are using the GPU nodes,   you can use <code>$TMPDIR</code> and request it with <code>#$ -l tmpfs=20G</code> for example, but the GPU nodes are the only   ones with local disks so everything else needs to be in Scratch).</p> </li> </ol> <ul> <li> <p>2024-09-04 15:10 - UKRI survey, new HBM nodes and GPU node driver updates</p> <p>EPSRC Large Scale Facilities - User survey</p> <p>EPSRC Large Scale Facilities survey</p> <p>Please can you fill in this survey that EPSRC are running on all their Large Scale Facilities - it  includes events run by facilities as well as accessibility of the remote service itself, so if you  attended the MMM Hub Conference last week and didn't fill it in at the end, please consider it now.</p> <p>New High Bandwidth Memory nodes</p> <p>We have 32 new high bandwidth memory nodes in Young, each with 64 cores, 503G usable memory and 3.5T  available to be requested as tmpfs in your job.</p> <p>They are currently configured in cache mode, where the HBM functions as a memory-side cache for the  contents of the DDR memory. This should allow all applications to take advantage of it without needing  to be specifically configured to use it. </p> <p>Please try this out with your applications and let us know how it is working and whether you see an  improvement. We have one VASP test case at present and would like to build up suitable test cases  with other applications, including more specific ones with VASP. Especially if you already know you  have been running up against memory bandwidth limitations, please contact us at rc-support@ucl.ac.uk  - if you have test cases and tell us where on the filesystem they are, we can get them from you.  After we upgrade the operating system on the cluster we will be testing other HBM modes as well, which  can have a greater impact if the application is aware of it.</p> <p>We will also be using this information to inform future hardware purchases for the MMM community so any  feedback, positive or negative, you have on these nodes would be gratefully received.</p> <p>Information about the nodes and how to use</p> <p>You will need to request these nodes explicitly in your jobscripts with <code>#$ -ac allow=W</code></p> <p>The Maximum job resources and  Node types tables are also updated.</p> </li> </ul>"},{"location":"Status_page/#latest-on-young","title":"Latest on Young","text":"<ul> <li> <p>2025-01-14 11:10 Extension to availability of <code>/old_lustre</code> on Young until 9am on Tues 28 Jan</p> <p>We've had multiple requests from people who have been unable to retrieve their data from Young's  old_lustre in time for the removal of the filesystem today, so we are extending the deadline for  two weeks.</p> <p>The old filesystem on Young will be unmounted shortly after 9am on Tuesday 28 January.</p> <p>There have been some questions about ways to transfer files in chunks if your transfers are being  interrupted, without repeating copying the same files - you can use <code>rsync</code> for this as it can  resume incomplete copies by running again, and if you use the <code>-a</code> option (\"archive\") it retains  many of the file properties, including e.g. timestamps.</p> <p>Use archive mode (recursively copy directories, copy symlinks without resolving, and preserve  permissions, ownership and modification times):</p> <p><code>rsync -a|--archive path/to/source path/to/destination</code></p> <p>(This shows that you can use <code>-a</code> or <code>--archive</code> as the option passed in).</p> <p>Alejandro Santana-Bonilla (KCL) also created a script for copying your first 10 (or other number)  directories, then second 10 and so on, if that is useful to you: </p> <ul> <li>old_lustre_folders script</li> </ul> </li> </ul>"},{"location":"Status_page/#michael","title":"Michael","text":"<ul> <li> <p>2024-01-24 16:40 - Problem on Michael's admin nodes causing DNS failures - now solved</p> <p>We have been having DNS problems on Michael today since around 14:40, meaning that scheduler  commands were not working and running jobs may have errors, including failed username lookups.  New jobs trying to start during this time are likely to have failed on start up. Running jobs  may have been affected, so please check the outputs for any jobs that were running during 14:40  to 16:30 today.</p> <p>This was caused by a problem on the admin nodes that has now been sorted out.</p> </li> <li> <p>2024-09 Michael's new filesystem (shared with Young) is being readied for service.</p> </li> </ul>"},{"location":"Status_page/#michael-new-filesystem","title":"Michael new filesystem","text":"<ul> <li> <p>2024-09-27 - Young and Michael outage for new filesystem on Mon 7 Oct - action required</p> <p>We will be replacing the two filesystems on Young and Michael with one new filesystem on Monday 7 October 2024.</p> <p>Both clusters will go into maintenance on Monday 7 Oct 2024 from 09:00am. Logins will not be possible until the maintenance is finished. Any jobs that won\u2019t finish by the start of the maintenance window will stay queued. We aim to finish the maintenance within one day, so that you can access the clusters again on Tues 8 Oct.</p> <p>Single login node outages between 2-4 Oct</p> <p>From 2-4 October, Young <code>login02</code> and Michael <code>login11</code> will each be out of service for a day during this period for testing updates before the filesystem migration. There will be no interruption to jobs or logins to the general addresses <code>young.rc.ucl.ac.uk</code> and <code>michael.rc.ucl.ac.uk</code>. If you are on <code>login02</code> or <code>login11</code> at the time, you may see a message that it is about to go down for reboot, and if you have a tmux or screen session on that login node then it will be terminated. You will be able to log back in and be assigned to the other login node, Young <code>login01</code> or Michael <code>login10</code>.</p> <p>Why the change:</p> <ul> <li>Young's filesystem is running on aging and error-prone hardware, and suffers from performance   issues, especially for interactive work on the login nodes. The new Lustre should provide a   vastly better experience.</li> <li>Michael's filesystem is old and replacement parts are no longer available.</li> <li>The new filesystem is a HPE ClusterStor Lustre filesystem and will enable both machines to keep   running in a supported and maintainable manner.</li> </ul> <p>After the maintenance, you have the following storage locations:</p> <ul> <li><code>/home/username</code>: your new home directory on the new Lustre; backed up</li> <li><code>/scratch/username</code>: your new scratch directory on the new Lustre; not backed up</li> <li><code>/old_lustre/home/username</code>: your old home directory on the old Lustre; read-only</li> <li><code>/old_lustre/scratch/username</code>: your old scratch directory on the old Lustre; read-only</li> </ul> <p>If you currently have accounts on both Young and Michael, you will need to log into Young to see Young's <code>old_lustre</code> and into Michael to see Michael's <code>old_lustre</code>, but your home and Scratch will be the same on both, and the data you copy into it will be visible on both.</p> <p>Quotas</p> <p>On the new filesystem we are able to set separate home and Scratch quotas.</p> <ul> <li>Home: 100G, backed up</li> <li>Scratch: 250G by default</li> </ul> <p>Previously the default quota was 250G total.</p> <p>If you have an existing non-expired quota increase, we will increase your Scratch quota on the new filesystem to this amount. If you find you need an increased Scratch quota, you can run the <code>request_quota</code> command on either cluster and it will ask you for some information and send us a support ticket.</p> <p>What you will need to do (after the maintenance):</p> <ul> <li>After login, you will notice that your new home and scratch directories are mostly empty.   Please copy any data you need from your old home and scratch directories under <code>/old_lustre</code> to   the appropriate new locations. Your existing SSH keys will all have been copied in so that you   can log in. You can do this copy on the login nodes.<ul> <li>E.g. <code>cp -rp /old_lustre/home/username/data /home/username</code> will recursively copy with   preserved permissions your old <code>data</code> directory and everything in it into your new home.</li> </ul> </li> <li>You have three months and one week to copy your data. After this, the <code>/old_lustre</code> will   become unavailable.</li> <li>Your queued jobs will be held (showing status <code>hqw</code> in qstat) and won\u2019t start running   automatically, as their job scripts will likely refer to locations on <code>/lustre</code> which won\u2019t   exist until you have copied over the data. After you have copied the data that your jobs   need to the new Lustre, you can release the hold on your queued jobs.<ul> <li>E.g. <code>qrls $JOB_ID</code> will release a specific job ID, and <code>qrls all</code> will release all your jobs.</li> <li>Released array jobs will have the first task in status <code>qw</code> and the rest in <code>hqw</code> - this is normal.</li> </ul> </li> <li>Depending on the amount of data, the copying may take some time, especially if you have many   small files. If you wish to archive some of your data, consider creating tar archives straight   away instead of copying data recursively.<ul> <li>E.g. <code>tar -czvf /home/username/Scratch/myarchive.tar.gz /old_lustre/home/username/data</code> will   (c)reate a g(z)ipped archive (v)erbosely in the specified (f)ilename location. The contents   will be everything in this user's old <code>data</code> directory.</li> </ul> </li> </ul> <p>Further reminders will be sent before the <code>/old_lustre</code> locations are removed on 14 January 2025.</p> <p>Terms &amp; Conditions update</p> <p>We have updated our Terms and Conditions for all services - please take a look. It now defines our data retention policies and when we can access your data, among other things.</p> <p>These outages are listed on Planned Outages.  The information above will also be copied into the https://www.rc.ucl.ac.uk/docs/Status_page/  sections for Young and Michael.</p> <p>Please email rc-support@ucl.ac.uk with any queries.</p> <p>If you are no longer using Young or Michael and wish to be removed from these mailing lists, email us confirming that we can delete your accounts and we will do so and remove you from the lists.</p> </li> <li> <p>2024-10-08 13:30 Logins are enabled.</p> <p>Logins are enabled again.</p> </li> </ul>"},{"location":"Status_page/#thomas","title":"Thomas","text":"<ul> <li>Thomas is now retired.</li> </ul>"},{"location":"Terms_and_Conditions/","title":"Terms and Conditions","text":"<p>Info</p> <p>The Terms and Conditions were changed on 10 September 2024. The previous Terms and Conditions are still available for reference.</p> <p>All use of Research Computing Platforms is subject to the general UCL Computing Regulations.</p> <p>The following terms also apply to the use of ARC HPC services, including:</p> <ul> <li>Myriad</li> <li>Kathleen</li> <li>Michael</li> <li>MMM Young</li> <li>ARC Cluster Filesystem (\"ACFS\").</li> </ul> <p>They do not apply to other ARC services, including:</p> <ul> <li>Data Safe Haven</li> <li>Research Data Storage Service</li> </ul> <p>These services have their own terms and conditions for use.</p>"},{"location":"Terms_and_Conditions/#commercial-services","title":"Commercial Services","text":"<p>It is not permitted to provide commercial services from your account on our systems. </p>"},{"location":"Terms_and_Conditions/#impairing-other-users","title":"Impairing Other Users","text":"<p>On nodes intended for shared use to access a service (\"login nodes\"), we run a system intended to restrict the load any single user can produce.</p> <p>We may also terminate running processes, or remove running or queued jobs, if they impair the availability of shared resources for other users.</p>"},{"location":"Terms_and_Conditions/#access-suspension","title":"Access Suspension","text":"<p>If you:</p> <ul> <li>are subject to a UCL disciplinary procedure,</li> <li>or breach UCL's Computing Regulations,</li> <li>or are suspected to have shared your access credentials,</li> <li>or are suspected to be involved in a security incident,</li> </ul> <p>we may raise this with relevant UCL teams<sup>1</sup> and suspend your access to services until any relevant processes have been completed.</p>"},{"location":"Terms_and_Conditions/#your-data","title":"Your Data","text":"<p>Our research computing services have data storage capabilities, but should not be treated as the sole repository for your data. We do not make contractual agreements about uptime or availability.</p> <p>Outages and data-loss events will be handled on a best-efforts basis within normal UCL working hours.</p>"},{"location":"Terms_and_Conditions/#transferring-data-ownership","title":"Transferring Data Ownership","text":"<p>You may contact us to arrange to transfer ownership of your data on a service to another user, with their consent. Please arrange this before you stop being a user.</p> <ul> <li>Requesting transfer of your data to another user</li> <li>Requesting data belonging to a user who has left</li> </ul>"},{"location":"Terms_and_Conditions/#data-access-by-support-staff","title":"Data Access by Support Staff","text":"<p>We will not access your data without your consent, except in the following circumstances:</p> <ul> <li>You contact us with a support issue and the data is relevant to your request.</li> <li>We have reason to believe the data is relevant to investigating an on-going security incident.</li> <li>In automated service processes, such as backups, or transfers between storage systems for service maintenance.</li> </ul> <p>Occasionally support requests can involve more involved work, including running jobs as a user. Since this can be disruptive or involve changes to files and directories, in this case we will check with you when it is safe to begin the work, and when the work has ended.</p>"},{"location":"Terms_and_Conditions/#data-retention","title":"Data Retention","text":""},{"location":"Terms_and_Conditions/#data-retention-on-leaving-ucl","title":"Data Retention on Leaving UCL","text":"<p>We intend to retain user data for 180 days after a user has either left UCL, requested that their account be removed, had their institution request that their account be removed, or become ineligible for an account in some other way.</p> <p>This process is not currently automated, so data may still be retained after this time. Please contact us if you need to ensure your data has been erased.</p>"},{"location":"Terms_and_Conditions/#backup-retention","title":"Backup Retention","text":"<ul> <li>On clusters where home is backed up, backups are retained for 15 days.</li> <li>On the ACFS, backups are retained for 30 days minimum.</li> </ul>"},{"location":"Terms_and_Conditions/#filesystem-retirement","title":"Filesystem Retirement","text":"<p>When filesystems or clusters are retired, they will be decommissioned and we will not retain the data left on them. Barring unforeseen circumstances, you will receive three months' notice before this happens.</p>"},{"location":"Terms_and_Conditions/#acknowledgement-in-works","title":"Acknowledgement in Works","text":"<p>We request that you acknowledge the use of our HPC services in any publications describing research that has used them, in any part. The following words should be used:</p> <p>\"The authors acknowledge the use of the UCL Myriad High Performance Computing Facility (Myriad@UCL), and associated support services, in the completion of this work\". </p> <p>Or analogous terminology for other services.</p> <ol> <li> <p>I.e. the Information Security Group, or Human Resources.\u00a0\u21a9</p> </li> </ol>"},{"location":"UCL_Service_For_Me/","title":"Which service(s) at UCL are right for me?","text":"<p>Depending on the type of jobs you would like to run each cluster can have different requirements that your jobs must meet. When you submit your user account application form you will be given access to the cluster depending on resources selected.</p> <p>Currently we categorise intended workloads into ones that use:</p> <ul> <li>Individual single core jobs</li> <li>Large numbers (&gt;1000) of single core jobs</li> <li>Multithreaded jobs</li> <li>Extremely large quantities of RAM (&gt;64GB)</li> <li>Small MPI jobs (&lt;32 cores)</li> <li>Medium-sized MPI jobs (32-256 cores)</li> <li>Large-sized MPI jobs (&gt;256 cores)</li> <li>At least one GPGPU</li> <li>At least ten GPGPUs</li> </ul> <p>Each cluster has its own specifications for the types of jobs that they run which is all dependable on list above. The cluster machines we have available are:</p> <ul> <li>Myriad</li> </ul> <p>Myriad is designed to be most suitable for serial work, including large numbers of serial jobs, and multi-threaded jobs (using e.g. OpenMP). It also includes a small number of GPUs for development or testing work. It went into service in July 2018. See Myriad.</p> <ul> <li>Kathleen</li> </ul> <p>Kathleen is intended for multi-node jobs (using e.g. MPI) and went into service in Feb 2020 as a replacement for Grace. We recommend using Kathleen if you intend to use more than 36 cores per job. See Kathleen.</p> <ul> <li>Young</li> </ul> <p>Young is the cluster for the UK's Tier 2 Materials and Molecular Modelling Hub, and replaces the previous Thomas cluster. It is accessible by members of partner institutions and relevant consortia, and is for materials and molecular modelling work only. It has separate access procedures from UCL's central clusters. Access is managed by a Point of Contact from the relevant institution or consortia, not by Research Computing. See Young.</p> <ul> <li>Michael</li> </ul> <p>Michael is an extension to the UCL-hosted Hub for Materials and Molecular Modelling, an EPSRC-funded Tier 2 system providing large scale computation to UK researchers; and delivers computational capability for the Faraday Institution, a national institute for electrochemical energy storage science and technology. Access is managed by a Point of Contact from the Faraday Institution, not by Research Computing. See Michael.</p>"},{"location":"howto/","title":"How do I?","text":"<p>I have an account, now:</p>"},{"location":"howto/#how-do-i-log-in","title":"How do I log in?","text":"<p>Logging in is most straightforward if you are inside the UCL firewall. If you are logging in from home or other external networks then you first have to get on to the UCL network.</p>"},{"location":"howto/#linux-unix-macos","title":"Linux / Unix / macOS","text":"<p>Use the terminal and type the below command to secure shell (ssh) into the machine you wish to access. Replace <code>&lt;your_UCL_user_id&gt;</code> with your central UCL username, and <code>&lt;system_name&gt;</code> with the name of the machine you want to log in to, eg. <code>myriad</code>, <code>kathleen</code>, <code>aristotle</code>. </p> <pre><code>ssh &lt;your_UCL_user_id&gt;@&lt;system_name&gt;.rc.ucl.ac.uk\n</code></pre>"},{"location":"howto/#windows","title":"Windows","text":"<p>On Windows you need something that will give you a suitable terminal and ssh - usually PuTTY, or on Windows 10 you can use OpenSSH from a command prompt and type the same <code>ssh</code> command as the  Linux instructions.</p>"},{"location":"howto/#using-putty","title":"Using PuTTY","text":"<p>PuTTY is a common SSH client on Windows and is available on Desktop@UCL. You can find it under:  <code>Start &gt; P &gt; PuTTY 0.76 (64-bit) &gt; PuTTY 0.76</code> or type \"putty\" in the toolbar's search box.</p> <p>You will need to create an entry for the host you are connecting to with the settings below. If you want to save your settings, give them an easily-identifiable name in the \"Saved Sessions\" box and press \"Save\". Then you can select it and \"Load\" next time you use PuTTY. </p> <p></p> <p>You will then be asked to enter your username and password. Only enter your username, not <code>@&lt;system_name&gt;.rc.ucl.ac.uk</code>. The password field will remain entirely blank when you type in to it - it does not show placeholders to indicate you have typed something. </p> <p>The first time you log in to a new server, you'll get a popup telling you that the server's host  key is not cached in the registry - this is normal and is because you have never connected to this server before. If you want to, you can check the host fingerprint against our current key fingerprints.</p>"},{"location":"howto/#logging-in-from-outside-the-ucl-firewall","title":"Logging in from outside the UCL firewall","text":"<p>You will need to either use the UCL Virtual Private Network or ssh in to UCL's Gateway system <code>ssh-gateway.ucl.ac.uk</code> first. From there you can then ssh in to our systems. </p> <pre><code>ssh &lt;your_UCL_user_id&gt;@ssh-gateway.ucl.ac.uk\nssh &lt;your_UCL_user_id&gt;@&lt;system_name&gt;.rc.ucl.ac.uk\n</code></pre> <p>Advanced: If you find you need to go via the Gateway often, you can set up this jump automatically, see Single-step logins using tunnelling</p>"},{"location":"howto/#login-problems","title":"Login problems","text":"<p>If you experience difficulties with your login, please make sure that you are typing your UCL user ID and your password correctly. If you have recently updated your password, it takes some hours to propagate to all UCL systems.</p> <p>If you still cannot get access but can access other UCL services like the SSH Gateway, please contact us on rc-support@ucl.ac.uk. Your account may have expired, or you may have gone over quota.</p> <p>If you cannot access anything, please see UCL MyAccount - you may need to request a password reset from the Service Desk. </p> <p>If you get a host key error message, you will need to delete old host keys - continue reading!</p>"},{"location":"howto/#remote-host-identification-has-changed","title":"Remote host identification has changed","text":"<p>When you log in via SSH, it keeps a record of the host key for the server you logged in to in  your <code>.ssh/known_hosts</code> file in your home directory, on the machine you are logging in from.  This helps make sure you are connecting directly to the server you think you are, but can cause  warnings to show up if the host key on that machine has genuinely changed (usually because of an  update or reinstall).</p> <p>Check the host key warning against our current key fingerprints:</p> <p>The error message looks like this if you are using OpenSSH in a terminal:</p> <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\nThe fingerprint for the ECDSA key sent by the remote host is\nSHA256:7FTryal3mIhWr9CqM3EPPeXsfezNk8Mm8HPCCAGXiIA.\nPlease contact your system administrator.\nAdd correct host key in /Users/uccaxxx/.ssh/known_hosts to get rid of this message.\nOffending ECDSA key in /Users/uccaxxx/.ssh/known_hosts:11\nECDSA host key for myriad.rc.ucl.ac.uk has changed and you have requested strict checking.\nHost key verification failed.\nKilled by signal 1.\n</code></pre> <p>This tells you that the old key is in line 11 of your <code>known_hosts</code> file.  Sometimes it will give you a direct command you can run to remove that specific key:</p> <pre><code>ssh-keygen -R myriad.rc.ucl.ac.uk\n</code></pre> <p>or you can manually delete line 11 yourself in a text editor.</p> <p>If you are logging in via the Gateway, you will need to remove the old key there too. On the Gateway,  <code>nano</code> and <code>vim</code> are available text editors. If you are not already familiar  with <code>vim</code>, use <code>nano</code> - it has the command shortcuts shown at the bottom, where <code>^O</code> means  press <code>Ctrl</code> and then the letter <code>o</code>.</p> <pre><code># to open the file for editing in nano\nnano ~/.ssh/known_hosts\n</code></pre> <p>Once you have removed the old host key you will be able to ssh in again. The first time  you log in to an unknown server you will get a message like this:</p> <pre><code>The authenticity of host 'myriad.rc.ucl.ac.uk can't be established.\nECDSA key fingerprint is SHA256:7FTryal3mIhWr9CqM3EPPeXsfezNk8Mm8HPCCAGXiIA.\nAre you sure you want to continue connecting (yes/no)?\n</code></pre> <p>Typing <code>yes</code> will allow you to continue logging in.</p> <p>PuTTY will display a warning and you can choose to continue or not.</p> <p>WinSCP will say <code>Server's host key does not match the one that WinSCP has in cache.</code>  and you will have the option to update the key.</p>"},{"location":"howto/#macos-connection-failures","title":"macOS connection failures","text":"<p>If you are on macOS and getting many ssh connection failures and broken pipe messages when trying to log in, try adding an ssh timeout to your ssh command:</p> <pre><code>ssh -o ConnectTimeout=10 &lt;your_UCL_user_id&gt;@myriad.rc.ucl.ac.uk\n</code></pre> <p>This has particularly been a problem with macOS Big Sur when using the VPN.</p>"},{"location":"howto/#how-do-i-log-out","title":"How do I log out?","text":"<p>You can log out of the systems by typing <code>exit</code> and pressing enter.</p> <p>(<code>logout</code> or pressing Ctrl+D also work)</p>"},{"location":"howto/#how-do-i-transfer-data-onto-the-system","title":"How do I transfer data onto the system?","text":"<p>You can transfer data to and from our systems using any program capable of using the Secure Copy (SCP) protocol. This uses the same SSH system as you use to log in to a command line session, but then transfers data over it. This means that if you can use SSH to connect to a system, you can usually use SCP to transfer files to it. </p>"},{"location":"howto/#copying-files-using-linux-or-macos","title":"Copying files using Linux or macOS","text":"<p>You can use the command-line utilities scp, sftp or rsync to copy your data about. You can also use a graphical client (Transmit, CyberDuck, FileZilla).</p>"},{"location":"howto/#scp","title":"scp","text":"<p>This will copy a data file from somewhere on your local machine to a specified location on the  remote machine (Myriad etc).</p> <pre><code>scp &lt;local_data_file&gt; &lt;remote_user_id&gt;@&lt;remote_hostname&gt;:&lt;remote_path&gt;\n</code></pre> <pre><code># Example: copy myfile from your local current directory into Scratch on Myriad\nscp myfile ccxxxxx@myriad.rc.ucl.ac.uk:~/Scratch/\n</code></pre> <p>This will do the reverse, copying from the remote machine to your local machine. (This is still run from your local machine).</p> <pre><code>scp &lt;remote_user_id&gt;@&lt;remote_hostname&gt;:&lt;remote_path&gt;&lt;remote_data_file&gt; &lt;local_path&gt;\n</code></pre> <pre><code># Example: copy myfile from Myriad into the Backups directory in your local current directory\nscp ccxxxxx@myriad.rc.ucl.ac.uk:~/Scratch/myfile Backups/\n</code></pre>"},{"location":"howto/#sftp","title":"sftp","text":"<p>You can use <code>sftp</code> to log in to the remote machine, navigate through directories and use <code>put</code> and <code>get</code> to copy files from and to your local machine. <code>lcd</code> and <code>lls</code> are local equivalents of <code>cd</code> and <code>ls</code> so you can navigate through your local directories as you go. </p> <pre><code>sftp &lt;remote_user_id&gt;@&lt;remote_hostname&gt;\ncd &lt;remote_path&gt;\nget &lt;remote_file&gt;\nlcd &lt;local_path&gt;\nput &lt;local_file&gt;\n</code></pre> <pre><code># Example: download a copy of file1 into your local current directory,\n# change local directory and upload a copy of file2\nsftp ccxxxxx@myriad.rc.ucl.ac.uk\ncd Scratch/files\nget file1\nlcd ../files_to_upload\nput file2\n</code></pre>"},{"location":"howto/#rsync","title":"rsync","text":"<p><code>rsync</code> is used to remotely synchronise directories, so can be used to only copy files which have changed. Have a look at <code>man rsync</code> as there are many options. </p>"},{"location":"howto/#copying-files-using-windows-and-winscp","title":"Copying files using Windows and WinSCP","text":"<p>WinSCP is a graphical client that you can use for <code>scp</code> or <code>sftp</code>.</p> <ol> <li>The login/create new session screen will open if this is the first time you are using WinSCP.</li> <li>You can choose SFTP or SCP as the file protocol. If you have an unstable connection with one, you may wish to try the other. SCP is probably generally better.</li> <li>Fill in the hostname of the machine you wish to connect to, your username and password.</li> <li>Click Save and give your settings a useful name.</li> <li>You'll then be shown your list of Stored sessions, which will have the one you just created.</li> <li>Select the session and click Login.</li> </ol>"},{"location":"howto/#mobaxterm","title":"MobaXterm","text":"<p>If using MobaXterm, you may need to set a password for the left side file manager  panel separately as well as for the main panel, to allow you to drag and drop  files and have them transferred to the cluster.</p>"},{"location":"howto/#transferring-files-from-outside-the-ucl-firewall","title":"Transferring files from outside the UCL firewall","text":"<p>As when logging in, when you are outside the UCL firewall you will need a method to connect inside it before you copy files.</p> <p>You can use the UCL Virtual Private Network and scp direct to our systems or you can do some form of SSH tunnelling.</p>"},{"location":"howto/#single-step-logins-using-tunnelling","title":"Single-step logins using tunnelling","text":""},{"location":"howto/#linux-unix-macos_1","title":"Linux / Unix / macOS","text":""},{"location":"howto/#on-the-command-line","title":"On the command line","text":"<pre><code># Log in to Myriad, jumping via the Gateway (replace ccxxxxx with your own username)\nssh -o ProxyJump=ccxxxxx@ssh-gateway.ucl.ac.uk ccxxxxx@myriad.rc.ucl.ac.uk\n</code></pre> <p>or</p> <pre><code># Copy 'my_file' from the machine you are logged in to into your Scratch on Grace\n# Replace ccxxxxx with your own username.\nscp -o ProxyJump=ccxxxxx@ssh-gateway.ucl.ac.uk my_file ccxxxxx@myriad.rc.ucl.ac.uk:~/Scratch/\n</code></pre> <p>This tunnels through the Gateway in order to get you to your destination - you'll be asked for your password twice, once for each machine. You can use this to log in or to copy files.</p> <p>You may also need to do this if you are trying to reach one cluster from another and there is a firewall in the way.</p>"},{"location":"howto/#using-a-config-file","title":"Using a config file","text":"<p>You can create a config file which does this without you needing to type it every time.</p> <p>Inside your <code>~/.ssh</code> directory on your local machine, add the below to your <code>config</code> file (or create a file called <code>config</code> if you don't already have one).</p> <p>Generally, it should be of this form where <code>&lt;name&gt;</code> can be anything you want to call this entry.</p> <pre><code>Host &lt;name&gt;\n   User &lt;remote_user_id&gt;\n   HostName &lt;remote_hostname&gt;\n   proxyCommand ssh -W &lt;remote_hostname&gt;:22 &lt;remote_user_id&gt;@ssh-gateway.ucl.ac.uk\n</code></pre> <p>This causes the commands you type in your client to be forwarded on over a secure channel to the specified remote host.</p> <p>Here are some examples - you can have as many of these as you need in your config file.</p> <pre><code>Host myriad\n   User ccxxxxx\n   HostName myriad.rc.ucl.ac.uk\n   proxyCommand ssh -W myriad.rc.ucl.ac.uk:22 ccxxxxx@ssh-gateway.ucl.ac.uk\n\nHost myriad12\n   User ccxxxxx\n   HostName login12.myriad.rc.ucl.ac.uk\n   proxyCommand ssh -W login12.myriad.rc.ucl.ac.uk:22 ccxxxxx@ssh-gateway.ucl.ac.uk\n\nHost aristotle\n   User ccxxxxx\n   HostName aristotle.rc.ucl.ac.uk\n   proxyCommand ssh -W aristotle.rc.ucl.ac.uk:22 ccxxxxx@ssh-gateway.ucl.ac.uk\n</code></pre> <p>You can now just type <code>ssh myriad</code> or <code>scp file1 aristotle:~</code> and you will go through the Gateway. You'll be asked for login details twice since you're logging in to two machines, a Gateway server and your endpoint.  </p>"},{"location":"howto/#windows-winscp","title":"Windows - WinSCP","text":"<p>WinSCP can also set up SSH tunnels.</p> <ol> <li>Create a new session as before, and tick the Advanced options box in the bottom left corner.</li> <li>Select Connection &gt; Tunnel from the left pane.</li> <li>Tick the Connect through SSH tunnel box and enter the hostname of the gateway you are tunnelling through, for example ssh-gateway.ucl.ac.uk</li> <li>Fill in your username and password for that host. (Central UCL ones for the Gateway).</li> <li>Select Session from the left pane and fill in the hostname you want to end up on after the tunnel.</li> <li>Fill in your username and password for that host and set the file protocol to SCP.</li> <li>Save your settings with a useful name.</li> </ol>"},{"location":"howto/#creating-a-tunnel-that-other-applications-can-use","title":"Creating a tunnel that other applications can use","text":"<p>Some applications do not read your SSH config file and also cannot set up tunnels themselves, but can use one that you have created separately. FileZilla in particular is something you may want to do this with to transfer your files directly to the clusters from outside UCL using  a graphical client.</p>"},{"location":"howto/#ssh-tunnel-creation-using-a-terminal","title":"SSH tunnel creation using a terminal","text":"<p>You can do this in Linux, macOS and the Windows Command Prompt on Windows 10 and later.</p> <p>Set up a tunnel between a port on your local computer (this is using 3333 as it is unlikely to be in use, but you can pick different ones) to Myriad's port 22 (which is the standard port for ssh),  going via a UCL gateway.</p> <pre><code># replace ccxxxxx with your UCL username\nssh -L 3333:myriad.rc.ucl.ac.uk:22 ccxxxxx@ssh-gateway.ucl.ac.uk \n</code></pre> <p>You may also want to use the <code>-N</code> option to tell it not to execute any remote commands and  <code>-f</code> to put this command into the background if you want to continue to type other commands  into the same terminal.</p> <p>The tunnel now exists, and <code>localhost:3333</code> on your computer connects to Myriad.</p> <p>You can do this with ports other than 22 if you are not wanting to ssh in but to instead connect with a local browser to something running on Myriad. Here the port remains as 3333, something could be launched on that port on Myriad and your browser could be pointed at  <code>localhost:3333</code> to connect to it.</p> <pre><code># replace ccxxxxx with your UCL username\nssh -L 3333:myriad.rc.ucl.ac.uk:3333 ccxxxxx@ssh-gateway.ucl.ac.uk\n</code></pre> <p>Do not leave things like this running for long periods on the login nodes.</p>"},{"location":"howto/#ssh-tunnel-creation-using-putty","title":"SSH tunnel creation using PuTTY","text":"<p>On Windows you can also set up a tunnel using PuTTY.</p>"},{"location":"howto/#connect-to-your-tunnel-with-an-application-like-filezilla","title":"Connect to your tunnel with an application (like FileZilla)","text":"<p>You can then tell your application to connect to <code>localhost:3333</code> instead of Myriad. If it has  separate boxes for hostname and port, put <code>localhost</code> as the hostname and <code>3333</code> as the port.</p>"},{"location":"howto/#managing-your-quota","title":"Managing your quota","text":"<p>After using <code>lquota</code> to see your total usage, you may wish to find what is using all your space.</p> <p><code>du</code> is a command that gives you information about your disk usage. Useful options are:</p> <pre><code>du -ch &lt;dir&gt;\ndu -h --max-depth=1\n</code></pre> <p>The first will give you a summary of the sizes of directory tree and subtrees inside the directory you specify, using human-readable sizes with a total at the bottom. The second will show you the totals for all top-level directories relative to where you are, plus the grand total. These can help you track down the locations of large amounts of data if you need to reduce your disk usage.</p>"},{"location":"howto/#how-do-i-connect-to-ucl-group-folders-n-drive-or-other-smb-mounts","title":"How do I connect to UCL group folders, N drive or other smb mounts?","text":"<p>You may have data stored in a UCL group folder (S drive) that you normally mount using smb.  You can use <code>smbclient</code> to copy the files across onto Myriad (you do want them to be copied  onto Myriad before you run any jobs using them, otherwise the compute node will be sitting  there waiting for the copy to complete before it can do anything useful).</p> <p>If the address you are trying to mount looks like <code>smb://ad.ucl.ac.uk/groupfolders</code> then you would do this:</p> <pre><code>smbclient //ad.ucl.ac.uk/groupfolders \n</code></pre> <p>This will give you a prompt where you can access that storage in an ftp-like way, where you  can use <code>get</code> commands to copy files from there on to Myriad, or <code>put</code> commands to copy data into there from Myriad.</p> <p>You can look at <code>man smbclient</code> on Myriad for the manual.</p> <p>If you get an error like this:</p> <pre><code>\\ad.ucl.ac.ukgroupfolders: Not enough '\\' characters in service\n</code></pre> <p>then you need to change the format from <code>\\\\ad.ucl.ac.uk\\groupfolders</code> to  <code>//ad.ucl.ac.uk/groupfolders</code> instead.</p>"},{"location":"howto/#ucl-n-drive","title":"UCL N drive","text":"<p>The N drive (Windows filestore, Filestore@UCL) can also be accessed using smb. To find out what  smb address to use, look at Adding your home N drive to a Linux machine.  Then use <code>smbclient</code> as above to connect to it.</p>"},{"location":"howto/#how-do-i-connect-out-to-an-ftp-server","title":"How do I connect out to an FTP server?","text":"<p>You cannot connect in to Myriad using FTP (we only allow SFTP access) but you can connect out  to FTP servers run by other people. </p> <p>Load the GNU inetutils module which provides ftp, telnet and tftp clients.</p> <pre><code>module load inetutils/1.9.4\n\n# connect to your desired server\nftp servername.ac.uk\n</code></pre> <p>You can then use <code>put</code> and <code>get</code> commands to put data on the remote FTP server or download it from there to Myriad. </p>"},{"location":"howto/#how-do-i-submit-a-job-to-the-scheduler","title":"How do I submit a job to the scheduler?","text":"<p>To submit a job to the scheduler you need to write a jobscript that contains the resources the job is asking for and the actual commands you want to run. This jobscript is then submitted using the <code>qsub</code> command.</p> <pre><code>qsub myjobscript\n</code></pre> <p>It will be put in to the queue and will begin running on the compute nodes at some point later when it has been allocated resources.</p>"},{"location":"howto/#passing-in-qsub-options-on-the-command-line","title":"Passing in qsub options on the command line","text":"<p>The <code>#$</code> lines in your jobscript are options to qsub. It will take each line which has <code>#$</code> as the first two characters and use the contents beyond that as an option. </p> <p>You can also pass options directly to the qsub command and this will override the settings in your script. This can be useful if you are scripting your job submissions in more complicated ways.</p> <p>For example, if you want to change the name of the job for this one instance of the job you can submit your script with:</p> <pre><code>qsub -N NewName myscript.sh\n</code></pre> <p>Or if you want to increase the wall-clock time to 24 hours:</p> <pre><code>qsub -l h_rt=24:0:0 myscript.sh\n</code></pre> <p>You can submit jobs with dependencies by using the <code>-hold_jid</code> option. For example, the command below submits a job that won't run until job 12345 has finished:</p> <pre><code>qsub -hold_jid 12345 myscript.sh\n</code></pre> <p>You may specify node type with the <code>-ac allow=</code> flags as below: </p> <pre><code>qsub -ac allow=L myscript.sh\n</code></pre> <p>This command tells this GPU job to only run the type L nodes which have Nvidia A100s</p> <pre><code>qsub -ac allow=EF myscript.sh\n</code></pre> <p>This tells this GPU job to only run on the type E and F nodes which have Nvidia V100s.</p> <p>Note that for debugging purposes, it helps us if you have these options inside your jobscript rather than passed in on the command line whenever possible. We (and you) can see the exact jobscript that was submitted for every job that ran but not what command line options you submitted it with.</p>"},{"location":"howto/#checking-your-previous-jobscripts","title":"Checking your previous jobscripts","text":"<p>If you want to check what you submitted for a specific job ID, you can do it with the <code>scriptfor</code> utility.</p> <pre><code>scriptfor 12345\n</code></pre> <p>As mentioned above, this will not show any command line options you passed in.</p>"},{"location":"howto/#how-do-i-monitor-a-job","title":"How do I monitor a job?","text":""},{"location":"howto/#qstat","title":"qstat","text":"<p>The <code>qstat</code> command shows the status of your jobs. By default, if you run it with no options, it shows only your jobs (and no-one else\u2019s). This makes it easier to keep track of your jobs. </p> <p>The output will look something like this:</p> <pre><code>job-ID  prior   name       user         state submit/start at     queue                          slots ja-task-ID \n-----------------------------------------------------------------------------------------------------------------\n123454 2.00685 DI_m3      ccxxxxx      Eqw   10/13/2017 15:29:11                                    12 \n123456 2.00685 DI_m3      ccxxxxx      r     10/13/2017 15:29:11 Yorick@node-x02e-006               24 \n123457 2.00398 DI_m2      ucappka      qw    10/12/2017 14:42:12                                    1 \n</code></pre> <p>This shows you the job ID, the numeric priority the scheduler has assigned to the job, the name you have given the job, your username, the state the job is in, the date and time it was submitted at (or started at, if it has begun), the head node of the job, the number of 'slots' it is taking up, and if it is an array job the last column shows the task ID.</p> <p>The queue name (<code>Yorick</code> here) is generally not useful. The head node name (<code>node-x02e-006</code>) is useful - the <code>node-x</code> part tells you this is an X-type node. </p> <p>If you want to get more information on a particular job, note its job ID and then use the -f and -j flags to get full output about that job. Most of this information is not very useful.</p> <pre><code>qstat -f -j 12345\n</code></pre>"},{"location":"howto/#job-states","title":"Job states","text":"<ul> <li><code>qw</code>: queueing, waiting</li> <li><code>r</code>: running</li> <li><code>Rq</code>: a pre-job check on a node failed and this job was put back in the queue</li> <li><code>Rr</code>: this job was rescheduled but is now running on a new node</li> <li><code>Eqw</code>: there was an error in this jobscript. This will not run.</li> <li><code>t</code>: this job is being transferred</li> <li><code>dr</code>: this job is being deleted</li> </ul> <p>Many jobs cycling between <code>Rq</code> and <code>Rr</code> generally means there is a dodgy compute node which is failing pre-job checks, but is free so everything tries to run there. In this case, let us know and we will investigate. </p> <p>If a job stays in <code>t</code> or <code>dr</code> state for a long time, the node it was on is likely to be unresponsive - again let us know and we'll investigate.</p> <p>A job in <code>Eqw</code> will remain in that state until you delete it - you should first have a look at what the error was with <code>qexplain</code>.</p>"},{"location":"howto/#qexplain","title":"qexplain","text":"<p>This is a utility to show you the non-truncated error reported by your job. <code>qstat -j</code> will show you a truncated version near the bottom of the output.</p> <pre><code>qexplain 123454\n</code></pre>"},{"location":"howto/#qdel","title":"qdel","text":"<p>You use <code>qdel</code> to delete a job from the queue.</p> <pre><code>qdel 123454\n</code></pre> <p>You can delete all your jobs at once:</p> <pre><code>qdel '*'\n</code></pre>"},{"location":"howto/#more-scheduler-commands","title":"More scheduler commands","text":"<p>Have a look at <code>man qstat</code> and note the commands shown in the <code>SEE ALSO</code> section of the manual page. Exit the manual page and then look at the <code>man</code> pages for those. (You will not be able to run all commands).</p>"},{"location":"howto/#nodesforjob","title":"nodesforjob","text":"<p>This is a utility that shows you the current percentage load, memory used and swap used on the nodes your job is running on. If your job is sharing the node with other people's jobs, it will show you the total resources in use, not just those used by your job. This is a snapshot of the current time and resource usage may change over the course of your job. Bear in mind that memory use in particular can increase over time as your job runs.</p> <p>If a cluster has hyperthreading enabled and you aren't using it, full load will show as 50% and not 100% - this is normal and not a problem.</p> <p>For a parallel job, very low (or zero) usage of any of the nodes suggests your job is either not capable of running over multiple nodes, or not partitioning its work effectively - you may be asking for more cores than it can use, or asking for a number of cores that doesn't fit well into the node sizes, leaving many idle.</p> <pre><code>[uccacxx@login02 ~]$ nodesforjob 1234\nNodes for job 1234:\n  Primary:\n    node-r99a-238:  103.1 % load, 12.9 % memory used, 0.1% swap used\n  Secondaries:\n    node-r99a-206:  1.7 % load, 1.6 % memory used, 0.1% swap used\n    node-r99a-238:  103.1 % load, 12.9 % memory used, 0.1% swap used\n    node-r99a-292:  103.1 % load, 12.9 % memory used, 0.1% swap used\n    node-r99a-651:  1.6 % load, 3.2 % memory used, 0.1% swap used\n</code></pre> <p>The above example shows a multi-node job, so all the usage belongs to this job itself. It is  running on four nodes, and node-r99a-238 is the head node (the one that launched the job) and  shows up in both Primary and Secondaries. The load is very unbalanced - it is using two nodes  flat out, and two are mostly doing nothing. Memory use is low. Swap use is essentially zero.</p>"},{"location":"howto/#jobhist","title":"jobhist","text":"<p>Once a job ends, it no longer shows up in <code>qstat</code>. To see information about your finished jobs -  when they started, when they ended, what node they ran on - use the command <code>jobhist</code>, part of the <code>userscripts</code> module.</p> <pre><code>[uccacxx@login02 ~]$ jobhist\n        FSTIME        |       FETIME        |   HOSTNAME    |  OWNER  | JOB NUMBER | TASK NUMBER | EXIT STATUS |   JOB NAME    \n----------------------+---------------------+---------------+---------+------------+-------------+-------------+---------------\n  2020-06-17 16:31:12 | 2020-06-17 16:34:19 | node-h00a-010 | uccacxx |    3854822 |           0 |           0 | m_job   \n  2020-06-17 16:56:50 | 2020-06-17 16:56:52 | node-d00a-023 | uccacxx |    3854836 |           0 |           1 | k_job  \n  2020-06-17 17:21:12 | 2020-06-17 17:21:46 | node-d00a-012 | uccacxx |    3854859 |           0 |           0 | k_job\n</code></pre> <p><code>FSTIME</code> - when the job started running on the node <code>FETIME</code> - when the job ended <code>HOSTNAME</code> - the head node of the job (if it ran on multiple nodes, it only lists the first) <code>TASK NUMBER</code> - if it was an array job, it will have a different number here for each task</p> <p>This shows jobs that finished in the last 24 hours by default. You can search for longer as well:</p> <pre><code>jobhist --hours=200\n</code></pre> <p>If a job ended and didn't create the files you expect, check the start and end times to see whether  it ran out of wallclock time. </p> <p>If a job only ran for seconds and didn't produce the expected output, there was probably something  wrong in your script - check the <code>.o</code> and <code>.e</code> files in the directory you submitted the job from  for errors.</p>"},{"location":"howto/#how-do-i-run-interactive-jobs","title":"How do I run interactive jobs?","text":"<p>Sometimes you need to run interactive programs, sometimes with a GUI. This can be achieved through <code>qrsh</code>.  We have a detailed guide to running interactive jobs.</p>"},{"location":"howto/#how-do-i-estimate-what-resources-to-request-in-my-jobscript","title":"How do I estimate what resources to request in my jobscript?","text":"<p>It can be difficult to know where to start when estimating the resources your job will need. One way you can find out what resources your jobs need is to submit one job which requests far more than you think necessary, and gather data on what it actually uses. If you aren't sure what 'far more' entails, request the maximum wallclock time and job size that will fit on one node, and reduce this after you have some idea. In the case for array jobs, each job in the array is treated independently by the scheduler and are each allocated the same resources as are requested. For example, in a job array of 40 jobs requesting for 24 hours wallclock time and 3GB ram, each job in the array will be allocated 24 hours wallclock time and 3GB ram. Wallclock time does not include the time spent waiting in the queue.</p> <p>Run your program as:</p> <pre><code> /usr/bin/time --verbose myprogram myargs\n</code></pre> <p>where <code>myprogram myargs</code> is however you normally run your program, with whatever options you pass to it.</p> <p>When your job finishes, you will get output about the resources it used and how long it took - the relevant one for memory is <code>maxrss</code> (maximum resident set size) which roughly tells you the largest amount of memory it used.</p> <p>If your job is not completing successfully or you need to know how the memory usage  changes throughout the job, there is a tool called Ruse that can measure this for you.</p> <p>Run your program as:</p> <pre><code>module load ruse/2.0\n\n# sample the current memory usage every 120s and output each step to stdout\nruse --stdout --time=120 -s myprogram myargs\n</code></pre> <p>where <code>myprogram myargs</code> is however you normally run your program, with whatever options you  pass to it.</p> <p>Remember that memory requests in your jobscript are always per core, so check the total you are requesting is sensible - if you increase it too much you may end up with a job that cannot be submitted.</p> <p>You can also look at nodesforjob while a job is running to see a snapshot of the memory, swap and load on the nodes your job is running on.</p>"},{"location":"howto/#how-can-i-see-what-types-of-node-a-cluster-has","title":"How can I see what types of node a cluster has?","text":"<p>As well as looking at the cluster-specific page in this documentation for more details (for example  Myriad), you can run <code>nodetypes</code>, which will give you basic information about  the nodes that exist in that cluster.</p> <pre><code>[uccacxx@login12 ~]$ nodetypes\n    3 type * nodes: 36 cores, 188.4G RAM\n    7 type B nodes: 36 cores,   1.5T RAM\n   66 type D nodes: 36 cores, 188.4G RAM\n    9 type E nodes: 36 cores, 188.4G RAM\n    1 type F nodes: 36 cores, 188.4G RAM\n   55 type H nodes: 36 cores, 188.4G RAM\n    3 type I nodes: 36 cores,   1.5T RAM\n    2 type J nodes: 36 cores, 188.4G RAM\n</code></pre> <p>This shows how many of each letter-labelled nodetype the cluster has, then the number of cores  and amount of memory the node is reporting it has. It also shows the cluster has some utility  nodes - those are part of the infrastructure. The <code>*</code> nodes are the login nodes.</p>"},{"location":"howto/#how-do-i-run-a-graphical-program","title":"How do I run a graphical program?","text":"<p>To run a graphical program on the cluster and be able to view the user interface on your own  local computer, you will need to have an X-Windows Server installed on your local computer and  use X-forwarding.</p>"},{"location":"howto/#x-forwarding-on-linux","title":"X-forwarding on Linux","text":"<p>Desktop Linux operating systems already have X-Windows installed, so you just need to ssh in with the correct flags.</p> <p>You need to make sure you use either the <code>-X</code> or <code>-Y</code> (look at <code>man ssh</code> for details) flags on all ssh commands you run to establish a connection to the cluster.</p> <p>For example, connecting from outside of UCL:</p> <pre><code>ssh -X &lt;your_UCL_user_id&gt;@ssh-gateway.ucl.ac.uk\n</code></pre> <p>and then</p> <pre><code>ssh -X &lt;your_UCL_user_id&gt;@myriad.rc.ucl.ac.uk\n</code></pre> <p>A video walkthrough of running remote applications using X11, X-forwarding on compute nodes.</p>"},{"location":"howto/#x-forwarding-on-macos","title":"X-forwarding on macOS","text":"<p>You will need to install XQuartz to provide an X-Window System for macOS. (Previously known as X11.app).</p> <p>You can then follow the Linux instructions using Terminal.app.</p>"},{"location":"howto/#x-forwarding-on-windows","title":"X-forwarding on Windows","text":"<p>You will need:</p> <ul> <li>An SSH client; e.g., PuTTY</li> <li>An X server program; e.g., Exceed, Xming</li> </ul> <p>Exceed is available on Desktop@UCL machines and downloadable from the UCL software database. Xming is open source (and mentioned here without testing).</p>"},{"location":"howto/#exceed-on-desktopucl","title":"Exceed on Desktop@UCL","text":"<ol> <li>Load Exceed. You can find it under Start &gt; All Programs &gt; Applications O-P &gt; Open Text Exceed 14 &gt; Exceed</li> <li>Open PuTTY (Applications O-P &gt; PuTTY)</li> <li>In PuTTY, set up the connection with the host machine as usual:<ul> <li>Host name: <code>myriad.rc.ucl.ac.uk</code> (for example)</li> <li>Port: <code>22</code></li> <li>Connection type: <code>SSH</code></li> </ul> </li> <li>Then, from the Category menu, select Connection &gt; SSH &gt; X11 for 'Options controlling SSH X11 forwarding'.<ul> <li>Make sure the box marked 'Enable X11 forwarding' is checked.</li> </ul> </li> <li>Return to the session menu and save these settings with a new identifiable name for reuse in future.</li> <li>Click 'Open' and login to the host as usual</li> <li>To test that X-forwarding is working, try running <code>nedit</code> which is a text editor in our default modules.</li> </ol> <p>If <code>nedit</code> works, you have successfully enabled X-forwarding for graphical applications.</p>"},{"location":"howto/#installing-xming","title":"Installing Xming","text":"<p>Xming is a popular open source X server for Windows. These are instructions for using it alongside PuTTY. Other SSH clients and X servers are available. We cannot verify how well it may be working.</p> <ol> <li>Install both PuTTY and Xming if you have not done so already. During Xming installation, choose not to install an SSH client.</li> <li>Open Xming - the Xming icon should appear on the task bar.</li> <li>Open PuTTY</li> <li>Set up PuTTY as shown in the Exceed section.</li> </ol>"},{"location":"Background/Cluster_Computing/","title":"Cluster Computing","text":""},{"location":"Background/Cluster_Computing/#what-is-a-cluster","title":"What is a cluster?","text":"<p>In this context, a cluster is a collection of computers (often referred to as \"nodes\").  They're networked together with some shared storage and a scheduling system that lets people  run programs on them without having to enter commands \"live\".</p> <p>The UCL Moodle course \"ARC - Introduction to High Performance Computing at UCL\" has a video explanation of this here: (Moodle) (UCL users)</p> <p>This video is also available here: (mediacentral) (non-UCL users)</p>"},{"location":"Background/Cluster_Computing/#why-would-i-want-to-use-one","title":"Why would I want to use one?","text":"<p>Some researchers have programs that require a lot of compute power, like simulating weather patterns or the quantum behaviour of molecules.</p> <p>Others have a lot of data to process, or need to simulate a lot of things at once, like simulating the spread of disease or assembling parts of DNA into a genome.</p> <p>Often these kinds of work are either impossible or would take far too long to do on a desktop or laptop computer, as well as making the computer unavailable to do everyday tasks like writing documents or reading papers.</p> <p>By running the programs on the computers in a cluster, researchers can use many powerful computers at once, without locking up their own one.</p>"},{"location":"Background/Cluster_Computing/#how-do-i-use-it","title":"How do I use it?","text":"<p>Most people use something like the following workflow:</p> <ul> <li>connect to the cluster's \"login nodes\"</li> <li>create a script of commands to run programs</li> <li>submit the script to the scheduler</li> <li>wait for the scheduler to find available \"compute nodes\" and run the script</li> <li>look at the results in the files the script created</li> </ul> <p>Most people connect using a program called a Secure Shell Client (\"ssh client\" for short), but some programs, like Matlab and Comsol, run on your own computer and can be set up to send work to the cluster automatically.  That can be a little tricky to get working, though.</p> <p>The ssh client gives you a command prompt when you can enter text commands, but you can also tell it to  pass graphical windows through the network connection, using a system called X-Forwarding. This can be useful for visualising your data without transferring all the files back, but the network  connection makes it a bit slower to use than running it on your own computer. You'll need an X server on your own computer to use this: check our page on X-Forwarding for details.</p> <p>Please be aware that login nodes are shared resources, so users should not be running memory intensive jobs nor jobs with long runtimes in the login node. Doing so may negatively impact the performance of the login node for other users. Hence, identified culprit user processes are systematically killed.</p>"},{"location":"Background/Data_Storage/","title":"Data Storage","text":"<p>Our clusters have local parallel filesystems consisting of  your home and Scratch directories where you can write data. These are \"close to\" the  compute, connected to it with a fast network.</p> <p>They may also have local storage on the compute node that can be used during your job.</p>"},{"location":"Background/Data_Storage/#on-cluster-storage","title":"On-cluster storage","text":""},{"location":"Background/Data_Storage/#home","title":"Home","text":"<p>Every user has a home directory. This is the directory you are in when you first log in.</p> <ul> <li>Location: <code>/home/&lt;username&gt;</code></li> <li>May also be referred to as: <code>~</code>, <code>$HOME</code>.</li> </ul> <p>Many programs will write hidden config files in here, with names beginning with <code>.</code> (eg <code>.config</code>, <code>.cache</code>). You can see these with <code>ls -al</code>.</p>"},{"location":"Background/Data_Storage/#scratch","title":"Scratch","text":"<p>Every user also has a Scratch directory. It is intended that this is a larger space to keep your working files as you do your research, but should not be relied on for secure long-term permanent storage.</p> <p>Important data should be regularly backed up to another location.</p> <ul> <li>Location: <code>/scratch/scratch/&lt;username&gt;</code></li> <li>Also at: <code>/home/&lt;username&gt;/Scratch</code> (a shortcut or symbolic link to the first location).</li> </ul>"},{"location":"Background/Data_Storage/#temporary-storage-for-jobs-tmpdir","title":"Temporary storage for jobs (TMPDIR)","text":"<p>If the cluster you are on is not described in its about page as being diskless, the compute nodes will have local disks that can be written to during your job. </p> <p>This is because a local disk is faster to write to than a parallel filesystem. </p> <p>This location is created on the compute node when your job begins, and deleted again when it ends. You will need to copy the data back out of it before the end of your job into your Scratch. If your job fails or runs out of time, you will not be able to recover this data.</p> <ul> <li>Location: <code>$TMPDIR</code></li> <li>Will only exist during your job and be deleted after.</li> </ul>"},{"location":"Background/Data_Storage/#arc-cluster-file-system-acfs","title":"ARC Cluster File System (ACFS)","text":"<p>The ARC Cluster File System (ACFS) is ARC's centralised storage system that will be available from multiple ARC systems.</p> <p>It is the backed-up location for data which you wish to keep.</p> <p>The ACFS is available read-write on the login nodes but read-only on the compute nodes. This means that your jobs can read from it, but not write to it, and it is intended that you copy data onto it after deciding which outputs from your jobs are important to keep.</p> <p>Initially rolled out on Kathleen, you will later be able to access it from Myriad too and see the same files from both clusters.</p> <ul> <li>Location: <code>/acfs/users/&lt;username&gt;</code></li> <li>Also at: <code>/home/&lt;username&gt;/ACFS</code> (a shortcut or symbolic link to the first location).</li> <li>Backed up daily.</li> </ul>"},{"location":"Background/Parallel_Filesystems/","title":"Parallel Filesystems","text":""},{"location":"Background/Parallel_Filesystems/#what-is-different-about-parallel-filesystems","title":"What is different about parallel filesystems?","text":"<p>Parallel filesystems are designed to be able to deal with being written to by many users' jobs at once. To do this they store the files in a distributed fashion across many servers and disks and have metadata servers to keep track of this. This can cause access to individual files to be slower than on your local computer, especially if you have an SSD.</p>"},{"location":"Background/Parallel_Filesystems/#working-effectively-with-parallel-filesystems","title":"Working effectively with parallel filesystems","text":"<p>Parallel filesystems do not deal well with directories full of very many small files. Running a <code>ls</code> command to list the contents of such a directory can take a notably long time to show results because it has to look up the metadata for everything in there. This will affect anything that is trying to use files in that directory, including your jobs and any moving, deleting or copying files you may be doing to tidy it up.</p> <p>Parallel filesystems perform better when accessing single large files instead.</p> <p>Parallel filesystems will also begin to perform less well as they get fuller. On our Lustre filesystems,  75% full is a danger point where performance will be significantly impacted. To allow everyone's jobs  to keep working as intended, we need to keep filesystem usage below that point.</p>"},{"location":"Background/Parallel_Filesystems/#tips-for-use","title":"Tips for use","text":"<ul> <li>Use different directories for different jobs. Do not write everything to the same place.</li> <li>If you are on a cluster with disks for local temporary storage on the nodes (<code>$TMPDIR</code>) then your   jobs can write temporary data there, faster, and you can decide what you need to keep and copy back   to Scratch at the end.</li> <li>Clear up Scratch after your jobs. Keep the files you need, archive or delete the ones you do not.</li> <li>Archive and compress directory trees you aren't currently using. (<code>tar</code> command for example). This   stores all their contents as one file, and compressing it saves space.</li> <li>Back up your important data to somewhere off the cluster regularly.</li> <li>If you haven't used particular files for some months and do not expect to in the near future, keep   them off-cluster and delete the copies on the cluster.</li> <li>If you are no longer using a cluster, remove your data to maintain filesystem performance and allow   the space to be used by current active users.</li> <li>Before you leave UCL, please consider what should happen to your data, and take steps to put it in   a Research Data archive and/or ensure that your colleagues are given access to it.</li> </ul>"},{"location":"Background/Parallel_Filesystems/#what-filesystems-are-available","title":"What filesystems are available?","text":"<p>See Data Storage.</p>"},{"location":"Clusters/Acknowledging_RC_Systems/","title":"Acknowledging the Use of RC Systems","text":"<p>To keep running our services, we depend on being able to demonstrate that they are used in published research.</p> <p>When preparing papers describing work that has used any of our clusters or services, please use the terms below, especially the \"service@UCL\" label, so that we can easily search for them.</p>"},{"location":"Clusters/Acknowledging_RC_Systems/#kathleen","title":"Kathleen","text":"<p>\"The authors acknowledge the use of the UCL Kathleen High Performance Computing Facility (Kathleen@UCL), and associated support services, in the completion of this work.\"</p>"},{"location":"Clusters/Acknowledging_RC_Systems/#myriad","title":"Myriad","text":"<p>\"The authors acknowledge the use of the UCL Myriad High Performance Computing Facility (Myriad@UCL), and associated support services, in the completion of this work.\"</p>"},{"location":"Clusters/Acknowledging_RC_Systems/#aristotle","title":"Aristotle","text":"<p>\"The authors acknowledge the use of the UCL Aristotle Computing Facility (Aristotle@UCL), and associated support services, in the completion of this work.\"</p>"},{"location":"Clusters/Acknowledging_RC_Systems/#mmm-hub-young","title":"MMM Hub Young","text":"<p>Please find the appropriate wording at Acknowledging the use of Young in publications.</p>"},{"location":"Clusters/Acknowledging_RC_Systems/#michael","title":"Michael","text":"<p>Please find the appropriate wording at Acknowledging the use of Michael in publications.</p>"},{"location":"Clusters/Kathleen/","title":"Kathleen","text":"<p>Kathleen is a compute cluster designed for extensively parallel, multi-node batch-processing jobs, having high-bandwidth connections between each individual node. It is named after Professor Dame Kathleen Lonsdale, a pioneering chemist and activist, and was installed in December 2019. It went into service at the end of January 2020.</p>"},{"location":"Clusters/Kathleen/#accounts","title":"Accounts","text":"<p>Kathleen accounts can be applied for via the Research Computing sign up process.</p> <p>As Kathleen is intended for multi-node jobs, users who specify that they will need to use multi-node jobs (e.g. with MPI) will be given access to Kathleen.</p>"},{"location":"Clusters/Kathleen/#logging-in","title":"Logging in","text":"<p>Please use your UCL username and password to connect to Kathleen with an SSH client.</p> <pre><code>ssh uccaxxx@kathleen.rc.ucl.ac.uk\n</code></pre> <p>If using PuTTY, put <code>kathleen.rc.ucl.ac.uk</code> as the hostname and your seven-character username (with no @ after) as the username when logging in, eg. <code>uccaxxx</code>. When entering your password in PuTTY no characters or bulletpoints will show on screen - this is normal.</p> <p>If you are outside the UCL firewall you will need to follow the instructions for Logging in from outside the UCL firewall.</p> <p>The login nodes allow you to manage your files, compile code and submit jobs. Very short (\\&lt;15mins) and non-resource-intensive software tests can be run on the login nodes, but anything more should be submitted as a job.</p>"},{"location":"Clusters/Kathleen/#logging-in-to-a-specific-node","title":"Logging in to a specific node","text":"<p>You can access a specific Kathleen login node by using their dedicated addresses instead of the main <code>kathleen.rc.ucl.ac.uk</code> address, for example:</p> <pre><code>ssh\u00a0uccaxxx@login01.kathleen.rc.ucl.ac.uk\n</code></pre> <p>The main address will unpredictably direct you to either one of these (to balance load), so if you need multiple sessions on one, this lets you do that.</p>"},{"location":"Clusters/Kathleen/#copying-data-onto-kathleen","title":"Copying data onto Kathleen","text":"<p>You will need to use an SCP or SFTP client to copy data onto Kathleen. Please refer to the page on How do I transfer data onto the system?</p> <p>If you find you cannot connect directly from one cluster to another, this is  generally because of firewalls in between and so you need to use tunnelling with the scp command.</p>"},{"location":"Clusters/Kathleen/#quotas","title":"Quotas","text":"<p>On Kathleen you have a single 250GB quota by default which covers your home and Scratch. </p> <p>This is a hard quota: once you reach it, you will no longer be able to write more data. Keep an eye on it, as this will cause jobs to fail if they cannot create their .o or .e files at the start, or their output files partway through.</p> <p>You can check your quota on Kathleen by running:</p> <pre><code>lquota\n</code></pre> <p>which will give you output similar to this:</p> <pre><code>     Storage        Used        Quota   % Used   Path\n      lustre  146.19 MiB   250.00 GiB       0%   /home/uccaxxx\n</code></pre> <p>You can apply for quota increases using the form at Additional Resource Requests.</p> <p>Here are some tips for managing your quota and finding where space is being used.</p>"},{"location":"Clusters/Kathleen/#job-sizes-and-durations","title":"Job sizes and durations","text":"<p>Please consider that Kathleen nodes have 40 physical cores - 2 nodes is 80 cores. Jobs do not share nodes, so although asking for 41 cores is possible, it means you are wasting the other 39 cores on your second node!</p> <p>For interactive and batch jobs:</p> Cores Max. Duration 41-240 48h 241-480 24h 481-5760 12h <p>These are numbers of physical cores.</p> <p>If you have a workload that requires longer jobs than this, you may be able to apply to our governance group for access to a longer queue. Applications will be expected to demonstrate that their work cannot be run using techniques like checkpointing that would allow their workload to be broken up into smaller parts. Please see the section on Additional Resource Requests for more details.</p> <p>The memory you request is always per core, not the total amount. Each node has 192 gigabytes of RAM, and each node has 40 cores, e.g. a job requesting for 40 cores should ask for no more than 4.8G RAM (192/40).</p>"},{"location":"Clusters/Kathleen/#node-types","title":"Node types","text":"<p>Kathleen's compute capability comprises 192 diskless compute nodes each with two 20-core Intel Xeon Gold 6248 2.5GHz processors, 192 gigabytes of 2933MHz DDR4 RAM, and an Intel OmniPath network.</p> <p>Two nodes identical to these, but with two 1 terabyte hard-disk drives added, serve as the login nodes.</p>"},{"location":"Clusters/Kathleen/#hyperthreading","title":"Hyperthreading","text":"<p>Kathleen has hyperthreading enabled and you can choose on a per-job basis whether you want to use it.</p> <p>Hyperthreading lets you use two virtual cores instead of one physical core - some programs can take advantage of this. </p> <p>If you do not ask for hyperthreading, your job only uses one thread per core as normal.</p> <p>The <code>-l threads=</code> request is not a true/false setting, instead you are telling the scheduler you want one slot to block one virtual cpu instead of the normal situation where it blocks two. If you have a script with a threads request and want to override it on the command line or set it back to normal, the usual case is <code>-l threads=2</code>. (Setting threads to 0 does not disable hyperthreading!)</p> <pre><code># request hyperthreading in this job\n#$ -l threads=1\n\n# request the number of virtual cores\n#$ -pe mpi 160\n\n# request 2G RAM per virtual core\n#$ -l mem=2G\n\n# set number of OpenMP threads being used per MPI process \nexport OMP_NUM_THREADS=2\n</code></pre> <p>This job would be using 80 physical cores, using 80 MPI processes each of which would create two threads (on Hyperthreads).</p> <p>Note that memory requests are now per virtual core with hyperthreading enabled.  If you asked for <code>#$ -l mem=4G</code>on a node with 80 virtual cores and 192G RAM then  you are requiring 320G RAM in total which will not fit on that node and so you  would be given a sparse process layout across more nodes to meet this requirement.</p> <pre><code># request hyperthreading in this job\n#$ -l threads=1\n\n# request the number of virtual cores\n#$ -pe mpi 160\n\n# request 2G RAM per virtual core\n#$ -l mem=2G\n\n# set number of OpenMP threads being used per MPI process\n# (a whole node's worth)\nexport OMP_NUM_THREADS=80\n</code></pre> <p>This job would still be using 80 physical cores, but would use one MPI process per node which would create 80 threads on the node (on Hyperthreads).</p>"},{"location":"Clusters/Kathleen/#diskless-nodes","title":"Diskless nodes","text":"<p>Kathleen nodes are diskless (have no local hard drives) - there is no <code>$TMPDIR</code> available on Kathleen, so you should not request <code>-l tmpfs=10G</code> in your jobscripts or your job will be rejected at submit time.</p> <p>If you need temporary space, you should use somewhere in your Scratch.</p>"},{"location":"Clusters/Kathleen/#test-software-stack","title":"Test software stack","text":"<p>There is a test version of our next software stack available now on Kathleen. This has a small number of packages at present. What is in it and the names of modules are liable to change over time, so please do not rely on it for production work. Instead, please test whether the applications you intend to use work the way you would expect.</p> <p>This stack is built using Spack.</p> <p>To use:</p> <pre><code>module load beta-modules\nmodule load test-stack/2025-02\n</code></pre> <p>After that, when you type \"module avail\" there will be several sections of additional modules at the top of the output.</p> <p>Not everything contained in the stack is visible by default - we have made the applications that we expect people  to use directly visible and lots of their dependencies are hidden. These will show up if you search for that package  specifically, for example:</p> <pre><code>module avail libpng\n-------------------------- /shared/ucl/apps/spack/0.23/deploy/2025-02/modules/applications/linux-rhel7-cascadelake --------------------------\nlibpng/1.6.39/gcc-12.3.0-iopfrab  \n</code></pre> <p>This module does not show up in the full list but is still installed. It has a hash at the end of its name <code>-iopfrab</code> and this will change over time with different builds.</p> <p>If you find you are needing one of these modules often, let us know and we'll make it one that is not hidden in the  next release of this stack.</p>"},{"location":"Clusters/Michael/","title":"Michael","text":"<p>Michael is an extension to the UCL-hosted Hub for Materials and Molecular Modelling, an EPSRC-funded Tier 2 system providing large scale computation to UK researchers; and delivers computational capability for the Faraday Institution, a national institute for electrochemical energy storage science and technology.</p>"},{"location":"Clusters/Michael/#applying-for-an-account","title":"Applying for an account","text":"<p>Michael accounts belong to you as an individual and are applied for via David Scanlon who is the point of  contact for Michael.</p> <p>You will need to supply an SSH public key, which is the only method used to log in.</p>"},{"location":"Clusters/Michael/#creating-an-ssh-key-pair","title":"Creating an ssh key pair","text":"<p>An ssh key consists of a public and a private part, typically named id_rsa and id_rsa.pub by default. The public part is what we need. You must not share your private key with anyone else. You can copy it onto multiple machines belonging to you so you can log in from all of them (or you can have a separate pair for each machine).</p>"},{"location":"Clusters/Michael/#creating-an-ssh-key-in-linuxunixmacos","title":"Creating an ssh key in Linux/Unix/macOS","text":"<pre><code>ssh-keygen\u00a0-t\u00a0rsa\n</code></pre> <p>The defaults should give you a reasonable key. If you prefer to use ed25519 instead, and/or longer keys, you can. You can also tell it to create one with a different name, so it doesn't overwrite any existing key.</p> <ul> <li>Do not use DSA as OpenSSH 7.0 has deprecated it     and does not use it by default on client or server.      We no longer accept DSA keys.</li> </ul> <p>You will be asked to add a passphrase for your key. A blank passphrase is not recommended; if you use one please make sure that no one else ever has access to your local computer account. How often you are asked for a passphrase depends on how long your local ssh agent keeps it.</p> <p>You may need to run <code>ssh-add</code> to add the key to your agent so you can use it. If you aren't sure what keys your agent can see, running <code>ssh-add -L</code> will show all the public parts of the keys it is aware of.</p>"},{"location":"Clusters/Michael/#creating-an-ssh-key-in-windows","title":"Creating an ssh key in Windows","text":"<p>Have a look at Key-Based SSH Logins With PuTTY which has step-by-step instructions. You can choose whether to use Pageant or not to manage your key. You can again pick RSA, ED25519, ECDSA etc but do not pick SSH-1 as that is an old and insecure key type. As above, DSA is no  longer accepted. The key must be at least 2048-bit.</p> <p>If you are using Windows 10, then you probably have OpenSSH installed and could instead run ssh-keygen in a terminal per the Linux instructions and use the ssh command to log in instead of PuTTY.</p>"},{"location":"Clusters/Michael/#information-for-points-of-contact","title":"Information for Points of Contact","text":"<p>Points of Contact have some tools they can use to manage users and allocations, documented at MMM Points of Contact.</p>"},{"location":"Clusters/Michael/#logging-in","title":"Logging in","text":"<p>You will be assigned a personal username and your SSH key pair will be used to log in. External users will have a username in the form <code>mmmxxxx</code> (where <code>xxxx</code> is a number) and UCL users will use their central username.</p> <p>You connect with ssh directly to:</p> <pre><code>michael.rc.ucl.ac.uk\n</code></pre>"},{"location":"Clusters/Michael/#ssh-timeouts","title":"SSH timeouts","text":"<p>Idle ssh sessions will be disconnected after 7 days.</p>"},{"location":"Clusters/Michael/#using-the-system","title":"Using the system","text":"<p>Michael is a batch system. The login nodes allow you to manage your files, compile code and submit jobs. Very short (\\&lt;15mins) and non-resource-intensive software tests can be run on the login nodes, but anything more should be submitted as a job.</p>"},{"location":"Clusters/Michael/#full-user-guide","title":"Full user guide","text":"<p>Michael has the same user environment as RC Support's other clusters, so the User guide is relevant and is a good starting point for further information about how the environment works. Any variations that Michael has should be listed on this page.</p>"},{"location":"Clusters/Michael/#submitting-a-job","title":"Submitting a job","text":"<p>Create a job script for non-interactive use and submit your jobscript using qsub.  Jobscripts must begin <code>#!/bin/bash -l</code> in order to run as a login shell  and get your login environment and modules.</p> <p>A job on Michael must also specify what type of job it is (Gold, Free, Test) and the project it is being submitted for. (See Budgets and allocations below).</p>"},{"location":"Clusters/Michael/#memory-requests","title":"Memory requests","text":"<p>Note: the memory you request is always per core, not the total amount. If you ask for 128G RAM and 24 cores, that will run on 24 nodes using only one core per node. This allows you to have sparse process placement when you do actually need that much RAM per process.</p>"},{"location":"Clusters/Michael/#monitoring-a-job","title":"Monitoring a job","text":"<p>In addition to qstat, <code>nodesforjob $JOB_ID</code> can be useful to see what proportion of cpu/memory/swap is being used on the nodes a certain job is running on.</p> <p><code>qexplain $JOB_ID</code> will show you the full error for a job that is in <code>Eqw</code> status.</p>"},{"location":"Clusters/Michael/#useful-utilities","title":"Useful utilities","text":"<p>As well as <code>nodesforjob</code>, there are the following utilities which can help you find information about your jobs after they have run.</p> <ul> <li><code>jobhist</code> - shows your job history for the last 24hrs by default,     including start and end times and the head node it ran on. You can     view a longer history by specifying <code>--hours=100</code> for example.</li> <li><code>scriptfor $JOB_ID</code> - show the script that was submitted for the     given job.</li> </ul> <p>These utilities live in GitHub at https://github.com/UCL-ARC/go-clustertools and https://github.com/UCL-ARC/rcps-cluster-scripts</p>"},{"location":"Clusters/Michael/#software","title":"Software","text":"<p>Michael mounts the RC Systems software stack.</p> <p>Have a look at Software Guides for specific information on running some applications, including example scripts. The list there is not exhaustive.</p> <p>Access to software is managed through the use of modules.</p> <ul> <li><code>module avail</code> shows all modules available.</li> <li><code>module list</code> shows modules currently loaded.</li> </ul> <p>Access to licensed software may vary based on your host institution and project.</p>"},{"location":"Clusters/Michael/#requesting-software-installs","title":"Requesting software installs","text":"<p>To request software installs, email us at the support address below or open an issue on our GitHub. You can see what software has already been requested in the Github issues and can add a comment if you're also interested in something already requested.</p>"},{"location":"Clusters/Michael/#installing-your-own-software","title":"Installing your own software","text":"<p>You may install software in your own space. Please look at Compiling Your Code for tips.</p>"},{"location":"Clusters/Michael/#maintaining-a-piece-of-software-for-a-group","title":"Maintaining a piece of software for a group","text":"<p>It is possible for people to be given central areas to install software that they wish to make available to everyone or to a select group - generally because they are the developers or if they wish to use multiple versions or developer versions. The people given install access would then be responsible for managing and maintaining these installs.</p>"},{"location":"Clusters/Michael/#licensed-software","title":"Licensed software","text":"<p>Reserved application groups exist for software that requires them. The group name will begin with <code>leg</code> or <code>lg</code>. After we add you to one of these groups, the central group change will happen overnight. You can check your groups with the <code>groups</code> command.</p> <p>Please let us know your username when you ask to be added to a group.</p> <ul> <li>CASTEP: You/your group leader need to have signed up for a CASTEP license.     Send us an acceptance email, or we can ask them to verify you have a     license. You will then be added to the reserved application group     <code>lgcastep</code>. If you are a member of UKCP you are already covered by a     license and just need to tell us when you request access.</li> <li>CRYSTAL: You/your group leader need to have signed up for an     Academic license. Crystal Solutions will send an email saying an     account has been upgraded to \"Academic UK\" - forward that to us     along with confirmation from the group leader that you should be in     their group. You will be added to the <code>legcryst</code> group.</li> <li>DL_POLY: has individual licenses for specific versions. Sign up at DL_POLY's website     and send us the acceptance email they give you. We will add you to     the appropriate version's reserved application group, eg <code>lgdlp408</code>.</li> <li>Gaussian: not currently accessible for non-UCL institutions. UCL     having a site license and another institute having a site license     does not allow users from the other institute to run Gaussian on     UCL-owned hardware.</li> <li>VASP: When you request access you need to send us the email     address you are named on a VASP license using. You can also send      name and email of the main VASP license holder along with the license      number if you wish. We will then check in the VASP portal if we can      add you. We will add you to the <code>legvasp5</code> or <code>legvasp6</code> reserved      application groups depending on which versions you are licensed for.      You may also install your own copy in your home, and we provide a simple     build script on Github     (tested with VASP 5.4.4, no patches). You need to download the VASP     source code and then you can run the script following the     instructions at the top.</li> <li>Molpro: Only UCL users are licensed to use our central copy and     can request to be added to the <code>lgmolpro</code> reserved application group.</li> </ul>"},{"location":"Clusters/Michael/#suggested-job-sizes-on-original-michael","title":"Suggested job sizes on original Michael","text":"<p>The target job sizes for original Michael K-type nodes are 48-120 cores (2-5 nodes). Jobs larger than this may have a longer queue time and are better suited to ARCHER, and single node jobs may be more suited to your local facilities.</p>"},{"location":"Clusters/Michael/#maximum-job-resources-on-original-michael","title":"Maximum job resources on original Michael","text":"Cores Max wallclock 864 48hrs <p>On Michael, interactive sessions using qrsh have the same wallclock limit as other jobs.</p> <p>The K-type nodes in Michael are 24 cores, 128GB RAM. The default maximum jobsize is 864 cores, to remain within the 36-node 1:1 nonblocking interconnect zones.</p> <p>Jobs on Michael do not share nodes. This means that if you request less than 24 cores, your job is still taking up an entire node and no other jobs can run on it, but some of the cores are idle. Whenever possible, request a number of cores that is a multiple of 24 for full usage of your nodes.</p> <p>There is a superqueue for use in exceptional circumstances that will allow access to a larger number of cores outside the nonblocking interconnect zones, going across the 3:1 interconnect between blocks. A third of each CU is accessible this way, roughly approximating a 1:1 connection. Access to the superqueue for larger jobs must be applied for: contact the support address below for details.</p> <p>Some normal multi-node jobs will use the superqueue - this is to make it easier for larger jobs to be scheduled, as otherwise they can have very long waits if every CU is half full.  </p>"},{"location":"Clusters/Michael/#2020-michael-expansion","title":"2020 Michael expansion","text":"<p>At the end of March 2020, Michael was expanded to include a new set of nodes. The old Michael nodes are the K-type nodes, while the new ones are the A-type nodes. The node name will look like <code>node-a14a-001</code> or <code>node-k10a-001</code>.</p> <p>The Michael expansion consists of 208 compute nodes each with two 20-core Intel Xeon Gold 6248 2.5GHz processors, 192 gigabytes of 2933MHz DDR4 RAM, 1TB disk, and an Intel OmniPath network. Expansion nodes have two Hyperthreads available.</p> <p>These are arranged in two 32-node CUs (a and b) and four 36-node CUs (c to f). Jobs are restricted to running either within a CU (all nodes connected to the same switch) or across CUs using only the bottom third of nodes attached to each switch. This approximates 1:1 blocking on a cluster that does not have it.</p>"},{"location":"Clusters/Michael/#maximum-job-resources-on-michael-expansion","title":"Maximum job resources on Michael expansion","text":"<p>Please consider that Michael's A-type nodes have 40 physical cores - 2 nodes is 80 cores. Jobs do not share nodes, so although asking for 41 cores is possible, it means you are wasting the other 39 cores on your second node!</p> Cores Max. Duration 2800 48h <p>These are numbers of physical cores: multiply by two for virtual cores with hyperthreads.</p>"},{"location":"Clusters/Michael/#hyperthreading","title":"Hyperthreading","text":"<p>The A-type nodes have hyperthreading enabled and you can choose on a per-job basis whether you want to use it.</p> <p>Hyperthreading lets you use two virtual cores instead of one physical core - some programs can take advantage of this.</p> <p>If you do not ask for hyperthreading, your job only uses one thread per core as normal.</p> <p>The <code>-l threads=</code> request is not a true/false setting, instead you are telling the scheduler  you want one slot to block one virtual cpu instead of the normal situation where it blocks two.  If you have a script with a threads request and want to override it on the command line or set  it back to normal, the usual case is <code>-l threads=2</code>. (Setting threads to 0 does not disable hyperthreading!)</p> <pre><code># request hyperthreading in this job\n#$ -l threads=1\n\n# request the number of virtual cores\n#$ -pe mpi 160\n\n# request 2G RAM per virtual core\n#$ -l mem=2G\n\n# set number of OpenMP threads being used per MPI process\nexport OMP_NUM_THREADS=2\n</code></pre> <p>This job would be using 80 physical cores, using 80 MPI processes each of which would create two threads (on Hyperthreads).</p> <p>Note that memory requests are now per virtual core with hyperthreading enabled.  If you asked for <code>#$ -l mem=4G</code>on a node with 80 virtual cores and 192G RAM then  you are requiring 320G RAM in total which will not fit on that node and so you  would be given a sparse process layout across more nodes to meet this requirement.</p> <pre><code># request hyperthreading in this job\n#$ -l threads=1\n\n# request the number of virtual cores\n#$ -pe mpi 160\n\n# request 2G RAM per virtual core\n#$ -l mem=2G\n\n# set number of OpenMP threads being used per MPI process\n# (a whole node's worth)\nexport OMP_NUM_THREADS=80\n</code></pre> <p>This job would still be using 80 physical cores, but would use one MPI process per node which would create 80 threads on the node (on Hyperthreads).</p>"},{"location":"Clusters/Michael/#choosing-node-types","title":"Choosing node types","text":"<p>Given the difference in core count on the original and expansion Michael nodes, we strongly suggest you always specify which type of node you intend your job to run on, to avoid unintentionally wasting cores if your total number does not cleanly fit on that node size.</p> <p>The old nodes are K-type while the new nodes with hyperthreading are A-type. Jobs never run across a mix of node types - it will be all K nodes or all A nodes.</p> <p>To specify node type in your jobscript, add either:</p> <pre><code># run on original 24-core nodes\n#$ -ac allow=K\n</code></pre> <p>or</p> <pre><code># run on expansion 40-core hyperthread-enabled nodes\n#$ -ac allow=A\n</code></pre>"},{"location":"Clusters/Michael/#queue-names","title":"Queue names","text":"<p>On Michael, users do not submit directly to queues - the scheduler assigns your job to one based on the resources it requested. The queues have somewhat unorthodox names as they are only used internally, and do not directly map to particular job types.</p>"},{"location":"Clusters/Michael/#preventing-a-job-from-running-cross-cu","title":"Preventing a job from running cross-CU","text":"<p>If your job must run within a single CU, you can request the parallel environment as <code>-pe wss</code> instead of <code>-pe mpi</code> (<code>wss</code> standing for 'wants single switch'). This will increase your queue times. It is suggested you only do this for benchmarking or if performance is being greatly affected by running in the superqueue.</p>"},{"location":"Clusters/Michael/#disk-quotas","title":"Disk quotas","text":"<p>You have per-user quotas for home and Scratch.</p> <ul> <li>home: 100G quota, backed up, no increases available</li> <li> <p>Scratch: 250G quota by default, not backed up, increases possible</p> </li> <li> <p><code>lquota</code> shows you your quota and total usage (twice).</p> </li> <li><code>request_quota</code> is how you request a quota increase.</li> </ul> <p>If you go over quota, you will no longer be able to create new files and your jobs will fail as they cannot write.</p> <p>Quota increases may be granted without further approval, depending on size and how full the filesystem is. Otherwise they may need to go to the Michael User Group for approval.</p>"},{"location":"Clusters/Michael/#budgets-and-allocations","title":"Budgets and allocations","text":"<p>We have enabled Gold for allocation management. Jobs that are run under a project budget have higher priority than free non-budgeted jobs. All jobs need to specify what project they belong to, whether they are paid or free.</p> <p>To see the name of your project(s) and how much allocation that budget has, run the command <code>budgets</code>.</p> <pre><code>$ budgets  \nProject\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Machines\u00a0Balance\n--------\u00a0\u00a0\u00a0\u00a0\u00a0--------\u00a0--------\nFaraday_Test\u00a0ANY\u00a0\u00a0\u00a0\u00a0\u00a0\u00a022781.89\n</code></pre>"},{"location":"Clusters/Michael/#submitting-a-job-under-a-project","title":"Submitting a job under a project","text":"<p>To submit a paid job that will take Gold from a particular project budget, add this to your jobscript:</p> <pre><code>#$\u00a0-P\u00a0Gold\n#$\u00a0-A\u00a0MyProject\n</code></pre> <p>To submit a free job that will not use up any Gold, use this instead:</p> <pre><code>#$\u00a0-P\u00a0Free\n#$\u00a0-A\u00a0MyProject\n</code></pre> <p>You can also submit testing jobs that will not use up any Gold, and will have higher priority than normal free jobs, but are limited to 2 nodes (48 cores) and 1 hour of walltime:</p> <pre><code>#$\u00a0-P\u00a0Test\n#$\u00a0-A\u00a0MyProject\n</code></pre>"},{"location":"Clusters/Michael/#troubleshooting-unable-to-verify-membership-in-policyjsv-project","title":"Troubleshooting: Unable to verify membership in policyjsv project","text":"<pre><code>Unable\u00a0to\u00a0run\u00a0job:\u00a0Rejected\u00a0by\u00a0policyjsv\nUnable\u00a0to\u00a0verify\u00a0membership\u00a0of\u00a0`&lt;username&gt;`\u00a0in\u00a0the\u00a0policyjsv\u00a0project\n</code></pre> <p>You asked for a Free job but didn't specify <code>#$ -A MyProject</code> in your jobscript.</p>"},{"location":"Clusters/Michael/#troubleshooting-unable-to-verify-membership-in-project-uninitialized-value","title":"Troubleshooting: Unable to verify membership in project / Uninitialized value","text":"<pre><code>Unable to run job: Rejected by policyjsv  \nReason:Unable to verify sufficient material worth to submit this job:  \nUnable to verify membership of mmmxxxx in the UCL_Example project\n</code></pre> <p>This error from <code>qsub</code> can mean that you aren't in the project you are trying to submit to, but also happens when the Gold daemon is not running.           </p> <pre><code>Use of uninitialized value in print at /opt/gold/bin/mybalance line 60, &lt;GBALANCE&gt; line 1.\nFailed sending message: (Unable to connect to socket (Connection refused)).\n</code></pre> <p>If you also get this error from the <code>budgets</code> command, then the Gold daemon is definitely not running and you should contact rc-support.</p>"},{"location":"Clusters/Michael/#gold-charging","title":"Gold charging","text":"<p>When you submit a job, it will reserve the total number of core hours that the job script is asking for. When the job ends, the Gold will move from 'reserved' into charged. If the job doesn't run for the full time it asked for, the unused reserved portion will be refunded after the job ends. You cannot submit a job that you do not have the budget to run.</p>"},{"location":"Clusters/Michael/#gold-costs-of-a-type-nodes","title":"Gold costs of A-type nodes","text":"<p>The A-type nodes have twice the peak theoretical performance of the K-type nodes. A 24-core job lasting an hour costs 24 Gold on the K-type nodes. A 40-physical-core job lasting one hour costs 80 Gold on the A-type nodes. An 80-virtual-core job on the A-type nodes also costs 80 Gold.</p>"},{"location":"Clusters/Michael/#troubleshooting-unable-to-verify-sufficient-material-worth","title":"Troubleshooting: Unable to verify sufficient material worth","text":"<pre><code>Unable\u00a0to\u00a0run\u00a0job:\u00a0Rejected\u00a0by\u00a0policyjsv\nReason:Unable\u00a0to\u00a0verify\u00a0sufficient\u00a0material\u00a0worth\u00a0to\u00a0submit\u00a0this\u00a0job:\nInsufficient\u00a0balance\u00a0to\u00a0reserve\u00a0job\n</code></pre> <p>This means you don't have enough Gold to cover the cores*wallclock time cost of the job you are trying to submit. You need to wait for queued jobs to finish and return unused Gold to your project, or submit a smaller/shorter job. Note that array jobs have to cover the whole cost of all the tasks at submit time.</p>"},{"location":"Clusters/Michael/#job-deletion","title":"Job deletion","text":"<p>If you <code>qdel</code> a submitted Gold job, the reserved Gold will be made available again. This is done by a cron job that runs every 15 minutes, so you may not see it back instantly.</p>"},{"location":"Clusters/Michael/#reporting-gold-usage","title":"Reporting Gold usage","text":"<p>There are a few commands that everyone can run that report Gold usage for their entire project, broken down by user. See  Reporting from Gold.</p>"},{"location":"Clusters/Michael/#support","title":"Support","text":"<p>Email rc-support@ucl.ac.uk with any support queries. It will be helpful to include Michael in the subject along with some descriptive text about the type of problem, and you should mention your username in the body.</p>"},{"location":"Clusters/Michael/#acknowledging-the-use-of-michael-in-publications","title":"Acknowledging the use of Michael in publications","text":"<p>To acknowledge this facility please include the following test in presentations   and papers: \"used the Michael Supercomputer (FIRG030).\"</p>"},{"location":"Clusters/Myriad/","title":"Myriad","text":"<p>Myriad is designed for high I/O, high throughput jobs that will run within a single node rather than multi-node parallel jobs.</p>"},{"location":"Clusters/Myriad/#accounts","title":"Accounts","text":"<p>Myriad accounts can be applied for via the Research Computing sign up process.</p> <p>As Myriad is our most general-purpose system, everyone who signs up for a Research Computing account is given access to Myriad.</p>"},{"location":"Clusters/Myriad/#logging-in","title":"Logging in","text":"<p>You will use your UCL username and password to ssh in to Myriad.</p> <pre><code>ssh\u00a0uccaxxx@myriad.rc.ucl.ac.uk\n</code></pre> <p>If using PuTTY, put <code>myriad.rc.ucl.ac.uk</code> as the hostname and your seven-character username (with no @ after) as the username when logging in, eg. <code>uccaxxx</code>. When entering your password in PuTTY no characters or bulletpoints will show on screen - this is normal.</p> <p>If you are outside the UCL firewall you will need to follow the instructions for Logging in from outside the UCL firewall.</p> <p>The login nodes allow you to manage your files, compile code and submit jobs. Very short (&lt; 15 mins) and non-resource-intensive software tests can be run on the login nodes, but anything more should be submitted as a job.</p>"},{"location":"Clusters/Myriad/#logging-in-to-a-specific-node","title":"Logging in to a specific node","text":"<p>You can access a specific Myriad login node with: </p> <pre><code>ssh\u00a0uccaxxx@login12.myriad.rc.ucl.ac.uk\nssh\u00a0uccaxxx@login13.myriad.rc.ucl.ac.uk\n</code></pre> <p>The main address will redirect you on to either one of them.</p>"},{"location":"Clusters/Myriad/#copying-data-onto-myriad","title":"Copying data onto Myriad","text":"<p>You will need to use an SCP or SFTP client to copy data onto Myriad. Please refer to the page on How do I transfer data onto the system?</p>"},{"location":"Clusters/Myriad/#quotas","title":"Quotas","text":"<p>The default quotas on Myriad are 150GB for home and 1TB for Scratch.</p> <p>These are hard quotas: once you reach them, you will no longer be able to write more data. Keep an eye on them, as this will cause jobs to fail if they cannot create their .o or .e files at the start, or their output files partway through.</p> <p>You can check both quotas on Myriad by running: </p> <pre><code>lquota\n</code></pre> <p>which will give you output similar to this:</p> <pre><code>     Storage        Used        Quota   % Used   Path\n        home  721.68 MiB   150.00 GiB       0%   /home/uccaxxx\n     scratch   52.09 MiB     1.00 TiB       0%   /scratch/scratch/uccaxxx\n</code></pre> <p>You can apply for quota increases using the form at Additional Resource Requests.</p> <p>Here are some tips for managing your quota and finding where space is being used.</p>"},{"location":"Clusters/Myriad/#job-sizes","title":"Job sizes","text":"Cores Max wallclock 1 72hrs 2 to 36 48hrs <p>Interactive jobs run with <code>qrsh</code> have the same maximum wallclock time as other jobs.</p>"},{"location":"Clusters/Myriad/#node-types","title":"Node types","text":"<p>Myriad contains three main node types: standard compute nodes, high memory nodes and GPU nodes. As new nodes as added over time with slightly newer processor variants, new letters are added.</p> Type Cores per node RAM per node tmpfs Nodes H,D 36 192GB 1500G 342 I,B 36 1.5TB 1500G 17 J 36 + 2 P100 GPUs 192GB 1500G 2 E,F 36 + 2 V100 GPUs 192GB 1500G 19 L 36 + 4 A100 GPUs 192GB 1500G 6 <p>You can tell the type of a node by its name: type H nodes are named <code>node-h00a-001</code> etc.</p> <p>Here are the processors each node type has:</p> <ul> <li>F, H, I, J: Intel(R) Xeon(R) Gold 6140 CPU @ 2.30GHz</li> <li>B, D, E, L: Intel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz</li> </ul> <p>(If you ever need to check this, you can include <code>cat /proc/cpuinfo</code> in your jobscript so you get it in your job's .o file for the exact node your job ran on. You will get an entry for every core).</p>"},{"location":"Clusters/Myriad/#gpus","title":"GPUs","text":"<p>Myriad has four types of GPU nodes: E, F, J and L. </p> <ul> <li>L-type nodes each have four NVIDIA 40G A100s. (Compute Capability 80)</li> <li>F-type and E-type nodes each have two NVIDIA Tesla V100s. The CPUs are slightly different on the different letters, see above. (Compute Capability 70)</li> <li>J-type nodes each have two NVIDIA Tesla P100s. (Compute Capability 60)</li> </ul> <p>You can include <code>nvidia-smi</code> in your jobscript to get information about the GPU your job ran on.</p>"},{"location":"Clusters/Myriad/#compute-capability","title":"Compute Capability","text":"<p>Compute Capability is how NVIDIA categorises its generations of GPU architectures.  When code is compiled, it targets one or multiple of these and so it may only be able  to run on GPUs of a specific Compute Capability.</p> <p>If you get an error like this:</p> <pre><code>CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid\n</code></pre> <p>then the software you are running does not support the Compute Capability of the GPU you tried to run it on, and you probably need a newer version.</p>"},{"location":"Clusters/Myriad/#requesting-multiple-and-specific-types-of-gpu","title":"Requesting multiple and specific types of GPU","text":"<p>You can request a number of GPUs by adding them as a resource request to your jobscript: </p> <pre><code># For 1 GPU\n#$\u00a0-l\u00a0gpu=1\n\n# For 2 GPUs\n#$ -l gpu=2\n\n# For 4 GPUs\n#$ -l gpu=4\n</code></pre> <p>If you ask for one or two GPUs your job can run on any type of GPU since it can fit on any of the nodetypes. If you ask for four, it can only be a node that has four.  If you need to specify one node type over the others because you need a particular  Compute Capability, add a request for that type of node to your jobscript:</p> <pre><code># request a V100 node only\n#$ -ac allow=EF\n\n# request an A100 node only\n#$ -ac allow=L\n</code></pre> <p>The GPU nodes page has some sample code for running GPU jobs if you need a test example.</p>"},{"location":"Clusters/Myriad/#tensorflow","title":"Tensorflow","text":"<p>Tensorflow is installed: type <code>module avail tensorflow</code> to see the available versions.</p> <p>Modules to load for the non-MKL GPU version: </p> <pre><code>module\u00a0load\u00a0python3/3.7\nmodule\u00a0load\u00a0cuda/10.0.130/gnu-4.9.2\nmodule\u00a0load\u00a0cudnn/7.4.2.24/cuda-10.0\nmodule\u00a0load\u00a0tensorflow/2.0.0/gpu-py37\n</code></pre> <p>Modules to load the most recent version we have installed with GPU support (2.11.0):</p> <pre><code>module unload compilers mpi gcc-libs\nmodule load gcc-libs/10.2.0\nmodule load python/3.9.6-gnu-10.2.0\nmodule load cuda/11.2.0/gnu-10.2.0\nmodule load cudnn/8.1.0.77/cuda-11.2\nmodule load tensorflow/2.11.0/gpu\n</code></pre>"},{"location":"Clusters/Myriad/#pytorch","title":"PyTorch","text":"<p>PyTorch is installed: type <code>module avail pytorch</code> to see the versions available.</p> <p>Modules to load the most recent release we have installed (May 2022) are:</p> <pre><code>module unload compilers mpi gcc-libs\nmodule load gcc-libs/10.2.0\nmodule load python3/3.9-gnu-10.2.0\nmodule load cuda/11.3.1/gnu-10.2.0\nmodule load cudnn/8.2.1.32/cuda-11.3\nmodule load pytorch/1.11.0/gpu\n</code></pre> <p>If you want the CPU only version then use:</p> <pre><code>module unload compilers mpi gcc-libs\nmodule load gcc-libs/10.2.0\nmodule load python3/3.9-gnu-10.2.0\nmodule load pytorch/1.11.0/cpu\n</code></pre>"},{"location":"Clusters/Young/","title":"MMM Hub Young","text":"<p>Young is the compute cluster for the UK National Tier 2 High Performance Computing Hub in Materials and Molecular Modelling.</p> <p>Young entered pilot phase on 3 Aug 2020, and entered full service on 1 Oct 2020.</p>"},{"location":"Clusters/Young/#training-resources","title":"Training resources","text":"<p>The MMM Hub's Events and Training page contains useful information for new users and about specific software.</p> <p>In particular the \"Software Training\" section:</p> <ul> <li>A basic introduction video to Young, including     details of hardware, how to submit jobs and an overview of types of parallelism</li> <li>A quick 4 minute overview of how to choose memory</li> <li>Longer videos on memory allocation for new users<ul> <li>A hands-on memory allocation example </li> </ul> </li> </ul>"},{"location":"Clusters/Young/#mmm-hub-hpe-nvidia-gpu-training-day","title":"MMM Hub: HPE / NVIDIA GPU Training Day","text":"<p>The MMM Hub: HPE / NVIDIA GPU Training Day took place on 31 March 2022 and recordings  of the talks are available at the link below. The day was split into two parts,  \"Technology and Partner sessions (UCL, HPE &amp; NVIDIA)\" which gave an overview of the  new GPU nodes being added to Young and tools useful for GPU programming, followed by  \"Materials community codes, experiences &amp; lessons learned (invited speakers)\" which  looked at how CASTEP, VASP, CP2K, GSGW and QMCpack had approached adding GPU support.</p> <ul> <li>MMM Hub: HPE / NVIDIA GPU Training Day</li> </ul>"},{"location":"Clusters/Young/#applying-for-an-account","title":"Applying for an account","text":"<p>Young accounts belong to you as an individual and are applied for through your own institution's Point of Contact. You will need to supply an SSH public key, which is the only method used to log in.</p>"},{"location":"Clusters/Young/#creating-an-ssh-key-pair","title":"Creating an ssh key pair","text":"<p>An ssh key consists of a public and a private part, typically named <code>id_rsa</code> and <code>id_rsa.pub</code> by default. The public part is what we need. You must not share your private key with anyone else. You can copy it onto multiple machines belonging to you so you can log in from all of them (or you can have a separate pair for each machine).</p>"},{"location":"Clusters/Young/#creating-an-ssh-key-in-linuxunixmacos","title":"Creating an ssh key in Linux/Unix/macOS","text":"<pre><code>ssh-keygen\u00a0-t\u00a0rsa\n</code></pre> <p>The defaults should give you a reasonable key. If you prefer to use ed25519 instead, and/or longer keys, you can. You can also tell it to create one with a different name, so it doesn't overwrite any existing key.</p> <ul> <li>Do not use DSA as OpenSSH 7.0 has deprecated it     and does not use it by default on client or server.      We no longer accept DSA keys.</li> </ul> <p>You will be asked to add a passphrase for your key. A blank passphrase is not recommended; if you use one please make sure that no one else ever has access to your local computer account. How often you are asked for a passphrase depends on how long your local ssh agent keeps it.</p> <p>You may need to run <code>ssh-add</code> to add the key to your agent so you can use it. If you aren't sure what keys your agent can see, running <code>ssh-add -L</code> will show all the public parts of the keys it is aware of.</p>"},{"location":"Clusters/Young/#creating-an-ssh-key-in-windows","title":"Creating an ssh key in Windows","text":"<p>Have a look at Key-Based SSH Logins With PuTTY which has step-by-step instructions. You can choose whether to use Pageant or not to manage your key. You can again pick RSA, ED25519, ECDSA etc but do not pick SSH-1 as that is a very old and insecure key type. As above, DSA is no  longer accepted. The key must be at least 2048-bit.</p> <p>If you are using Windows 10, then you probably have OpenSSH installed and could instead run ssh-keygen in a terminal per the Linux instructions and use the ssh command to log in instead of PuTTY. </p>"},{"location":"Clusters/Young/#information-for-points-of-contact","title":"Information for Points of Contact","text":"<p>Points of Contact have some tools they can use to manage users and allocations, documented at MMM Points of Contact.</p>"},{"location":"Clusters/Young/#logging-in","title":"Logging in","text":"<p>You will be assigned a personal username and your SSH key pair will be used to log in. External users will have a username in the form <code>mmmxxxx</code> (where <code>xxxx</code> is a number) and UCL users will use their central username.</p> <p>You ssh directly to:</p> <pre><code>young.rc.ucl.ac.uk\n</code></pre> <p>Young has two login nodes and you will be round-robin assigned to one or the other each time.  You can also ssh directly into a specific login node, useful if you had a <code>tmux</code> or  <code>screen</code>  session running there.</p> <pre><code># replace 'mmmxxxx' with your username\nssh mmmxxxx@login01.young.rc.ucl.ac.uk\n# or\nssh mmmxxxx@login02.young.rc.ucl.ac.uk\n</code></pre>"},{"location":"Clusters/Young/#ssh-timeouts","title":"SSH timeouts","text":"<p>Idle ssh sessions will be disconnected after 7 days.</p>"},{"location":"Clusters/Young/#using-the-system","title":"Using the system","text":"<p>Young is a batch system. The login nodes allow you to manage your files, compile code and submit jobs. Very short (\\&lt;15mins) and non-resource-intensive software tests can be run on the login nodes, but anything more should be submitted as a job.</p>"},{"location":"Clusters/Young/#full-user-guide","title":"Full user guide","text":"<p>Young has the same user environment as RC Support's other clusters, so the User guide is relevant and is a good starting point for further information about how the environment works. Any variations that Young has should be listed on this page.</p>"},{"location":"Clusters/Young/#submitting-a-job","title":"Submitting a job","text":"<p>Create a jobscript for non-interactive use and  submit your jobscript using qsub.  Jobscripts must begin <code>#!/bin/bash -l</code> in order to run as a login shell  and get your login environment and modules.</p> <p>A job on Young must also specify what type of job it is (Gold, Free, Test) and the project it is being submitted for.  (See Budgets and allocations below.)</p>"},{"location":"Clusters/Young/#memory-requests","title":"Memory requests","text":"<p>Note: the memory you request is always per core, not the total amount. If you ask for 192GB RAM and 40 cores, that may run on 40 nodes using only one core per node. This allows you to have sparse process placement when you do actually need that much RAM per process.</p> <p>Young also has high memory nodes, where a job like this may run.</p> <p>If you want to avoid sparse process placement and your job taking up more nodes than you were expecting, the maximum memory request you can make when using all the cores in a standard node is 4.6G.</p>"},{"location":"Clusters/Young/#monitoring-a-job","title":"Monitoring a job","text":"<p>In addition to qstat, <code>nodesforjob $JOB_ID</code> can be useful to see what proportion of cpu/memory/swap is being used on the nodes a certain job is running on.</p> <p><code>qexplain $JOB_ID</code> will show you the full error for a job that is in <code>Eqw</code> status.</p>"},{"location":"Clusters/Young/#useful-utilities","title":"Useful utilities","text":"<p>As well as <code>nodesforjob</code>, there are the following utilities which can help you find information about your jobs after they have run.</p> <ul> <li><code>jobhist</code> - shows your job history for the last 24hrs by default,     including start and end times and the head node it ran on. You can     view a longer history by specifying <code>--hours=100</code> for example.</li> <li><code>scriptfor $JOB_ID</code> - show the script that was submitted for the     given job.</li> </ul> <p>These utilities live in GitHub at https://github.com/UCL-ARC/go-clustertools and https://github.com/UCL-ARC/rcps-cluster-scripts</p>"},{"location":"Clusters/Young/#known-problems","title":"Known problems","text":""},{"location":"Clusters/Young/#slowness-in-vim","title":"Slowness in vim","text":"<p>Use of vim has frequent lags which can make editing (or viewing) files in it annoying. This is caused by how often vim autosaves status (every few seconds by default) which causes a lot of metadata accesses. It might include saving things like the current cursor position into .viminfo). When Young's filesystem is being particularly slow, this can make use of vim impossible.</p> <p>You can turn off autosaves entirely:</p> <pre><code>:set noswapfile\n</code></pre> <p>or you can set the autosave frequency in milliseconds:</p> <pre><code>:set updatetime=\n</code></pre> <p>You can save these in your <code>.vimrc</code>.</p>"},{"location":"Clusters/Young/#software","title":"Software","text":"<p>Young mounts the RC Systems software stack.</p> <p>Have a look at Software Guides for specific information on running some applications, including example scripts. The list there is not exhaustive. </p> <p>Access to software is managed through the use of modules.</p> <ul> <li><code>module avail</code> shows all modules available.</li> <li><code>module list</code> shows modules currently loaded.</li> </ul> <p>Access to licensed software may vary based on your host institution and project.</p>"},{"location":"Clusters/Young/#loading-and-unloading-modules","title":"Loading and unloading modules","text":"<p>Young has a newer version of <code>modulecmd</code> which tries to manage module dependencies automatically by loading or unloading prerequisites for you  whenever possible.</p> <p>If you get an error like this:</p> <pre><code>[uccaxxx@login01 ~]$ module unload compilers mpi\nUnloading compilers/intel/2018/update3\n  ERROR: compilers/intel/2018/update3 cannot be unloaded due to a prereq.\n    HINT: Might try \"module unload default-modules/2018\" first.\n\nUnloading mpi/intel/2018/update3/intel\n  ERROR: mpi/intel/2018/update3/intel cannot be unloaded due to a prereq.\n    HINT: Might try \"module unload default-modules/2018\" first.\n</code></pre> <p>You can use the <code>-f</code> option to force the module change. It will carry it out  and warn you about modules it thinks are dependent.</p> <pre><code>[uccaxxx@login01 ~]$ module unload -f compilers mpi\nUnloading compilers/intel/2018/update3\n  WARNING: Dependent default-modules/2018 is loaded\n\nUnloading mpi/intel/2018/update3/intel\n  WARNING: Dependent default-modules/2018 is loaded\n</code></pre>"},{"location":"Clusters/Young/#requesting-software-installs","title":"Requesting software installs","text":"<p>To request software installs, email us at the support address below or open an issue on our GitHub. You can see what software has already been requested in the Github issues and can add a comment if you're also interested in something already requested.</p>"},{"location":"Clusters/Young/#installing-your-own-software","title":"Installing your own software","text":"<p>You may install software in your own space. Please look at Compiling Your Code for tips.</p>"},{"location":"Clusters/Young/#maintaining-a-piece-of-software-for-a-group","title":"Maintaining a piece of software for a group","text":"<p>It is possible for people to be given central areas to install software that they wish to make available to everyone or to a select group - generally because they are the developers or if they wish to use multiple versions or developer versions. The people given install access would then be responsible for managing and maintaining these installs.</p>"},{"location":"Clusters/Young/#licensed-software","title":"Licensed software","text":"<p>Reserved application groups exist for software that requires them. The group name will begin with <code>leg</code> or <code>lg</code>. After we add you to one of these groups, the central group change will happen overnight. You can check your groups with the <code>groups</code> command.</p> <p>Please let us know your username when you ask to be added to a group.</p> <ul> <li>CASTEP: You/your group leader need to have      signed up for a CASTEP license.     Send us an acceptance email, or we can ask them to verify you have a     license. You will then be added to the reserved application group     <code>lgcastep</code>. If you are a member of UKCP you are already covered by a     license and just need to tell us when you request access.</li> <li>CRYSTAL: You/your group leader need to have signed up for an     Academic license. Crystal Solutions will send an email saying an     account has been upgraded to \"Academic UK\" - forward that to us     along with confirmation from the group leader that you should be in     their group. You will be added to the <code>legcryst</code> group.</li> <li>DL_POLY: has individual licenses for specific versions.     Sign up at DL_POLY's website     and send us the acceptance email they give you. We will add you to     the appropriate version's reserved application group, eg <code>lgdlp408</code>.</li> <li>Gaussian: not currently accessible for non-UCL institutions. UCL     having a site license and another institute having a site license     does not allow users from the other institute to run Gaussian on     UCL-owned hardware.</li> <li>VASP: When you request access you need to send us the email     address you are named on a VASP license using. You can also send      name and email of the main VASP license holder along with the license      number if you wish. We will then check in the VASP portal if we can      add you. We will add you to the <code>legvasp5</code> or <code>legvasp6</code> reserved      application groups depending on which versions you are licensed for.      You may also install your own copy in your home, and we provide a simple     build script on Github     (tested with VASP 5.4.4, no patches). You need to download the VASP     source code and then you can run the script following the     instructions at the top.</li> <li>Molpro: Only UCL users are licensed to use our central copy and      can request to be added to the <code>lgmolpro</code> reserved application group.</li> </ul>"},{"location":"Clusters/Young/#suggested-job-sizes","title":"Suggested job sizes","text":"<p>The target job sizes for Young are 2-5 nodes. Jobs larger than this may have a longer queue time and are better suited to ARCHER, and single node jobs may be more suited to your local facilities.</p>"},{"location":"Clusters/Young/#maximum-job-resources","title":"Maximum job resources","text":"Job type Cores GPUs Max wallclock Gold CPU job, any 5120 0 48hrs Free CPU job, any 5120 0 24hrs Free GPU job, any 320 40 48hrs Free GPU fast interactive 64 8 6hrs HBM CPU job, any 2048 0 48hrs <p>CPU jobs or GPU jobs can be run on Young, and there are  different nodes dedicated for each.</p> <p>These are numbers of physical cores: multiply by two for virtual cores  with hyperthreads on the CPU nodes.</p> <p>On Young, interactive sessions using qrsh have the same wallclock limit as other jobs.</p> <p>CPU jobs on Young do not share nodes, whereas GPU jobs do.  This means that if you request less than 40 cores for a CPU job,  your job is still taking up an entire node and no other jobs can run on it, but some of the cores are idle. Whenever possible, request a number of cores that is a multiple of 40 for full usage of your CPU nodes.</p> <p>There is a superqueue for use in exceptional circumstances that will allow access to a larger number of cores outside the nonblocking interconnect zones, going across the interconnect between blocks. A third of each CU is accessible this way, roughly approximating a 1:1 connection. Access to the superqueue for larger jobs must be applied for: contact the support address below for details.</p> <p>Some normal multi-node jobs will use the superqueue - this is to make it easier for larger jobs to be scheduled, as otherwise they can have very long waits if every CU is half full.</p>"},{"location":"Clusters/Young/#preventing-a-job-from-running-cross-cu","title":"Preventing a job from running cross-CU","text":"<p>If your job must run within a single CU, you can request the parallel environment as <code>-pe wss</code> instead of <code>-pe mpi</code> (<code>wss</code> standing for 'wants single switch'). This will increase your queue times. It is suggested you only do this for benchmarking or if performance is being greatly affected by running in the superqueue.</p>"},{"location":"Clusters/Young/#node-types","title":"Node types","text":"<p>Young has five types of node: standard nodes, big memory nodes, really big memory nodes,  GPU nodes and HBM nodes. Note those last three have different processors and number of  CPU cores per node.</p> Type Cores per node RAM per node tmpfs Nodes Memory request necessary GPU C 40 192G (188G usable) None 576 Any None Y 40 1.5T None 3 mpi: mem &gt;=19G, smp: &gt;186G total None Z 36 3.0T None 3 mpi: mem &gt;=42G, smp: &gt;1530G total None X 64 1T 200G 6 Any 8 x Nvidia 40G A100 W 64 503G usable 3.5T 32 Any None <p>These are numbers of physical cores: multiply by two for virtual cores with hyperthreading. </p> <p>The 'memory request necessary' column shows what memory requests a job needs to  make to be eligible for that node type. For MPI jobs it looks at the memory per  slot requested. For SMP jobs they will go on the node that their total memory  request (slots * mem) fits on.</p> <p>Here are the processors each node type has:</p> <ul> <li>C: Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz </li> <li>Y: Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz</li> <li>Z: Intel(R) Xeon(R) Gold 6240M CPU @ 2.60GHz</li> <li>X: dual AMD EPYC 7543 32-Core Processor</li> <li>W: Intel (R) Xeon(R) CPU Max 9462</li> </ul> <p>(If you ever need to check this, you can include <code>cat /proc/cpuinfo</code> in your jobscript so  you get it in your job's .o file for the exact node your job ran on. You will get an entry for every core).</p>"},{"location":"Clusters/Young/#gpu-nodes","title":"GPU nodes","text":"<p>Now available for general use, for Free jobs only. There will be separate GPU Gold budgets in  future.</p> <p>How to use the GPU nodes.</p>"},{"location":"Clusters/Young/#high-bandwidth-memory-nodes","title":"High Bandwidth Memory nodes","text":"<p>The HBM nodes have 64GB of integrated High Bandwidth Memory per socket and two sockets per node.</p> <p>HBM nodes can be set on the system level in two modes. </p> <ul> <li> <p>HBM cache mode: In cache mode, HBM functions as a memory-side cache for contents of DDR memory.    In this mode, HBM is transparent to all software because the HBM cache is managed by hardware    memory controllers. No code changes are required.</p> </li> <li> <p>HBM flat mode: In flat mode, both DDR and the HBM address spaces are visible to software.    Applications may need to be modified or tuned to be aware of the additional memory hierarchy.</p> </li> </ul> <p>At present, we have the nodes set in cache mode. We will be re-evaluating this after the  operating system is upgraded and will have full support for flat mode - at this point we  may have some nodes in each mode to allow you to experiment.</p> <p>There are more details about HBM and the modes at Enabling High Bandwidth Memory for HPC and AI Applications for Next Gen Intel Xeon Processors</p>"},{"location":"Clusters/Young/#requesting-hbm-nodes","title":"Requesting HBM nodes","text":"<p>You need to request these nodes explicitly in your job.</p> <pre><code># Request HBM nodes\n#$ -ac allow=W\n</code></pre>"},{"location":"Clusters/Young/#restricting-to-one-node-type","title":"Restricting to one node type","text":"<p>The scheduler will schedule your job on the relevant nodetype  based on the resources you request, but if you really need to specify  the nodetype yourself, use:</p> <pre><code># Only run on Z-type nodes\n#$ -ac allow=Z\n</code></pre>"},{"location":"Clusters/Young/#hyperthreading","title":"Hyperthreading","text":"<p>Young has hyperthreading enabled and you can choose on a per-job basis  whether you want to use it.</p> <p>Hyperthreading lets you use two virtual cores instead of one physical  core - some programs can take advantage of this.</p> <p>If you do not ask for hyperthreading, your job only uses one thread per core as normal.</p> <p>The <code>-l threads=</code> request is not a true/false setting, instead you are telling the scheduler you want one slot to block one virtual cpu instead of the normal situation where it blocks two. If you have a script with a threads request and want to override it on the command line or set it back to normal, the usual case is <code>-l threads=2</code>. (Setting threads to 0 does not disable hyperthreading!)</p> <pre><code># request hyperthreading in this job\n#$ -l threads=1\n\n# request the number of virtual cores\n#$ -pe mpi 160\n\n# request 2G RAM per virtual core\n#$ -l mem=2G\n\n# set number of OpenMP threads being used per MPI process\nexport OMP_NUM_THREADS=2\n</code></pre> <p>This job would be using 80 physical cores, using 80 MPI processes each of  which would create two threads (on Hyperthreads).</p> <p>Note that memory requests are now per virtual core with hyperthreading enabled.  If you asked for <code>#$ -l mem=4G</code>on a node with 80 virtual cores and 192G RAM then  you are requiring 320G RAM in total which will not fit on that node and so you  would be given a sparse process layout across more nodes to meet this requirement.</p> <pre><code># request hyperthreading in this job\n#$ -l threads=1\n\n# request the number of virtual cores\n#$ -pe mpi 160\n\n# request 2G RAM per virtual core\n#$ -l mem=2G\n\n# set number of OpenMP threads being used per MPI process\n# (a whole node's worth)\nexport OMP_NUM_THREADS=80\n</code></pre> <p>This job would still be using 80 physical cores, but would use one MPI  process per node which would create 80 threads on the node (on Hyperthreads).</p>"},{"location":"Clusters/Young/#diskless-nodes","title":"Diskless nodes","text":"<p>Young standard and big memory CPU nodes are diskless (have no local hard drives) -  there is no <code>$TMPDIR</code> available, so you should not request <code>-l tmpfs=10G</code> in your  jobscripts or your job will be rejected at submit time.</p> <p>If you need temporary space, you should use somewhere in your Scratch.</p> <p>The GPU nodes and HBM nodes do have disks and so <code>tmpfs</code> can be requested there.</p>"},{"location":"Clusters/Young/#disk-quotas","title":"Disk quotas","text":"<p>You have per-user quotas for home and Scratch.</p> <ul> <li>home: 100G quota, backed up, no increases available</li> <li> <p>Scratch: 250G quota by default, not backed up, increases possible</p> </li> <li> <p><code>lquota</code> shows you your quota and total usage.</p> </li> <li><code>request_quota</code> is how you request a Scratch quota increase.</li> </ul> <p>If you go over quota, you will no longer be able to create new files and your jobs will fail as they cannot write.</p> <p>Quota increases may be granted without further approval, depending on size and how full the  filesystem is. Otherwise they may need to go to the MMM Hub User Group for approval.</p>"},{"location":"Clusters/Young/#budgets-and-allocations","title":"Budgets and allocations","text":"<p>We have enabled Gold for allocation management. Jobs that are run under a project budget have higher priority than free non-budgeted jobs. All jobs need to specify what project they belong to, whether they are paid or free.</p> <p>To see the name of your project(s) and how much allocation that budget has, run the command <code>budgets</code>.</p> <pre><code>budgets  \nProject\u00a0\u00a0Machines\u00a0Balance\u00a0  \n--------\u00a0--------\u00a0--------  \nUCL_Test\u00a0ANY\u00a0\u00a0\u00a0\u00a0\u00a0\u00a022781.89\n</code></pre> <p>Pilot users temporarily have access to a project for their institution, eg. Imperial_pilot. These will be deactivated after the pilot and no  longer show up.</p> <p>Info</p> <p>1 Gold unit is 1 hour of using 1 virtual processor core (= 0.5 physical core).</p> <p>Since Young has hyperthreading, a job asking for 40  physical cores and one asking for 80 virtual cores with hyperthreading  on both cost the same amount: 80 Gold.</p>"},{"location":"Clusters/Young/#subprojects-and-pools","title":"Subprojects and pools","text":"<p>You might be in a subproject that either does not have an allocation itself, or has a smaller allocation and is nested under a larger pool that it can  take allocation from once its own is exhausted:</p> <pre><code>Project\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Machines\u00a0Balance\n--------\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--------\u00a0--------\nUCL_physM\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ANY\u00a0\u00a0\u00a0474999.70  \nUCL_physM_Bowler\u00a0ANY\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a00.00\n\nProject\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Machines\u00a0Balance\n--------\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--------\u00a0---------\nMCC_pool\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ANY\u00a0\u00a0\u00a0 240533.60\nMCC_react_cat  \u00a0ANY\u00a0\u00a0\u00a0\u00a0 \u00a09372.00\n</code></pre> <p>In this case, you submit jobs using the subproject (<code>UCL_physM_Bowler</code> or <code>MCC_react_cat</code> here) and once it runs out of budget it will take Gold from  the pool.</p>"},{"location":"Clusters/Young/#submitting-a-job-under-a-project","title":"Submitting a job under a project","text":"<p>To submit a paid job that will take Gold from a particular project budget, add this to your jobscript:</p> <pre><code>#$\u00a0-P\u00a0Gold\n#$\u00a0-A\u00a0MyProject\n</code></pre> <p>To submit a free job that will not use up any Gold, use this instead:</p> <pre><code>#$\u00a0-P\u00a0Free\n#$\u00a0-A\u00a0MyProject\n</code></pre> <p>You can also submit testing jobs that will not use up any Gold, and will have higher priority than normal free jobs, but are limited to 2 nodes (80 cores) and 1 hour of walltime:</p> <pre><code>#$\u00a0-P\u00a0Test\n#$\u00a0-A\u00a0MyProject\n</code></pre>"},{"location":"Clusters/Young/#troubleshooting-unable-to-verify-membership-in-policyjsv-project","title":"Troubleshooting: Unable to verify membership in policyjsv project","text":"<pre><code>Unable\u00a0to\u00a0run\u00a0job:\u00a0Rejected\u00a0by\u00a0policyjsv\nUnable\u00a0to\u00a0verify\u00a0membership\u00a0of\u00a0`&lt;username&gt;`\u00a0in\u00a0the\u00a0policyjsv\u00a0project\n</code></pre> <p>You asked for a Free job but didn't specify <code>#$ -A MyProject</code> in your jobscript.</p>"},{"location":"Clusters/Young/#troubleshooting-unable-to-verify-membership-in-project-uninitialized-value","title":"Troubleshooting: Unable to verify membership in project / Uninitialized value","text":"<pre><code>Unable to run job: Rejected by policyjsv \nReason:Unable to verify sufficient material worth to submit this job: \nUnable to verify membership of mmmxxxx in the UCL_Example project\n</code></pre> <p>This error from <code>qsub</code> can mean that you aren't in the project you are trying to submit to, but also happens when the Gold daemon is not running. </p> <pre><code>Use of uninitialized value in print at /opt/gold/bin/mybalance line 60, &lt;GBALANCE&gt; line 1.\nFailed sending message: (Unable to connect to socket (Connection refused)).\n</code></pre> <p>If you also get this error from the <code>budgets</code> command, then the Gold daemon is definitely not running and you should contact rc-support.</p>"},{"location":"Clusters/Young/#gold-charging","title":"Gold charging","text":"<p>When you submit a job, it will reserve the total number of core hours that the job script is asking for. When the job ends, the Gold will move from 'reserved' into charged. If the job doesn't run for the full time it asked for, the unused reserved portion will be refunded after the job ends. You cannot submit a job that you do not have the budget to run.</p>"},{"location":"Clusters/Young/#troubleshooting-unable-to-verify-sufficient-material-worth","title":"Troubleshooting: Unable to verify sufficient material worth","text":"<pre><code>Unable\u00a0to\u00a0run\u00a0job:\u00a0Rejected\u00a0by\u00a0policyjsv\nReason:Unable\u00a0to\u00a0verify\u00a0sufficient\u00a0material\u00a0worth\u00a0to\u00a0submit\u00a0this\u00a0job:\nInsufficient\u00a0balance\u00a0to\u00a0reserve\u00a0job\n</code></pre> <p>This means you don't have enough Gold to cover the cores \u2a09 wallclock time cost of the job you are trying to submit. You need to wait for queued jobs to finish and return unused Gold to your project, or submit a smaller/shorter job. Note that array jobs have to cover the whole cost of all the tasks at submit time.</p>"},{"location":"Clusters/Young/#job-deletion","title":"Job deletion","text":"<p>If you <code>qdel</code> a submitted Gold job, the reserved Gold will be made available again. This is done by a cron job that runs every 15 minutes, so you may not see it back instantly.</p>"},{"location":"Clusters/Young/#reporting-gold-usage","title":"Reporting Gold usage","text":"<p>There are a few commands that everyone can run that report Gold usage for their entire project, broken down by user. See  Reporting from Gold.</p> <p>Specifically, <code>gstatement</code> can show you a summary of who in your budget used what amount during a given time period:</p> <pre><code># summarise per user usage of the MyProject budget from 00:00 on these dates\ngstatement -p MyProject -s 2023-01-01 -e 2023-04-01 --summarize\n</code></pre>"},{"location":"Clusters/Young/#requests-for-longer-wallclock-time","title":"Requests for longer wallclock time","text":"<p>You can apply for access to a 96-hour queue for Gold jobs only using this form:</p> <ul> <li>Wall clock request form</li> </ul> <p>The request must be sent to your local MMM Hub point of contact (PoC) who will  evaluate the request and approve or reject it, as appropriate.  The list of PoCs may be found on the MMM Hub website.</p> <p>Please complete the form and send it to your PoC.</p> <p>The request must include a clear and compelling justification of why the usual  48-hour wall time is insufficient, how a 96-hour wall time would alleviate  the problem and what it will enable you to achieve scientifically.  Justifications that are solely based on simplifying your workflow and job  management will not be approved - clear reasoning must be provided for why it  is either very difficult or impossible to work within a 48-hour wall time.</p>"},{"location":"Clusters/Young/#support","title":"Support","text":"<p>Email rc-support@ucl.ac.uk with any support queries. It will be helpful to include Young in the subject along with some descriptive text about the type of problem, and you should mention your username in the body.</p>"},{"location":"Clusters/Young/#acknowledging-the-use-of-young-in-publications","title":"Acknowledging the use of Young in publications","text":"<p>All work arising from this facility should be properly acknowledged in presentations and papers with the following text:</p> <p>\"We are grateful to the UK Materials and Molecular Modelling Hub for computational resources, which is partially funded by EPSRC (EP/T022213/1, EP/W032260/1 and EP/P020194/1)\"</p>"},{"location":"Clusters/Young/#mcc","title":"MCC","text":"<p>When publishing work that benefited from resources allocated by the MCC: please include the following acknowledgment:</p> <p>\"Via our membership of the UK's HEC Materials Chemistry Consortium, which is funded by EPSRC (EP/L000202), this work used the UK Materials and Molecular Modelling Hub for computational resources, MMM Hub, which is partially funded by EPSRC (EP/T022213/1, EP/W032260/1 and EP/P020194/1)\"</p>"},{"location":"Clusters/Young/#ukcp","title":"UKCP","text":"<p>When publishing work that benefited from resources allocated by UKCP, please include:</p> <p>\"We are grateful for computational support from the UK Materials and Molecular Modelling Hub, which is partially funded by EPSRC (EP/T022213/1, EP/W032260/1 and EP/P020194/1), for which access was obtained  via the UKCP consortium and funded by EPSRC grant ref EP/P022561/1\"</p>"},{"location":"Installed_Software_Lists/module-packages/","title":"General Software Lists","text":"<p>Our clusters have a wide range of software installed, available by using the modules system.</p> <p>The module files are organised by name, version, variant (where applicable) and, if relevant, the compiler version used to build the software. If no compiler version is given, either no compiler was required, or only the base system compiler (<code>/usr/bin/gcc</code>) and libraries were used.</p> <p>When we install applications, we try to install them on all of our clusters, but sometimes licence restrictions prevent it. If something seems to be missing, it may be because we are not able to provide it. Please contact us for more information if this is hindering your work.</p> <p>The lists below were last updated at 08:05:14 (+0100) on 04 Apr 2025, and are generated from the software installed on the Myriad cluster.</p>"},{"location":"Installed_Software_Lists/module-packages/#bundles","title":"Bundles","text":"<p>Some applications or tools depend on a lot of other modules, or have some awkward requirements. For these, we sometimes make a \"bundle\" module in this section, that loads all the dependencies.</p> <p>For Python and R in particular, we also have <code>recommended</code> bundles that load the module for a recent version of Python or R, along with a collection of packages for it that have been requested by users, and the modules those packages require.</p> <p>The lists of Python and R packages installed for those bundles are on separate pages:</p> <ul> <li>Python packages</li> <li>R packages</li> </ul> <p>We'll sometimes include <code>/new</code> and <code>/old</code> versions of these bundles, if we've recently made a version switch or are intending to make one soon. We send out emails to the user lists about version changes, so if you use these bundles, you should look out for those.</p> Module Description <code>beta-modules</code> This module adds the beta module space to your environment. <code>bioperl/recommended</code> Loads all the modules needed to use BioPerl. <code>blic-modules</code> Adds Cancer Biology supported modules to your environment. <code>brunel-modules</code> Adds Brunel licensed software module space to module avail. <code>cancerit/20190218</code> adds UCL set of cancerit packages to your environment variables <code>cancerit/recommended</code> adds UCL recommended set of cancerit packages to your environment variables <code>castep-modules</code> Adds the CASTEP dev install space on Young to module avail. <code>chemistry-modules</code> Adds Chemistry Department supported modules to your environment. <code>climate-tools/recommended</code> Adds set of default applications to the environment for climate science users. <code>deep_earth</code> Sets up VASP, Gnuplot etc for Earth Sciences <code>default-modules-aristotle</code> Adds default Aristotle modules to your environment. <code>default-modules/2015</code> Adds default modules to your environment. <code>default-modules/2017</code> Adds default modules to your environment. <code>default-modules/2018</code> Adds default modules to your environment. <code>economics-modules</code> Adds Economics Department modules to your environment. <code>farr-modules</code> Adds FARR supported modules to your environment. <code>farr/recommended</code> Adds set of default applications to the environment for FARR users. <code>gmt/new</code> Adds set of default modules to the environment for GMT users. <code>gmt/old</code> Adds set of default modules to the environment for gmt users. <code>gmt/recommended</code> Adds set of default modules to the environment for gmt users. <code>grass/8.0dev</code> adds UCL set of modules for GRASS GIS 8.0 Dev to your environment <code>imperial-modules</code> Adds Imperial College licensed software module space to module avail. <code>naglib/mark27-intel-2019</code> adds the NAG Library Mark 27 and required modules to your environment. <code>octave/recommended</code> Octave is an open source competitor to Matlab. <code>personal-modules</code> Adds personal modules to your environment. <code>physics-modules</code> Adds Pysics Department supported modules to your environment. <code>pypy3/3.5-compat</code> Adds UCL recommended set of Pypy3 python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/pypy-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/pypy-3.list <code>python2/recommended</code> Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-2.list <code>python3/3.4</code> Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list <code>python3/3.5</code> Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list <code>python3/3.6</code> Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list <code>python3/3.7</code> Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list <code>python3/3.8</code> Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list <code>python3/3.9</code> Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list <code>python3/3.9-gnu-10.2.0</code> Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list <code>python3/3.11</code> Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list <code>python3/recommended</code> Adds UCL recommended set of python packages to your environment variables. To see what is included, check https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-shared.list and https://github.com/UCL-RITS/rcps-buildscripts/blob/master/lists/python-3.list <code>r/new</code> adds UCL recommended set of R packages for R 4.2.0 to your environment variables <code>r/old</code> adds UCL recommended set of R packages to your environment for R 3.6.0 <code>r/r-3.5.1_bc-3.7</code> adds UCL recommended set of R packages to your environment variables <code>r/r-3.6.0_bc-3.9</code> adds UCL recommended set of R packages to your environment variables <code>r/r-3.6.3_bc-3.10</code> adds UCL recommended set of R packages to your environment variables <code>r/r-4.0.2</code> adds UCL recommended set of R packages for R 4.0.2 to your environment variables <code>r/r-4.0.2_bc-3.11</code> adds UCL recommended set of R packages for R 4.0.2 to your environment variables <code>r/r-4.1.1_bc-3.13</code> adds UCL recommended set of R packages for R 4.1.1 to your environment variables <code>r/r-4.2.0_bc-3.15</code> adds UCL recommended set of R packages for R 4.2.0 to your environment variables <code>r/r-4.2.2_bc-3.16</code> adds UCL recommended set of R packages for R 4.2.2 to your environment variables <code>r/r-4.2.3</code> adds UCL recommended set of R packages for R 4.2.3 to your environment variables <code>r/r-4.2.3_bc-3.16</code> adds UCL recommended set of R packages for R 4.2.3 to your environment variables <code>r/r-4.3.3_bc-3.18</code> adds UCL recommended set of R packages for R 4.3.3 to your environment variables <code>r/r-4.4.0_bc-3.19</code> adds UCL recommended set of R packages for R 4.3.3 to your environment variables <code>r/r-4.4.2_bc-3.20</code> adds UCL recommended set of R packages for R 4.4.2 to your environment variables <code>r/recommended</code> adds UCL recommended set of R packages for R 4.2.0 to your environment variables <code>rsd-modules</code> Adds Research Software Development supported modules to your environment. <code>thermo-modules</code> Adds modules for Molecular Thermodynamics to your environment. <code>torch-deps</code> Loads the dependencies for Torch and makes a quick-install alias. <code>workaround-modules</code> This module adds the workarounds module space to your environment."},{"location":"Installed_Software_Lists/module-packages/#applications","title":"Applications","text":"Module Description <code>abaqus/2017</code> Adds Abaqus 2017 to your environment. <code>abaqus/2017-intelmpi</code> Adds Abaqus 2017 to your environment. <code>abinit/9.6.2/intel-2018-update3</code> adds ABINIT Version 9.6.2 compiled using Intel 2018 to your environment. <code>abinit/9.10.3/intel-2022</code> adds ABINIT Version 9.10.3 compiled using Intel 2022 to your environment. <code>adf/2014.10</code> Adds ADF 2014.10 to your environment. <code>afni/22.2.05</code> Adds AFNI to your environment. <code>afni/23.0.02</code> Adds AFNI to your environment. <code>afni/20151030</code> Adds AFNI to your environment. <code>afni/20181011</code> Adds AFNI to your environment. <code>amber/14/mpi/intel-2015-update2</code> Adds AMBER 14 to your environment <code>amber/14/openmp/intel-2015-update2</code> Adds AMBER 14 to your environment <code>amber/14/serial/intel-2015-update2</code> Adds AMBER 14 to your environment <code>amber/16/mpi/gnu-4.9.2</code> Adds AMBER 16 to your environment <code>amber/16/mpi/intel-2015-update2</code> Adds AMBER 16 to your environment <code>amber/16/openmp/gnu-4.9.2</code> Adds AMBER 16 to your environment <code>amber/16/openmp/intel-2015-update2</code> Adds AMBER 16 to your environment <code>amber/16/serial/gnu-4.9.2</code> Adds AMBER 16 to your environment <code>amber/16/serial/intel-2015-update2</code> Adds AMBER 16 to your environment <code>amber/20/mpi/gnu-10.2.0</code> Adds AMBER 20 and AmberTools 21 to your environment <code>amber/20/openmp/gnu-10.2.0</code> Adds AMBER 20 and AmberTools 21 to your environment. OpenMP threaded version. <code>amber/20/serial/gnu-10.2.0</code> Adds AMBER 20 and AmberTools 21 to your environment <code>ampliconarchitect/1.3r1</code> Amplicon Architect is a tool for finding amplicons in Illumina reads. <code>ams/2023.101</code> Adds the Amsterdam Modeling Suite Version 2023.101 to your environment. <code>ansys/17.2</code> Adds Ansys CFX/Fluent etc to your environment <code>ansys/18.0</code> Adds Ansys CFX/Fluent etc to your environment <code>ansys/19.1</code> Adds Ansys CFX/Fluent, EM etc to your environment <code>ansys/2019.r3</code> Adds Ansys CFX/Fluent, EM etc to your environment <code>ansys/2021.r2</code> Adds Ansys CFX/Fluent, EM etc to your environment <code>ansys/2023.r1</code> Adds Ansys CFX/Fluent, EM etc to your environment <code>ansys/2024.r1</code> Adds Ansys CFX/Fluent, EM etc to your environment <code>ants/2.1.0</code> Adds ANTs 2.1.0 (Advanced Normalization Tools) to your environment. ANTs is popularly considered a state-of-the-art medical image registration and segmentation toolkit. <code>ants/2.5.1/gnu-10.2.0</code> ANTs (Advanced Normalization Tools) computes high-dimensional mappings to capture the statistics of brain structure and function. <code>approxwf/gnu-4.9.2</code> Adds ApproxWF to your environment. <code>arrayfire/3.5.0/gnu-4.9.2</code> Adds ArrayFire 3.5.0 to your environment. <code>asp/2.6.2</code> Adds NASA Ames Stereo Pipeline (ASP) 6.2.2 to your environment. <code>atlas-adna/0.9_2021-04-16/gnu-10.2.0</code> Adds ATLAS Ancient DNA Analysis package to your environment. <code>atlas-adna/0.9_2021-08-13/gnu-10.2.0</code> Adds ATLAS Ancient DNA Analysis package to your environment. <code>autodock/4.2.6</code> Adds AutoDock and AutoGrid 4.2.6 to your environment. AutoDock is a suite of automated docking tools. It is designed to predict how small molecules, such as substrates or drug candidates, bind to a receptor of known 3D structure. <code>bamtools/2.4.0/gnu-4.9.2</code> Adds BamTools 2.4.0 to your environment. BamTools provides both a programmer's API and an end-user's toolkit for handling BAM files. <code>bcftools/1.2/gnu-4.9.2</code> Adds BCFtools 1.2 to your environment. Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants <code>bcftools/1.3.1/gnu-4.9.2</code> Adds BCFtools 1.3.1 to your environment. Reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants <code>bcftools/1.11/gnu-4.9.2</code> Tools for reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants. <code>bcftools/1.19</code> Tools for reading/writing BCF2/VCF/gVCF files and calling/filtering/summarising SNP and short indel sequence variants. <code>bcl2fastq/1.8.4</code> Adds bcl2fastq 1.8.4 to your environment. <code>bcl2fastq2/2.19.1</code> Adds bcl2fastq2 2.19.1 to your environment. <code>bcl2fastq2/2.20.0-rpm</code> Adds bcl2fastq2 2.20.0.422 from rpm to your environment. <code>beast/2.3.0</code> Adds BEAST 2.3.0 with addons to your PATH. <code>bedtools/2.25.0</code> Adds bedtools 2.25.0 to your environment. The bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. <code>bedtools/2.30.0/gnu-10.2.0</code> Adds bedtools 2.30.0 to your environment. The bedtools utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. <code>bgen/1.1.4</code> Adds BGen 1.1.4 to your environment. <code>blast+/2.2.30/intel-2015-update2</code> This module adds the BLAST+ 2.2.30 package to your environment. <code>blast+/2.13.0/gnu-7.3.0</code> This module adds the BLAST+ 2.13.0 package to your environment. <code>blast/2.2.26</code> Adds Blast 2.2.26 to your environment. <code>blender/2.79</code> Adds Blender Version 2.79 to your environment. <code>boltztrap/1.2.5/intel-2018</code> Adds boltztrap 1.2.5 to your environment. <code>bowtie/1.1.2</code> Adds Bowtie 1.1.2 to your environment. <code>bowtie2/2.2.5</code> Adds Bowtie2 2.2.5 to your environment. <code>bwa/0.6.2/gnu-4.9.2</code> Adds BWA 0.7.12 to your environment. BWA is a software package for mapping DNA sequences against a large reference genome, such as the human genome. <code>bwa/0.7.12/gnu-4.9.2</code> Adds BWA 0.7.12 to your environment. BWA is a software package for mapping DNA sequences against a large reference genome, such as the human genome. <code>caffe/1.0/cpu</code> Adds Caffe 1.0 for CPU to your environment. <code>caffe/1.0/cudnn</code> Adds Caffe 1.0 for CUDA+CudNN to your environment. <code>caffe/1.0/gpu</code> Adds Caffe 1.0 for CUDA to your environment. <code>cancerit/20190218-python-2.7.12/gnu-4.9.2</code> Adds CancerIT program versions as of 20190218 to your environment. The CancerIT Suite is a collection of linked bioinformatics tools. <code>cancerit/gnu-4.9.2</code> Adds the cancer it suite to your environment. <code>castep/17.2/intel-2017</code> Adds castep 17.2 to your environment. CASTEP is a program that uses density functional theory to calculate the properties of materials from first principles. <code>castep/17.21/intel-2017</code> Adds castep 17.21 to your environment. CASTEP is a program that uses density functional theory to calculate the properties of materials from first principles. <code>castep/19.1.1/intel-2019</code> CASTEP is a program that uses density functional theory to calculate the properties of materials from first principles. <code>cctools/5.4.1/gnu-4.9.2</code> Adds cctools 5.4.1 to your environment. <code>cctools/7.0.11/gnu-4.9.2</code> Adds cctools 7.0.11 to your environment. <code>cdo/2.0.6/gnu-10.2.0</code> This module adds the CDO 2.0.6 package to your environment. <code>cellranger/5.0.1</code> Adds Cell Ranger 5.0.1 to your environment. Cell Ranger is a set of analysis pipelines that process Chromium single-cell RNA-seq output to align reads, generate feature-barcode matrices and perform clustering and gene expression analysis. <code>cellranger/6.0.1</code> Adds Cell Ranger 6.0.1 to your environment. Cell Ranger is a set of analysis pipelines that process Chromium single-cell RNA-seq output to align reads, generate feature-barcode matrices and perform clustering and gene expression analysis. <code>cesm/1.0.6/intel-2015-update2</code> Adds CESM 1.0.6 to your environment. <code>cesm/1.2.2/intel-2015-update2</code> Adds CESM 1.2.2 to your environment. <code>cfd-ace/2014.1</code> Adds CFD-ACE+ to your execution path. Only on Kathleen and Myriad. <code>cfd-ace/2018.0</code> Adds CFD-ACE+ to your execution path. Only on Kathleen and Myriad. <code>chemshell/3.7.1/mpi/gulp4.5</code> This is a modulefile for ChemShell 3.7.1, MPI+GULP version. Can be used to run other packages if you load a module for those. <code>chemshell/3.7.1/standalone</code> This is a modulefile for ChemShell 3.7.1, standalone serial version. Can be used to run GULP and other packages if you load a module for those. <code>clustal-omega/1.2.1</code> Adds Clustal Omega 1.2.1 to your environment. <code>clustal-w/2.1</code> Adds Clustal W 2.1 to your environment. <code>cmg/2017.101</code> Adds CMG Reservoir Simulation Software Version 2017.101 to your environment. <code>cmg/2018.101</code> Adds CMG Reservoir Simulation Software Version 2018.101 to your environment. <code>cmg/2019.101</code> Adds CMG Reservoir Simulation Software Version 2019.101 to your environment. <code>collectl/4.0.2</code> [collectl/4.0.2] collectl is a tool for tracking and monitoring various node usage statistics. <code>compucell3d/3.7.4</code> Adds CompuCell3D to your environment <code>comsol/6.0</code> COMSOL is a general-purpose macroscopic physics simulation package. <code>comsol/6.1</code> COMSOL is a general-purpose macroscopic physics simulation package. <code>comsol/6.1-eee</code> COMSOL is a general-purpose macroscopic physics simulation package. Installed for EEE Dept <code>comsol/6.2</code> COMSOL is a general-purpose macroscopic physics simulation package. <code>comsol/52</code> Adds the COMSOL 52 binaries to your environment. COMSOL Multiphysics\u00ae is a general-purpose software platform, based on advanced numerical methods, for modeling and simulating physics-based problems. Module must be loaded once from a login node prior to running jobs. <code>comsol/52a</code> Adds the COMSOL 52a binaries to your environment. COMSOL Multiphysics\u00ae is a general-purpose software platform, based on advanced numerical methods, for modeling and simulating physics-based problems. Module must be loaded once from a login node prior to running jobs. <code>comsol/53a</code> Adds COMSOL Multiphysics Version 53a to your environment. <code>comsol/56</code> Adds COMSOL Multiphysics Version 56 to your environment. <code>connectome-workbench/1.5.0</code> Connectome Workbench is an open-source visualization and discovery tool used to explore data generated by the Human Connectome Project. <code>cosi-corr/oct14</code> Adds COSI-Corr Version Oct14 for use with ENVI 5.5.2/5.5.3 to your environment. <code>covid-19-spatial-sim/0.8.0/intel-2020</code> SpatialSim COVID-19 pandemic modelling tool from Imperial College. <code>covid-19-spatial-sim/0.9.0/gnu-4.9.2</code> SpatialSim COVID-19 pandemic modelling tool from Imperial College. <code>covid-19-spatial-sim/0.13.0/gnu-4.9.2</code> SpatialSim COVID-19 pandemic modelling tool from Imperial College. <code>covid-19-spatial-sim/0.14.0/gnu-4.9.2</code> SpatialSim COVID-19 pandemic modelling tool from Imperial College. <code>covid-19-spatial-sim/0.14.0/intel-2020</code> SpatialSim COVID-19 pandemic modelling tool from Imperial College. <code>covid-19-spatial-sim/0.15.0/gnu-4.9.2</code> SpatialSim COVID-19 pandemic modelling tool from Imperial College. <code>covid-19-spatial-sim/0.15.0/intel-2020</code> SpatialSim COVID-19 pandemic modelling tool from Imperial College. <code>cp2k/4.1/ompi/gnu-4.9.2</code> Adds CP2K to your environment. <code>cp2k/5.1/ompi-plumed/gnu-4.9.2</code> Adds CP2K to your environment. <code>cp2k/5.1/ompi/gnu-4.9.2</code> Adds CP2K to your environment. <code>cp2k/6.1/ompi/gnu-4.9.2</code> Adds CP2K to your environment. <code>cp2k/7.1/ompi/gnu-4.9.2</code> Adds CP2K to your environment. <code>cp2k/8.2/ompi/gnu-10.2.0</code> Adds CP2K to your environment. <code>cpmd/4.1/intel-2017</code> Adds  CPMD 4.1 to your environment. <code>crystal14/v1.0.3</code> Adds Crystal14 v1.0.3 to your environment. <code>crystal14/v1.0.4</code> Adds Crystal14 v1.0.4 to your environment. <code>crystal14/v1.0.4_2017</code> Adds Crystal14 v1.0.4 to your environment. <code>crystal17/v1.0.1</code> Adds Crystal17 v1.0.1 to your environment. <code>crystal17/v1.0.2/intel-2017</code> The CRYSTAL program computes the electronic structure of periodic systems within Hartree Fock, density functional or various hybrid approximations. <code>crystal23/1.0.1/intel-2022</code> The CRYSTAL program computes the electronic structure of periodic systems within Hartree Fock, density functional or various hybrid approximations. <code>cuba/4.2/gnu-4.9.2</code> adds Cuba Numerical Integration Package Version 4.2 to your environment. <code>cufflinks/2.2.1</code> Adds Cufflinks 2.2.1 to your environment. <code>curl/7.47.1/gnu-4.9.2</code> Adds curl 7.47.1 to your environment. <code>curl/7.86.0/gnu-4.9.2</code> Adds curl 7.86.0 to your environment. <code>dakota/6.12.0-bindist</code> Dakota is a package for performing parametric analysis with other software packages. <code>datamash/1.4</code> This is a module with no description string. <code>deeptools/3.0.2</code> Adds deeptools to your environment. <code>delly/0.7.8-bindist</code> Delly is an integrated structural variant (SV) prediction method that can discover, genotype and visualize deletions, tandem duplications, inversions and translocations at single-nucleotide resolution in short-read massively parallel sequencing data. <code>dftbplus/17.1/intel-2017</code> DFTB+ is a quantum mechanical simulation software package, based on the Density Functional Tight Binding (DFTB) method. <code>dftbplus/18.2/intel-2018</code> DFTB+ is a software package for carrying out fast quantum mechanical atomistic calculations based on the Density Functional Tight Binding method. <code>dftbplus/19.1/intel-2018</code> DFTB+ is a software package for carrying out fast quantum mechanical atomistic calculations based on the Density Functional Tight Binding method. <code>dftbplus/dev/d07f92e/intel-2017</code> DFTB+ is a quantum mechanical simulation software package, based on the Density Functional Tight Binding (DFTB) method. <code>dl_monte/2.07/intel-2018</code> DL_MONTE is an open-source program for applying Monte Carlo molecular simulation to a wide range of systems. <code>dl_poly/4.07/intel-2015-update2</code> Adds DL_POLY 4.07 to your environment <code>dl_poly/4.08-plumed-2.3.1/intel-2017</code> Adds dl_poly 4.08 to your environment. DL_POLY is a general purpose classical molecular dynamics (MD) simulation software developed at Daresbury Laboratory. This version has been linked against the PLUMED metadynamics library. <code>dl_poly/4.08/intel-2015-update2</code> Adds DL_POLY 4.08 to your environment. <code>dl_poly/4.09/intel-2018</code> Adds DL_POLY 4.09 to your environment. <code>dl_poly/4.10.0/intel-2018</code> Adds DL_POLY 4.10.0 to your environment. DL_POLY is a general purpose classical molecular dynamics (MD) simulation software. <code>dl_poly/5.0.0/intel-2018</code> DL_POLY is a general-purpose classical molecular dynamics (MD) simulation software package. <code>dl_poly/classic/1.9/intel-2015-update2</code> Adds DL_POLY Classic 1.9 to your environment <code>dock/6.9-impi/intel-2018</code> The DOCK suite of programs is designed to find favorable orientations of a ligand in a receptor. This is the Intel MPI build, intended for high-performance parallel runs. <code>dock/6.9-reference/gnu-4.9.2</code> The DOCK suite of programs is designed to find favorable orientations of a ligand in a receptor. This is a reference build intended to be close to the version of the software the developers test with: a serial build using the GNU compilers. <code>dos2unix/7.3</code> Adds dos2unix 7.3 to your environment. Text format converters dos2unix, unix2dos, mac2unix, unix2mac. <code>dssp/3.0.0/gnu-4.9.2</code> Adds dssp 3.0.0 to your environment. DSSP calculates DSSP entries from Protein Databank (PDB) entries. <code>dymola/2020.1-1</code> Dymola is a commercial modeling and simulation environment based on the open Modelica modeling language. <code>ea-utils/822</code> Adds ea-utils to your environment. <code>easylausanne/55c7bf0</code> Adds Easy Lausanne to your environment. <code>ecmwf-ai-models/graphcast/0.1.0</code> Adds the ECMWF AI-models wrapped DeepMind GraphCast to your environment. <code>eigensoft/6.1.1/gnu-4.9.2</code> Adds EIGENSOFT 6.1.1 to your environment. Population genetics methods and EIGENSTRAT stratification correction method. <code>elk/4.0.15/intel-2018-wa</code> Adds Elk 4.0.15 to your environment. Binary is elk. <code>elk/4.3.6/intel-2017-wa</code> Adds Elk 4.3.6 to your environment. Binary is elk. <code>elk/5.2.14/intel-2018</code> Elk is an all-electron full-potential linearised augmented-planewave (FP-LAPW) code. <code>elk/6.8.4/intel-2018</code> An all-electron full-potential linearised augmented-planewave (FP-LAPW) code. <code>energyplus/8.9.0-bindist</code> EnergyPlus\u2122 is a whole building energy simulation program that engineers, architects, and researchers use to model both energy consumption\u2014for heating, cooling, ventilation, lighting and plug and process loads\u2014and water use in buildings. <code>energyplus/9.1.0-bindist</code> EnergyPlus\u2122 is a whole building energy simulation program that engineers, architects, and researchers use to model both energy consumption\u2014for heating, cooling, ventilation, lighting and plug and process loads\u2014and water use in buildings. <code>energyplus/24.1.0-bindist</code> EnergyPlus\u2122 is a whole building energy simulation program that engineers, architects, and researchers use to model both energy consumption\u2014for heating, cooling, ventilation, lighting and plug and process loads\u2014and water use in buildings. <code>envi/5.5.2</code> Adds ENVI 5.5.2 with IDL 8.7.2 to your environment. <code>envi/5.5.3</code> Adds ENVI 5.5.3 with IDL 8.7.3 to your environment. <code>epacts/3.3.0/gnu-4.9.2</code> Adds EPACTS 3.3.0 to your environment. <code>examl/8dcf2cc/gnu-4.9.2</code> Adds ExaML to your environment. <code>exonerate/2.2.0</code> Adds Exonerate to your environment. <code>fasta/36.3.8d/gnu-4.9.2</code> Adds the cancer it suite to your environment. <code>fastqc/0.11.5</code> Adds FastQC 0.11.5 to your environment. A quality control application for high throughput sequence data. <code>fastqc/0.11.8</code> Adds FastQC 0.11.8 to your environment. A quality control application for high throughput sequence data. <code>ffmpeg/4.1/gnu-4.9.2</code> FFmpeg is a framework for encoding, decoding, muxing, demuxing, encoding, transcoding, streaming, filtering, and playing many types of audio and video media. <code>fgbio/0.5.1</code> Adds fgbio to your environment. fgbio is a command line toolkit for working with genomic and particularly next generation sequencing data. <code>fgbio/0.6.1</code> Adds fgbio to your environment. fgbio is a command line toolkit for working with genomic and particularly next generation sequencing data. <code>figtree/1.4.2</code> Adds Figtree 1.4.2. <code>foldx/4</code> Adds FoldX Suite 4.0 to your environment. <code>foldx/5</code> Adds FoldX Suite 5.0 to your environment. <code>freesurfer/5.3.0</code> Adds FreeSurfer 5.3.0 to your environment. FreeSurfer is a set of automated tools for reconstruction of the brain's cortical surface from structural MRI data, and overlay of functional MRI data onto the reconstructed surface. <code>freesurfer/6.0.0</code> Adds FreeSurfer 6.0.0 to your environment. FreeSurfer is a set of automated tools for reconstruction of the brain's cortical surface from structural MRI data, and overlay of functional MRI data onto the reconstructed surface. <code>freesurfer/7.2.0</code> Adds FreeSurfer 7.2.0 to your environment. FreeSurfer is a set of automated tools for reconstruction of the brain's cortical surface from structural MRI data, and overlay of functional MRI data onto the reconstructed surface. <code>freesurfer/7.4.1</code> Adds FreeSurfer 7.4.1 to your environment. FreeSurfer is a set of automated tools for reconstruction of the brain's cortical surface from structural MRI data, and overlay of functional MRI data onto the reconstructed surface. <code>fsl/5.0.9</code> Adds FSL 5.0.9 (FMRIB Software Library) to your environment. FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. <code>fsl/5.0.10</code> Adds FSL 5.0.10 (FMRIB Software Library) to your environment. FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. <code>fsl/6.0.0</code> Adds FSL 6.0.0 (FMRIB Software Library) to your environment. FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. <code>fsl/6.0.0_cuda</code> Adds FSL 6.0.0 CUDA (FMRIB Software Library) to your environment. FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. <code>fsl/6.0.4</code> Adds FSL 6.0.4 CUDA (FMRIB Software Library) to your environment. FSL is a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data. <code>gamess/5Dec2014_R1/intel-2015-update2</code> Adds GAMESS 5Dec2014_R1 to your environment, built for Intel MPI. Uses ~/Scratch/gamess/randomLabel for USERSCR, and TMPDIR or ~/Scratch/gamess/scr.randomLabel for SCR. You can override by exporting GAMESS_USERSCR and GAMESS_SCR as other paths. <code>gatk/3.4.46</code> Adds GATK 3.4.46 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php <code>gatk/3.8.0</code> Adds GATK 3.8.0 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php <code>gatk/4.0.3.0</code> Adds GATK 4.0.3.0 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php <code>gatk/4.0.8.0</code> Adds GATK 4.0.8.0 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php <code>gatk/4.2.1.0</code> Adds GATK 4.2.1.0 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php <code>gatk/4.2.5.0</code> Adds GATK 4.2.1.0 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php <code>gatk/4.4.0.0</code> Adds GATK 4.4.0.0 to your environment. The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. Website: https://www.broadinstitute.org/gatk/index.php <code>gaussian/g09-c01_linda/pgi-2013.9</code> Adds Gaussian 09 Revision C01 and GaussView 5 to your environment. <code>gaussian/g09-d01/pgi-2015.4</code> Adds Gaussian G09-D01 to your environment and also includes Linda and Gaussview 5. <code>gaussian/g09-d01/pgi-2015.7</code> Adds Gaussian G09-D01 to your environment and also includes Linda and Gaussview 5 <code>gaussian/g16-a03/pgi-2016.5</code> Adds Gaussian G16-A03 to your environment and also includes Linda and Gaussview 6. <code>gaussian/g16-c01/pgi-2018.10</code> Adds Gaussian G16-C01 to your environment and also includes Linda and Gaussview 6. <code>gcta/1.93.2beta</code> GCTA: a tool for Genome-wide Complex Trait Analysis. <code>gdal/2.0.0</code> Adds GDAL 2.0.0 to your environment variables. Works with Python 2. <code>gdal/2.1.1</code> adds GDAL 2.1.1 with PROJ.4 4.9.1 to your environment variables. Works with Python 2. <code>gdal/2.1.4</code> adds GDAL 2.1.4 with PROJ.4 6.1.0 to your environment variables. Works with Python 2. <code>gdal/3.0.4/gnu-4.9.2</code> adds GDAL 3.0.4 with PROJ.4 6.1.0 to your environment variables. Works with Python 2. <code>gdal/3.0.4/gnu-9.2.0</code> adds GDAL 3.0.4 with PROJ.4 7.0.0 to your environment variables. <code>gdal/3.1.3/gnu-9.2.0</code> adds GDAL 3.1.3 with PROJ.4 7.0.0 to your environment variables. <code>gdal/3.3.2/gnu-10.2.0</code> adds GDAL 3.3.2 with PROJ.4 8.1.1 to your environment variables. <code>gdal/3.3.3/gnu-10.2.0</code> adds GDAL 3.3.3 with PROJ.4 9.2.0 to your environment variables. <code>gdal/3.10.0/gnu-10.2.0</code> adds GDAL 3.10.0 with PROJ.4 9.2.0 to your environment variables. <code>gephi/0.9.1</code> Adds Gephi Version 0.9.1 to your environment. <code>gftp/2.9.1b</code> gFTP is a file transfer client that supports a wide range of FTP-like protocols. <code>ghostscript/9.16/gnu-4.9.2</code> Adds Ghostscript 9.16 to your environment. <code>ghostscript/9.19/gnu-4.9.2</code> Adds Ghostscript 9.19 to your environment. <code>gmsh/2.12.0-bindist</code> Adds gmsh 2.12.0 to your environment. Gmsh is a free 3D finite element grid generator with a build-in CAD engine and post-processor. <code>gmt/5.1.2</code> adds GMT 5.1.2 to your environment variables <code>gmt/5.3.1</code> adds GMT 5.3.1 to your environment variables <code>gmt/5.4.5</code> adds GMT 5.4.5 to your environment variables <code>gmt/6.0.0/gnu-9.2.0</code> adds GMT 6.0.0 to your environment variables <code>gmt/6.2.0/gnu-10.2.0</code> adds GMT 6.2.0 to your environment variables <code>gmt/6.5.0/gnu-10.2.0</code> adds GMT 6.5.0 to your environment variables <code>gnuplot/5.0.1</code> Adds gnuplot 5.0.1 to your environment. Gnuplot is a portable command-line driven graphing utility. <code>gnuplot/6.0.1</code> Gnuplot is an open-source, portable, command-line driven graphing utility. <code>grace/5.1.25</code> Adds Grace 5.1.25 to your environment. Grace is a 2D plotting tool. <code>graphicsmagick/1.3.21</code> adds GraphicsMagick 1.3.21 to your environment variables <code>graphviz/2.38.0/gnu-4.9.2</code> This module adds the Graphviz 2.38.0 package to your environment. Graphviz is open source graph visualization software. <code>graphviz/2.40.1/gnu-4.9.2</code> This module adds the Graphviz 2.40.1 package to your environment. Graphviz is open source graph visualization software. <code>grass/8.0-dev/gnu-10.2.0</code> Adds GRASS GIS 8.0 Development Version to your environment. <code>groff/1.22.3/gnu-4.9.2</code> Adds GNU groff Version 1.22.3 to your environment. <code>gromacs/5.0.4/intel-2015-update2</code> Adds GROMACS 5.0.4 to your environment, built using MKL <code>gromacs/5.0.4/plumed/intel-2015-update2</code> Adds GROMACS 5.0.4 with Plumed 2.1.2 to your environment. Note: Plumed will always run in double precision even if GROMACS is single-precision, so only use that combination if you need it and are aware of the effects. <code>gromacs/5.1.1/intel-2015-update2</code> Adds GROMACS 5.1.1 to your environment, built using MKL <code>gromacs/5.1.1/plumed/intel-2015-update2</code> Adds GROMACS 5.1.1 with Plumed 2.2 to your environment. Note: Plumed will always run in double precision even if GROMACS is single-precision, so only use that combination if you need it and are aware of the effects. <code>gromacs/5.1.3/plumed/intel-2015-update2</code> GROMACS 5.1.3 molecular dynamics package, built with Intel 2015u2 compilers, PLUMED 2.2.3 patches (including libmatheval), and OpenBLAS 0.2.14. <code>gromacs/2016.3/intel-2017-update1</code> Adds GROMACS 2016.3 to your environment, built using MKL <code>gromacs/2016.3/plumed/intel-2017-update1</code> GROMACS 2016.3 molecular dynamics package, built with Intel 2017u1 compilers, PLUMED 2.3.1 patches (including libmatheval), and OpenBLAS 0.2.14. <code>gromacs/2016.4/plumed/intel-2017</code> GROMACS 2016.4 molecular dynamics package, built with Intel 2017u4 compilers, PLUMED 2.4.1 patches (including libmatheval) with hrex, and OpenBLAS 0.2.14. <code>gromacs/2018.2/intel-2018</code> Adds gromacs 2018 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. <code>gromacs/2018.3/intel-2018</code> Adds gromacs 2018 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. <code>gromacs/2018.3/plumed/intel-2018</code> GROMACS 2018.3 molecular dynamics package, built with Intel 2018u3 compilers, PLUMED 2.4.3 patches (including libmatheval). <code>gromacs/2018/intel-2017</code> Adds gromacs 2018 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. <code>gromacs/2019.3/cuda-10</code> Adds gromacs 2019 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. <code>gromacs/2019.3/intel-2018</code> Adds gromacs 2019 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. <code>gromacs/2019.3/plumed/intel-2018</code> GROMACS 2019.3 molecular dynamics package, built with Intel 2018u3 compilers, PLUMED 2.5.2 patches (including libmatheval). <code>gromacs/2020.1/cuda-10.1</code> Adds gromacs 2020.1 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. <code>gromacs/2020.1/intel-2020</code> Adds gromacs 2020.1 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. <code>gromacs/2020.4/cuda-10.1</code> Adds gromacs 2020.4 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. <code>gromacs/2020.4/intel-2020</code> Adds gromacs 2020.4 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. <code>gromacs/2021.2/gnu-7.3.0</code> Adds gromacs 2021.2 to your environment. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. <code>gromacs/2021.3/plumed/gnu-10.2.0</code> GROMACS 2021.3 molecular dynamics package, built with GNU 10.2.0 compilers, PLUMED 2.7.2 patches (including libmatheval). <code>gromacs/2021.5/cuda-11.3</code> Adds gromacs 2021.5 built with CUDA 11.3 to your environment. ThreadMPI and OpenMPI versions included. GROMACS is a package for performing molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. <code>gulp/4.5/intel-2018</code> Adds GULP 4.5 to your environment. Built with FoX and without plumed. GULP is a materials simulation code. <code>gulp/5.1.1/intel-2018</code> Adds GULP 5.1.1 to your environment. Built with FoX and without plumed. GULP is a materials simulation code. <code>gurobi/7.5.1</code> Adds Gurobi 7.5.1 to your environment. <code>gurobi/8.1.1</code> Adds Gurobi 8.1.1 to your environment. <code>gurobi/9.1.2</code> Adds Gurobi 9.1.2 to your environment. <code>gurobi/11.0.0</code> Adds Gurobi 11.0.0 to your environment. <code>h5utils/1.12.1</code> Adds h5utils 1.12.1 to your environment. h5utils is a set of utilities for visualization and conversion of scientific data in HDF5 format. <code>hammock/1.0.5</code> Loads the dependencies for Hammock 1.0.5 to your environment and makes a quick-install alias, do-hammock-install. Run as java -Xmx2g -jar $HAMMOCKPATH/Hammock.jar mode param1 param2 -d outputpath. Will use Scratch for temporary files. <code>hhsuite/3.0-beta.1/gnu-4.9.2</code> Adds hhsuite 3.0-beta.1 to your environment. <code>hisat2/2.2.1/gnu-4.9.2</code> Adds HISAT2 Version 2.2.1 to your environment. <code>hmmer/3.1b2</code> Adds HMMER 3.1b2  to your environment. <code>hmri/0.4.0/spm12.jan2020</code> Adds the hMRI Toolbox for use with SPM12 to your environment <code>hoomd-blue/2.4.2</code> Adds HOOMD-blue to your environment. <code>hopspack/2.0.2/gnu-4.9.2</code> Adds HOPSPACK 2.0.2 to your environment <code>hopspack/2.0.2/intel-2017</code> Adds HOPSPACK 2.0.2 to your environment <code>icommands/4.1.7</code> [icommands/4.1.7] The iRODS iCommands are the command-line clients to an iRODS system. <code>idl/8.4.1</code> Adds IDL 8.4.1 to your environment. <code>idl/8.7.3</code> Adds IDL 8.7.3 to your environment. <code>idl/8.8.3</code> Adds IDL 8.8.3 to your environment. <code>illustrate/20190807</code> adds Illustrate to your environment variables <code>imagemagick/7.1.1/gnu-10.2.0</code> adds ImageMagick 7.1.1 to your environment variables <code>impute2/2.3.2</code> adds  Impute2 V2.3.2 to your environment. <code>inetutils/1.9.4</code> GNU inetutils is a package of utilities for performing a range of network tasks including FTP and telnet clients. <code>intltool/0.51.0</code> Adds intltool 0.51.0 to your environment. intltool is a set of tools to centralize translation of many different file formats using GNU gettext-compatible PO files. <code>iva/0.11.6</code> Adds IVA 0.11.6 to your environment. <code>iva/1.0.0</code> Adds IVA 1.0.0 to your environment. <code>jags/3.4.0/gnu.4.9.2-atlas</code> Adds JAGS 3.4.0 to your environment. <code>jags/3.4.0/gnu.4.9.2-openblas</code> Adds JAGS 3.4.0 to your environment. <code>jags/4.2.0/gnu.4.9.2-openblas</code> Adds JAGS 4.2.0 to your environment. <code>jags/4.3.0/gnu-10.2.0-openblas</code> Adds JAGS 4.3.0 to your environment. <code>jags/4.3.1/gnu-10.2.0-openblas</code> Adds JAGS 4.3.1 to your environment. <code>jags/4.3.2/gnu-10.2.0-openblas</code> Adds JAGS 4.3.2 to your environment. <code>jagurs/gnu-10.2.0-ompi</code> adds JAGURS V0516 MPI version to your environment variables <code>jagurs/mpi/recommended</code> Adds set of default modules to the environment for JAGURS users. <code>jq/1.5/gnu-4.9.2</code> adds jq for GCC 4.9.2 to your environment. <code>jq/1.6-bindist</code> jq is a lightweight and flexible command-line JSON processor. <code>kallisto/v0.42.5</code> Adds Kallisto v0.42.5 to your environment. <code>kallisto/v0.46.1</code> Adds Kallisto v0.46.1 to your environment. <code>keras/2.2.4</code> Adds Keras to your environment. <code>kmc/2.1.1/gnu-4.9.2</code> Adds KMC 2.1.1 to your environment. KMC is a disk-based program for counting k-mers from FASTQ/FASTA files. <code>knitro/12.0.0/gnu-4.9.2</code> Adds Knitro solver 12.0.0 to your environment. <code>knitro/12.4.0/gnu-4.9.2</code> Adds Knitro solver 12.4.0 to your environment. <code>knitro/13.1.0/gnu-4.9.2</code> Adds Knitro solver 13.1.0 to your environment. <code>lammps/2aug23/basic-fftw/gnu-10.2.0</code> Adds LAMMPS 2 August 2023 to your environment. Binary is lmp_mpi. <code>lammps/2aug23/basic/intel-2022.2</code> Adds LAMMPS 2nd August 2023 to your environment. Binary is lmp_mpi or lmp_default. <code>lammps/2aug23/gpu/gnu-10.2.0</code> Adds LAMMPS 2 August 2023 GPU build to your environment. Binary is lmp_gpu. <code>lammps/2aug23/userintel/intel-2022.2</code> Adds LAMMPS 2 August 2023 with INTEL package to your environment. Binary is lmp_mpi or lmp_default. <code>lammps/3Mar20/plumed-colvars/intel-2018</code> Adds LAMMPS 3Mar20 to your environment. LAMMPS is a GPL molecular dynamics code which shows exceptional scaling on a wide variety of machines. Binary is lmp_mpi or lmp_default. This version was built with packages kspace, manybody, molecule, rigid, lib-linalg, user-colvars and user-plumed. <code>lammps/7Aug19/basic/intel-2018</code> Adds LAMMPS 7Aug19 to your environment. Binary is lmp_default. <code>lammps/7Aug19/gpu/intel-2018</code> Adds LAMMPS 7Aug19 to your environment. Binary is lmp_default. <code>lammps/7Aug19/userintel/intel-2018</code> Adds LAMMPS 7Aug19 to your environment. Binary is lmp_default. <code>lammps/8Dec15/intel-2015-update2</code> Adds LAMMPS 8Dec15 to your environment. Binary is lmp_default. <code>lammps/10Feb15/intel-2015-update2</code> Adds LAMMPS 10Feb15 to your environment. Binary is lmp_default. <code>lammps/13Apr17/intel-2017</code> Adds LAMMPS 13Apr17 to your environment. Binary is lmp_default. <code>lammps/16Mar18/basic/intel-2018</code> Adds LAMMPS 16Mar18 to your environment. Binary is lmp_default. <code>lammps/16Mar18/gpu/intel-2018</code> Adds LAMMPS 16Mar18 to your environment. Binary is lmp_default. <code>lammps/16Mar18/intel-2017</code> Adds LAMMPS 16Mar18 to your environment. Binary is lmp_default. <code>lammps/16Mar18/userintel/intel-2018</code> Adds LAMMPS 16Mar18 to your environment. Binary is lmp_default. <code>lammps/28mar23/basic-fftw/gnu-10.2.0</code> Adds LAMMPS 28 March 2023 to your environment. Binary is lmp_mpi. <code>lammps/29sep21up2/basic-fftw/gnu-10.2.0</code> Adds LAMMPS 29Sep2021 Update 2 to your environment. Binary is lmp_mpi. <code>lammps/29sep21up2/basic/gnu-10.2.0</code> Adds LAMMPS 29Sep2021 Update 2 to your environment. Binary is lmp. <code>lammps/29sep21up2/basic/gnu-10.2.0-aristotle</code> Adds LAMMPS 29Sep2021 Update 2 to your environment. Binary is lmp_aristotle. <code>lammps/29sep21up2/basic/intel-2020</code> Adds LAMMPS 29Sep2021 Update 2 to your environment. Binary is lmp_mpi or lmp_default. <code>lammps/29sep21up2/gpu/gnu-10.2.0</code> Adds LAMMPS 29Sep2021 Update 2 to your environment. Binary is lmp. <code>lammps/29sep21up2/gpu/intel-2020</code> Adds LAMMPS 29Sep2021 Update 2 with gpu package to your environment. Binary is lmp_gpu or lmp_default. <code>lammps/29sep21up2/userintel/intel-2020</code> Adds LAMMPS 29Sep2021 Update 2 with INTEL package to your environment. Binary is lmp_mpi or lmp_default. <code>lynx/2.8.9</code> Adds Lynx Version 2.8.9 to your environment. <code>mathematica/10.1.0</code> Adds Mathematica 10.1.0 to your environment. <code>mathematica/10.2.0</code> Adds Mathematica 10.2.0 to your environment. <code>mathematica/10.4.0</code> Adds Mathematica 10.4.0 to your environment. <code>mathematica/11.0.1</code> Adds Mathematica 11.0.1 to your environment. <code>mathematica/11.2.0</code> Adds Mathematica 11.2 to your environment. <code>mathematica/11.3.0</code> Adds Mathematica 11.3 to your environment. <code>mathematica/12.2.0</code> Adds Mathematica 12.2 to your environment. <code>mathematica/13.1.0</code> Adds Mathematica 13.1 to your environment. <code>matlab/full/r2015a/8.5</code> Adds Matlab R2015a for SPM to your environment. <code>matlab/full/r2015b/8.6</code> Adds Matlab R2015b to your environment. <code>matlab/full/r2016b/9.1</code> Adds Matlab R2016b to your environment. <code>matlab/full/r2017a/9.2</code> Adds Matlab R2017a to your environment. <code>matlab/full/r2018a/9.4</code> Adds Matlab R2018a to your environment. <code>matlab/full/r2018a/9.4-prefdir-fix</code> Adds Matlab R2018a to your environment. <code>matlab/full/r2018b/9.5</code> Adds Matlab R2018b to your environment. <code>matlab/full/r2019b/9.7</code> Adds Matlab R2019b to your environment. <code>matlab/full/r2021a/9.10</code> Adds Matlab R2021a to your environment. <code>matlab/full/r2023a/9.14</code> Adds Matlab R2023a to your environment. <code>mcl/14-137</code> Adds MCL 14-137 your environment. <code>meep/1.3-ompi/gnu-4.9.2</code> Adds meep 1.3-ompi to your environment. <code>meep/1.3/gnu-4.9.2</code> Adds meep 1.3 to your environment. <code>meep/1.11.0-ompi/gnu-4.9.2</code> Adds meep 1.11.0-ompi to your environment. MEEP is a package for electromagnetics simulation via the finite-diffe    rence time-domain (FDTD) method. <code>meme/4.10.1_4</code> Adds MEME Suite 4.10.1_4 to your environment. The MEME Suite: Motif-based sequence analysis tools. This install is for the command-line tools and connects to their website for further analysis. Web: http://meme-suite.org <code>mgltools/1.5.6</code> Adds MGLTools 1.5.6 to your environment. Applications for visualization and analysis of molecular structures. Contains AutoDockTools (ADT), Python Molecular Viewer (PMV) and Vision. <code>micress/7.201/gcc-4.9.2</code> Adds MICRESS 7.201 to your environment. MICRESS is a software enabling the calculation of microstructure formation in time and space during phase transformations. <code>mirdeep/2.0.0.7</code> Adds mirdeep 2.0.0.7 to your environment. <code>molden/5.2.2</code> Adds Molden 5.2.2 to your environment. <code>molpro/2012.1.25/gnu-4.9.2</code> Adds Molpro to your environment <code>molpro/2015.1.3</code> Adds Molpro 2015.1.3 binary (no Infiniband support) to your environment. <code>molpro/2015.1.5/intel-2015-update2</code> Adds Molpro 2015.1.5 built from source with MPI to your environment. <code>molpro/2020.1/openmp</code> Adds Molpro 2020.1 mpi binary to your environment. <code>mosek/8.1.0.83</code> The MOSEK Optimization Suite is a software package for solving large optimization problems with many constraints and variables. <code>mosek/9.1.12</code> Adds Mosek 9.1.12 to your environment. <code>mothur/1.41.3-bindist</code> Mothur is an expandable, multi-purpose bioinformatics tool aimed at microbial ecology. <code>mpb/1.5-ompi/gnu-4.9.2</code> Adds mpb 1.5 to your environment. <code>mpb/1.5/gnu-4.9.2</code> Adds mpb 1.5 to your environment. <code>mpb/1.9.0-hdf5-ompi/gnu-4.9.2</code> Adds serial mpb 1.9.0 to your environment. Built with HDF5-ompi for use by parallel MEEP. <code>mrbayes/3.2.5/mpi/intel-2015-update2</code> Adds MrBayes 3.2.5 to your environment <code>mrbayes/3.2.5/serial/intel-2015-update2</code> Adds MrBayes 3.2.5 to your environment <code>mrtrix/0.3.12/nogui</code> Adds MRtrix3 0.3.12 to your environment. MRtrix3 provides a set of tools to perform analysis of diffusion MRI data, based around the concept of spherical deconvolution and probabilistic tractography. Note: mrview and shview cannot be run over a remote X11 connection so are not usable. <code>mrtrix/0.3.16/gnu-4.9.2/nogui</code> MRtrix provides a set of tools to perform various advanced diffusion MRI analyses, including constrained spherical deconvolution (CSD), probabilistic tractography, track-density imaging, and apparent fibre density. <code>mrtrix/3.0.4/gnu-10.2.0/nogui</code> Adds MRtrix 3.0.4 to your environment. <code>mrtrix/3.0rc3/gnu-4.9.2/nogui</code> Adds MRtrix 3.0RC3 to your environment. <code>mstor/2013/gnu-4.9.2</code> MSTor is a program for calculating partition functions, free energies, enthalpies, entropies, and heat capacities of complex molecules including torsional anharmonicity. <code>mumax/3.9.3</code> Adds Mumax 3.9.3 to your environment. <code>mumax/3.10-bindist</code> mumax3 is a GPU-accelerated micromagnetic simulation program. <code>mummer/3.23/gnu-4.9.2</code> Adds MUMmer 3.23 to your environment. MUMmer is a system for rapidly aligning entire genomes, whether in complete or draft form. <code>muscle/3.8.31</code> Adds MUSCLE 3.8.31 to your environment. <code>mutect/1.1.7</code> Adds MuTect 1.1.7 to your environment. MuTect is a GATK-based variant caller specialized for somatic/cancer variants. <code>namd/2.10/intel-2015-update2</code> Adds NAMD 2.10 to your environment <code>namd/2.11/intel-2015-update2</code> Adds NAMD 2.11 to your environment <code>namd/2.12/intel-2015-update2</code> Adds NAMD 2.12 to your environment <code>namd/2.12/intel-2017-update1</code> Adds NAMD 2.12 to your environment <code>namd/2.12/intel-2018-update3</code> Adds NAMD 2.12 to your environment <code>namd/2.13/intel-2018-update3</code> Adds NAMD 2.13 to your environment <code>namd/2.13/plumed/intel-2018-update3</code> Adds NAMD 2.13 to your environment <code>namd/2.14/multicore-gpu</code> NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. This is NAMD's Linux-x86_64-multicore-CUDA binary. <code>namd/2.14/ofi-smp-gpu/intel-2019</code> Adds NAMD 2.14 OFI-SMP-GPU version to your environment. This version should be run using charmrun. <code>namd/2.14/ofi-smp/intel-2019</code> Adds NAMD 2.14 OFI-SMP version to your environment. This version should be run using charmrun. <code>namd/2.14/ofi/intel-2019</code> Adds NAMD 2.14 OFI version to your environment. This version should be run using charmrun. <code>namd/3.0b7/multicore-gpu/gnu-10.2.0</code> NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. This is NAMD's Linux-x86_64-multicore-CUDA binary. <code>namd/3.0b7/ofi-smp-gpu/gnu-10.2.0</code> Adds NAMD 3.0b7 OFI-SMP-GPU version to your environment. This version should be run using charmrun. <code>ncdu/2.2.1-bindist</code> Ncdu is a disk usage analyzer with an ncurses interface. <code>nco/4.5.0</code> Adds nco to your environment. <code>nektar++/4.3.5-impi/intel-2017-update1</code> Adds  Nektar++ Version 4.3.5 to your environment <code>nektar++/4.3.5-ompi/gnu-4.9.2</code> Adds  Nektar++ Version 4.3.5 to your environment <code>ngsutils/0.5.9</code> Adds a set of python scripts for handling various NGS tasks to your environment. <code>nighres/1.1.0b</code> Adds Nighres to your environment. <code>nonmem/7.3.0/gnu-4.9.2</code> Adds NONMEM 7.3.0 using GCC Fortran 4.9.2 to your environment. <code>nonmem/7.3.0/intel-2015-update2</code> Adds NONMEM 7.3.0 using Intel Fortran 2015 to your environment. <code>nonmem/7.5.1/intel-2018-update3</code> Adds NONMEM 7.5.1 using Intel Fortran 2018 to your environment. <code>novocraft/3.04.06</code> Adds novocraft 3.04.06 to your environment. Novocraft is a set of tools for bioinformatics, including Novoalign for short-read mapping. <code>nwchem/6.5-r26243/atlas/intel-2015-update2</code> Adds NWChem 6.5 revision 26243 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and ATLAS. Global .nwchemrc: /shared/ucl/apps/nwchem/6.5-r26243-atlas/intel-2015-update2.nwchemrc <code>nwchem/6.5-r26243/intel-2015-update2</code> Adds NWChem 6.5 revision 26243 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and MKL with ScaLAPACK. Global .nwchemrc: /shared/ucl/apps/nwchem/6.5-r26243/intel-2015-update2/.nwchemrc <code>nwchem/6.6-r27746/intel-2015-update2</code> Adds NWChem 6.6 revision 27746 patched 2016-01-20 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and MKL with ScaLAPACK. Global .nwchemrc: /shared/ucl/apps/nwchem/6.6-r27746/intel-2015-update2/.nwchemrc <code>nwchem/6.6-r27746/intel-2017</code> Adds NWChem 6.6 revision 27746 patched 2016-01-20 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and MKL with ScaLAPACK. Global .nwchemrc: /shared/ucl/apps/nwchem/6.6-r27746/intel-2017/.nwchemrc <code>nwchem/6.8-47-gdf6c956/intel-2017</code> Adds NWChem 6.8 47-gdf6c956 to your PATH, creates symlink to global .nwchemrc. You may need to alter/remove any old ~/.nwchemrc. Built with Python 2.7 interface and MKL with ScaLAPACK. Global .nwchemrc: /shared/ucl/apps/nwchem/6.8-47-gdf6c956/intel-2017/.nwchemrc <code>oasislmf/1.2.4</code> Oasis LMF 1.2.4 <code>oasislmf/1.26.3</code> Oasis LMF 1.26.3 <code>oasislmf/ktools/3.0.3/gnu-4.9.2</code> OasisLMF ktools package built with the GNU compilers <code>oasislmf/ktools/3.4.1/gnu-4.9.2</code> OasisLMF ktools package built with the GNU compilers <code>oasislmf/ktools/3.9.5/gnu-4.9.2</code> OasisLMF ktools package built with the GNU compilers <code>oasislmf/ktools/f92a41f/gnu-4.9.2</code> OasisLMF ktools package built with the GNU compilers <code>octave/4.4.1</code> Octave is an open source competitor to Matlab which is mostly compatible with Matlab. <code>octopus/4.1.2-impi/intel-2015-update2</code> Adds octopus 4.1.2 to your environment. <code>octopus/4.1.2/intel-2015-update2</code> Adds octopus 4.1.2 to your environment. <code>octopus/5.0.1-ompi/gnu-4.9.2</code> Adds octopus 5.0.1 to your environment. <code>octopus/5.0.1/gnu-4.9.2</code> Adds octopus 5.0.1 to your environment. <code>octopus/6.0-ompi/gnu-4.9.2</code> Adds octopus 6.0 to your environment. <code>octopus/6.0/gnu-4.9.2</code> Adds octopus 6.0 to your environment. <code>openbabel/2.4.1/gnu-4.9.2</code> OpenBabel is a library and command-line tool for manipulating and converting between various chemistry file formats. <code>opencv/2.4.13/gnu-4.9.2</code> Adds OpenCV 2.4.13 to your environment. Open Source Computer Vision Library. <code>opencv/3.4.1/gnu-4.9.2</code> Adds OpenCV 3.4.1 to your environment. Open Source Computer Vision Library. <code>openfoam/2.3.1/intel-2015-update2</code> Adds OpenFOAM 2.3.1 to your environment <code>openfoam/2.4.0/intel-2017-update1</code> Adds OpenFOAM 2.4.0 to your environment <code>openfoam/7.20200120/gnu-7.3.0</code> Adds OpenFOAM 7 to your environment <code>openfoam/12.20240902/gnu-7.3.0</code> Adds OpenFOAM 12 to your environment <code>openfoamplus/v1706/gnu-4.9.2</code> Adds OpenFOAMplus v1706 to your environment <code>openfoamplus/v1906/gnu-7.3.0</code> Adds OpenFOAMplus v1906 to your environment <code>openfoamplus/v1906/gnu-7.3.0-64</code> Adds OpenFOAMplus v1906 with 64 bit labels to your environment <code>openfoamplus/v2112/gnu-7.3.0-64</code> Adds OpenFOAMplus v2112 with 64 bit labels to your environment <code>openmm/7.3.1/cuda-10</code> Adds OpenMM to your environment. <code>openmm/7.3.1/gnu-4.9.2</code> Adds OpenMM to your environment. <code>openmx/3.8.3</code> Adds OpenMX 3.8.3 to your environment. <code>optimet/1.0.1/gnu-4.9.2</code> Adds Optimet to your environment. <code>orca/4.2.1-bindist/gnu-4.9.2</code> ORCA is an ab initio, DFT, and semiempirical SCF-MO package. <code>orca/5.0.4-sbindist</code> ORCA is an ab initio, DFT, and semiempirical SCF-MO package. This installation is the official statically-linked binary distribution. <code>p7zip/15.09/gnu-4.9.2</code> Adds p7zip 15.09 to your environment. To expand 7z files: 7za x archive.7z <code>p7zip/16.02/gnu-4.9.2</code> p7zip is a port of the command-line version of the 7-zip file compression tool to UNIX-like systems. <code>pandoc/1.19.2.1</code> Adds pandoc Version 1.19.2.1 to your environment. <code>parallel/20181122</code> GNU parallel is a shell tool for executing jobs in parallel using one or more computers. <code>paraview/5.3.0</code> This module adds the ParaView 5.3.0 binaries to your environment. ParaView is an open-source, multi-platform data analysis and visualization application. <code>paraview/5.10.1</code> This module adds the ParaView 5.10.1 binaries to your environment. ParaView is an open-source, multi-platform data analysis and visualization application. <code>parmed/3.2.0</code> Adds ParmEd to your environment. <code>petsc/3.12.1/gnu-4.9.2</code> Adds Petsc 3.12.1 to your environment <code>phon/1.39/gnu-4.9.2</code> Adds Phon 1.3.9 with addons to your PATH. <code>phon/1.43/gnu-4.9.2</code> Adds Phon 1.43 with addons to your PATH. <code>picard-tools/1.136</code> Adds Picard Tools 1.136 to your environment. If using the java -jar command, you should pass TMP_DIR=$TMPDIR to Picard. <code>picard-tools/2.18.9</code> Adds Picard Tools to your environment. If using the java -jar command, you should pass TMP_DIR=$TMPDIR to Picard. <code>platypus/3e72641</code> Adds Platypus to your environment. <code>plink/1.07</code> Adds Plink 1.07 with addons to your PATH. <code>plink/1.90b3.40</code> Adds PLINK 1.90b3.40 to your environment. A comprehensive update to the PLINK association analysis toolset. <code>plink/2.0alpha-git</code> Adds PLINK 2.0 alpha to your environment. A comprehensive update to the PLINK association analysis toolset. <code>plumed/2.1.2/intel-2015-update2</code> Adds PLUMED 2.1.2 to your environment, built using OpenBLAS <code>plumed/2.2.3/intel-2015-update2</code> Adds PLUMED 2.2.3 to your environment, built using OpenBLAS and libmatheval <code>plumed/2.2/intel-2015-update2</code> Adds PLUMED 2.2 to your environment, built using OpenBLAS <code>plumed/2.3.1/intel-2017-update1</code> Adds PLUMED 2.3.1 to your environment, built using OpenBLAS and libmatheval <code>plumed/2.4.1/gnu-4.9.2</code> Adds PLUMED 2.4.1 to your environment, built using OpenBLAS and libmatheval <code>plumed/2.4.1/intel-2017-update4</code> Adds PLUMED 2.4.1 to your environment, built using OpenBLAS and libmatheval <code>plumed/2.4.3/intel-2018</code> Adds PLUMED 2.4.3 to your environment, built using MKL and libmatheval <code>plumed/2.5.2/intel-2018</code> Adds PLUMED 2.5.2 to your environment, built using MKL and libmatheval <code>plumed/2.6.0/intel-2018</code> Adds PLUMED 2.6.0 to your environment, built using MKL and libmatheval <code>plumed/2.7.2/gnu-10.2.0</code> Adds PLUMED 2.7.2 to your environment, built using GCC, OpenBLAS and libmatheval. PLUMED is a plugin that works with a large number of molecular dynamics codes. <code>plumed/2.7.2/intel-2020</code> Adds PLUMED 2.7.2 to your environment, built using MKL and libmatheval. PLUMED is a plugin that works with a large number of molecular dynamics codes. <code>plumed/2.9.3/gnu-10.2.0</code> Adds PLUMED 2.9.3 to your environment, built using GCC, OpenBLAS and libmatheval. PLUMED is a plugin that works with a large number of molecular dynamics codes. <code>poppler/22.10.0/gnu-9.2.0</code> Adds Poppler 22.10.0 to your environment. <code>postgres+postgis/9.5.3+2.2.2/gnu-4.9.2</code> Adds postgres+postgis 9.5.3+2.2.2 to your environment. PostgreSQL is a relational database, and PostGIS is a geographical information enhancement for PostgreSQL. <code>postgresql/9.5.3/gnu-4.9.2</code> Adds postgresql 9.5.3 to your environment. PostgreSQL is a relational database. <code>primer3/2.3.6</code> This module adds the primer3 package to your environment. <code>probabel/0.4.5/gnu-4.9.2</code> Adds ProbABEL to your environment. <code>proj.4/4.9.1</code> Adds the PROJ.4 Cartographic Projections library to your environment. <code>proj.4/5.2.0</code> Adds the PROJ.4 Cartographic Projections library to your environment. <code>proj.4/6.0.0</code> Adds the PROJ.4 Cartographic Projections library to your environment. <code>proj.4/6.1.0</code> Adds the PROJ.4 Cartographic Projections library to your environment. <code>proj.4/7.0.0/gnu-9.2.0</code> Adds the PROJ.4 Cartographic Projections library to your environment. <code>proj.4/8.1.1/gnu-10.2.0</code> Adds the PROJ.4 Cartographic Projections library to your environment. <code>proj.4/9.2.0/gnu-10.2.0</code> Adds the PROJ.4 Cartographic Projections library to your environment. <code>proovread/2.13.11-8Jan2016-f6a856a</code> Adds proovread 2.13.11-8Jan2016-f6a856a to your environment. f6a856a is the commit for this version. <code>prsice/2.3.3/gnu-9.2.0</code> PRSice (pronounced 'precise') is a software package for calculating, applying, evaluating and plotting the results of polygenic risk scores (PRS). <code>pymol/1.7.7.2</code> Adds PyMol to your environment. <code>pymol/1.8.2.1</code> Adds PyMol to your environment. <code>pyrosetta/release-73</code> Adds PyRosetta to your environment. <code>pytorch/1.2.0/cpu</code> Adds PyTorch 1.2.0 to your environment. <code>pytorch/1.2.0/gpu</code> Adds PyTorch 1.2.0 to your environment. <code>pytorch/1.11.0/cpu</code> Adds PyTorch 1.11.0 to your environment. <code>pytorch/1.11.0/gpu</code> Adds PyTorch 1.11.0 to your environment. <code>pytorch/2.1.0/cpu</code> Adds PyTorch 2.1.0 to your environment. <code>pytorch/2.1.0/gpu</code> Adds PyTorch 2.1.0 to your environment. <code>qctool/2/beta/ba5eaa44a62f</code> This module adds qctool v2 beta to your environment. <code>quantum-espresso/5.2.0-impi/intel-2015-update2</code> Adds quantum-espresso 5.2.0 to your environment. <code>quantum-espresso/6.1-impi/intel2017</code> Adds quantum-espresso 6.1 to your environment. <code>quantum-espresso/6.3-impi/thermo_pw-1.0.9/intel-2018</code> Adds quantum-espresso 6.3 + thermo_pw 1.0.9 to your environment. <code>quantum-espresso/6.4.1-impi/intel-2018</code> Adds quantum-espresso 6.4.1 to your environment. <code>quantum-espresso/6.5-impi/intel-2018</code> Adds quantum-espresso 6.5 to your environment. <code>quantum-espresso/6.5-impi/thermo_pw-1.2.1/intel-2018</code> Adds quantum-espresso 6.5 + thermo_pw 1.2.1 to your environment. <code>quantum-espresso/7.0-impi/intel-2018</code> Adds quantum-espresso 7.0 to your environment. <code>quantum-espresso/7.3.1-cpu/gnu-10.2.0</code> Adds quantum-espresso 7.3.1 to your environment. <code>quantum-espresso/7.3.1-gpu/nvidia-22.9</code> Adds quantum-espresso 7.3.1 to your environment. <code>r/3.2.0-atlas/gnu-4.9.2</code> Adds R 3.2.0 and Bioconductor 3.2 to your environment. <code>r/3.2.2-openblas/gnu-4.9.2</code> Adds R 3.2.2 and Bioconductor 3.2 to your environment. <code>r/3.3.0-openblas/gnu-4.9.2</code> Adds R 3.3.0 and Bioconductor 3.3 to your environment. <code>r/3.3.2-openblas/gnu-4.9.2</code> Adds R 3.3.2 and Bioconductor 3.4 to your environment. <code>r/3.4.0-openblas/gnu-4.9.2</code> Adds R 3.4.0 and Bioconductor 3.5 to your environment. <code>r/3.4.2-openblas/gnu-4.9.2</code> Adds R 3.4.2 and Bioconductor 3.6 to your environment. <code>r/3.5.0-openblas/gnu-4.9.2</code> Adds R 3.5.0 and Bioconductor 3.7 to your environment. <code>r/3.5.1-openblas/gnu-4.9.2</code> Adds R 3.5.1 and Bioconductor 3.7 to your environment. <code>r/3.5.3-openblas/gnu-4.9.2</code> Adds R 3.5.3 and Bioconductor 3.8 to your environment. <code>r/3.6.0-openblas/gnu-4.9.2</code> Adds R 3.6.0 and Bioconductor 3.9 to your environment. <code>r/3.6.3-openblas/gnu-9.2.0</code> Adds R 3.6.3 and Bioconductor 3.10 to your environment. <code>r/4.0.2-openblas/gnu-9.2.0</code> Adds R 4.0.2 and Bioconductor 3.11 to your environment. <code>r/4.1.1-openblas/gnu-10.2.0</code> Adds R 4.1.1 and Bioconductor 3.13 to your environment. <code>r/4.2.0-openblas/gnu-10.2.0</code> Adds R 4.2.0 and Bioconductor 3.15 to your environment. <code>r/4.2.2-openblas/gnu-10.2.0</code> Adds R 4.2.2 and Bioconductor 3.16 to your environment. <code>r/4.2.3-openblas/gnu-10.2.0</code> Adds R 4.2.3 and Bioconductor 3.16 to your environment. <code>r/4.3.3-openblas/gnu-10.2.0</code> Adds R 4.3.3 and Bioconductor 3.18 to your environment. <code>r/4.4.0-openblas/gnu-10.2.0</code> Adds R 4.4.0 and Bioconductor 3.19 to your environment. <code>r/4.4.2-openblas/gnu-10.2.0</code> Adds R 4.4.2 and Bioconductor 3.20 to your environment. <code>randfold/2.0/gnu-4.9.2</code> Adds randfold 2.0 to your environment. <code>rclone/1.51.0</code> RClone is a command-line program intended to download and upload files from and to various storage services and providers. <code>rclone/1.61.0</code> RClone is a command-line program intended to download and upload files from and to various storage services and providers. <code>repast-hpc/2.1/gnu-4.9.2</code> Adds Repast HPC 2.1 compiled with GCC 4.9.2 and OpenMPI to your environment. <code>rfaa/20250304</code> Installation of RoseTTAFold All-Atom (RFAA <code>root/5.34.30/gnu-4.9.2</code> Adds ROOT 5.34.30 to your environment. <code>root/5.34.30/gnu-4.9.2-fftw-3.3.6</code> Adds ROOT 5.34.30 to your environment. <code>root/5.34.36/gnu-4.9.2-fftw-3.3.6</code> Adds ROOT 5.34.36 to your environment. <code>root/5.34.36/gnu-4.9.2-fftw-3.3.6-gsl-2.4</code> Adds ROOT 5.34.36 to your environment. <code>root/6.04.00/gnu-4.9.2</code> Adds ROOT 6.04.00 to your environment. <code>rosetta/2015.31.58019</code> Adds  Rosetta 2015.31.58019 to your environment. <code>rosetta/2015.31.58019-mpi</code> Adds  Rosetta 2015.31.58019 with MPI to your environment. <code>rosetta/2018.48.60516</code> Adds Rosetta 2018.48.60516 serial version to your environment. <code>rosetta/2018.48.60516-mpi</code> Adds Rosetta 2018.48.60516 MPI version to your environment. <code>rsem/1.2.31</code> Adds RSEM 1.2.31 to your environment. <code>rvtests/2.1.0</code> Rvtests: Rare variant test software for next generation sequencing data <code>sac/101.6a</code> Adds SAC 101.6a to your environment. <code>sac/102.0</code> Adds SAC 102.0 to your environment. <code>salmon/1.9.0</code> Adds Salmon 1.9.0 to your environment. <code>sambamba/0.6.7-bindist</code> A tool for extracting information from SAM/BAM files. <code>samblaster/0.1.24/gnu-4.9.2</code> samblaster is a program for marking duplicates in read-id grouped paired-end SAM files. <code>samsrf/5.84/matlab.r2019b</code> Adds the SamSrf Matlab toolbox to your environment <code>samtools/0.1.19</code> This module adds the Samtools 0.1.19 package to your environment. <code>samtools/1.2/gnu-4.9.2</code> Adds SAMtools 1.2 to your environment. Reading/writing/editing/indexing/viewing SAM/BAM/CRAM format. <code>samtools/1.3.1/gnu-4.9.2</code> Adds SAMtools 1.3.1 to your environment. Reading/writing/editing/indexing/viewing SAM/BAM/CRAM format. <code>samtools/1.9/gnu-4.9.2</code> Adds SAMtools 1.9 to your environment. Reading/writing/editing/indexing/viewing SAM/BAM/CRAM format. <code>samtools/1.11/gnu-4.9.2</code> Tools for reading/writing/editing/indexing/viewing SAM/BAM/CRAM formatted data. <code>sas/9.4-M6/64</code> Adds SAS 9.4 (9.04.01M6) 64 bit to your environment <code>sas/9.4-m7/64</code> Adds SAS 9.4 (9.04.01M7) 64 bit to your environment <code>sas/9.4/64</code> Adds SAS 9.4 64 bit to your environment <code>sc/7.16</code> Adds sc 7.16 to your environment. <code>siesta/4.0.1/intel-2017</code> Adds SIESTA 4.0.1 to your environment. <code>siesta/5.0-dev/intel-2018</code> Adds SIESTA 5.0 Development Version to your environment. <code>skewer/0.2.2</code> Adds skewer 0.2.2 to your environment. <code>slim/4.0.1/gnu-4.9.2</code> This module adds SLiM 4.0.1 evolutionary simulation framework to your environment. <code>smalt/0.7.6/gnu-4.9.2</code> Adds SMALT 0.7.6 to your environment. SMALT aligns DNA sequencing reads with a reference genome. Compiled with bambamc support for SAM/BAM input and BAM output. <code>snpAD/0.3.4-bindist</code> snpAD is an ancient DNA aware genotype caller. <code>snptest/2.5.4-beta3</code> Adds SNPtest 2.5.4-beta3 to your environment. <code>sod/3.2.7</code> Adds SOD 3.2.7 to your environment. SOD is a program that automates tedious data selection, downloading, and routine processing tasks in seismology. <code>sod/3.2.10</code> Adds SOD 3.2.10 to your environment. SOD is a program that automates tedious data selection, downloading, and routine processing tasks in seismology. <code>spaceranger/1.2.2</code> Adds Space Ranger 1.2.2 to your environment. Space Ranger is a set of analysis pipelines that process Visium spatial RNA-seq output and brightfield and fluorescence microscope images. <code>spm/8/r6313/matlab.r2015a</code> Adds SPM8 to your environment <code>spm/12/jan2020/matlab.r2019b</code> Adds SPM12 to your environment <code>spm/12/r6470/matlab.r2015a</code> Adds SPM12 to your environment <code>spss/25</code> Adds SPSS 25 to your environment <code>spss/26</code> Adds SPSS 26 to your environment <code>sqlite/3.31.1/gnu-9.2.0</code> Adds SQLite Version 3.31.1 to your environment. <code>sqlite/3.36.0/gnu-10.2.0</code> Adds SQLite Version 3.36.0 to your environment. <code>sqlite/3.41.2</code> SQLite is a C-language library that implements a small, fast, self-contained, high-reliability, full-featured, SQL database engine. <code>sra-tools/3.0.6/gnu-10.2.0</code> adds SRA Tools 3.0.6 for GCC 10.2.0 to your environment. <code>stacks/2.54/gnu-4.9.2</code> Stacks is a software pipeline for building loci from short-read sequences, such as those generated on the Illumina platform. Stacks was developed to work with restriction enzyme-based data, such as RAD-seq, for the purpose of building genetic maps and conducting population genomics and phylogeography. <code>star-ccm+/9.06.011</code> Adds STAR-CCM+ and STAR-View to your environment. <code>star-ccm+/11.04.010-R8</code> Adds STAR-CCM+ and STAR-View to your environment. <code>star-ccm+/12.04.010</code> Adds STAR-CCM+ and STAR-View to your environment. <code>star-ccm+/13.02.011</code> Adds STAR-CCM+ and STAR-View to your environment. <code>star-ccm+/13.06.012</code> Adds STAR-CCM+ and STAR-View to your environment. <code>star-ccm+/14.06.013</code> Adds STAR-CCM+ and STAR-View to your environment. <code>star-cd/4.22.058</code> Adds STAR-CD 4.22.058 to your environment. <code>star-cd/4.26.011</code> Adds STAR-CD 4.26.011 using Intel 2016 compiler suite to your environment. STAR-CD is a code for performing CFD simulations. It is designed for modelling fluid flow, heat transfer, mass transfer and chemical reactions. <code>star-cd/4.26.022</code> Adds STAR-CD 4.26.022 using Intel 2016 compiler suite to your environment. STAR-CD is a code for performing CFD simulations. It is designed for modelling fluid flow, heat transfer, mass transfer and chemical reactions. <code>star-cd/4.28.050</code> Adds STAR-CD 4.28.050 using Intel 2016 compiler suite to your environment. STAR-CD is a code for performing CFD simulations. It is designed for modelling fluid flow, heat transfer, mass transfer and chemical reactions. <code>star/2.5.2a</code> Adds STAR 2.5.2a to your environment. <code>star/2.7.3a</code> Adds STAR 2.7.3a to your environment. <code>star/2.7.10b/sbindist</code> The Spliced Transcripts Alignment to a Reference (STAR) package for RNA sequence alignment. This installation uses the statically-linked binaries by default. <code>stata/14</code> Adds Stata/MP 14 to your environment. <code>stata/15</code> Adds Stata/MP 15 to your environment. <code>stata/16</code> Adds Stata/MP 16 to your environment. <code>stata/18</code> Adds Stata/MP 18.5 to your environment. <code>supermagic/1.2/intel-2017</code> Adds supermagic 1.2 to your environment. Supermagic is a simple MPI sanity test. <code>taup/2.1.2</code> adds TauP 2.1.2 to your environment variables <code>tensorflow/1.4.1/cpu</code> Adds Tensorflow 1.4.1 to your environment. <code>tensorflow/1.4.1/gpu</code> Adds Tensorflow 1.4.1 to your environment. <code>tensorflow/1.4.1/mkl</code> Adds Tensorflow 1.4.1 to your environment. <code>tensorflow/1.8.0/cpu</code> Adds Tensorflow 1.8.0 to your environment. <code>tensorflow/1.8.0/gpu</code> Adds Tensorflow 1.8.0 to your environment. <code>tensorflow/1.8.0/mkl</code> Adds Tensorflow 1.8.0 to your environment. <code>tensorflow/1.12.0/cpu</code> Adds Tensorflow 1.12.0 to your environment. <code>tensorflow/1.12.0/gpu</code> Adds Tensorflow 1.12.0 to your environment. <code>tensorflow/1.12.0/mkl</code> Adds Tensorflow 1.12.0 to your environment. <code>tensorflow/1.13.1/cpu</code> Adds Tensorflow 1.13.1 to your environment. <code>tensorflow/1.13.1/gpu</code> Adds Tensorflow 1.13.1 to your environment. <code>tensorflow/1.13.1/mkl</code> Adds Tensorflow 1.13.1 to your environment. <code>tensorflow/1.14.0/cpu</code> Adds Tensorflow 1.14.0 to your environment. <code>tensorflow/1.14.0/gpu</code> Adds Tensorflow 1.14.0 to your environment. <code>tensorflow/1.14.0/mkl</code> Adds Tensorflow 1.14.0 to your environment. <code>tensorflow/2.0.0/gpu-py37</code> Adds Tensorflow 2.0.0 to your environment. <code>tensorflow/2.0.0/gpu-py37-cudnn75</code> Adds Tensorflow 2.0.0 to your environment. <code>tensorflow/2.0.0/mkl-py37</code> Adds Tensorflow 2.0.0 to your environment. <code>tensorflow/2.8.0/python-3.8.6</code> Adds Tensorflow 2.8.0 to your environment. <code>tensorflow/2.11.0/cpu</code> Adds Tensorflow 2.11.0 to your environment. <code>tensorflow/2.11.0/gpu</code> Adds Tensorflow 2.11.0 to your environment. <code>tephra2/2.0/gnu-4.9.2</code> Adds Tephra2 version 2.0 to your environment. <code>tephra2/normal/r149</code> Adds Tephra2 version r149 to your environment. <code>tesseract/3.05.01</code> Adds Tesseract 3.05.01 to your environment. <code>texinfo/5.2/gnu-4.9.2</code> Adds GNU texinfo 5.2 to your environment. <code>texinfo/6.6/gnu-4.9.2</code> Adds GNU texinfo 6.6 to your environment. <code>texlive/2014</code> Adds TeX Live 2014 to your environment. <code>texlive/2015</code> Adds TeX Live 2015 to your environment. <code>texlive/2019</code> Adds TeX Live 2019 to your environment. <code>textract/1.5.0</code> Adds textract 1.5.0 to your environment. textract extracts text from a wide range of document types. <code>tinker-hp/1.2/intel-2018</code> Tinker-HP is a CPUs and GPUs based, multi-precision, MPI massively parallel package dedicated to long polarizable molecular dynamics simulations and to polarizable QM/MM. <code>tmux/2.2</code> This module adds the tmux 2.2 package to your environment. <code>tmux/3.2a</code> This module adds the tmux 3.2a package to your environment. <code>tmux/3.3a</code> This module adds the tmux 3.3a package to your environment. <code>tophat/2.1.0</code> Adds Tophat 2.1.0 to your environment. <code>tracer/1.6</code> Adds Tracer 1.6. <code>tractor/3.2.5</code> Adds TractoR 3.2.5 to your environment. <code>tree/1.7.0</code> Adds tree 1.7.0 to your environment. This shows your directory structure as a tree. <code>trim_galore/0.4.1</code> Adds Trim Galore 0.4.1 to your environment. A wrapper tool around Cutadapt and FastQC to consistently apply quality and adapter trimming to FastQ files. <code>trim_galore/0.6.10</code> Adds Trim Galore 0.6.10 to your environment. A wrapper tool around Cutadapt and FastQC to consistently apply quality and adapter trimming to FastQ files. <code>trimmomatic/0.33</code> Adds Trimmomatic 0.33 to your environment. A flexible read trimming tool for Illumina NGS data. <code>turbomole/6.4/mpi</code> Adds turbomole 6.4 (using MPI) to your environment. <code>turbomole/6.4/serial</code> Adds turbomole 6.4 (serial) to your environment. <code>turbomole/6.4/smp</code> Adds turbomole 6.4 (using SMP) to your environment. <code>turbomole/6.5/mpi</code> Adds turbomole 6.5 (using MPI) to your environment. <code>turbomole/6.5/serial</code> Adds turbomole 6.5 (serial) to your environment. <code>turbomole/6.5/smp</code> Adds turbomole 6.5 (using SMP) to your environment. <code>turbomole/6.6/mpi</code> Adds turbomole 6.6 (using MPI) to your environment. <code>turbomole/6.6/serial</code> Adds turbomole 6.6 (serial) to your environment. <code>turbomole/6.6/smp</code> Adds turbomole 6.6 (using SMP) to your environment. <code>ubpred/1-bin32dist</code> UbPred is a random forest-based predictor of potential ubiquitination sites in proteins. <code>varscan/2.3.9</code> Adds VarScan v2.3.9 to your environment. VarScan is a platform-independent mutation caller for targeted, exome, and whole-genome resequencing data generated on Illumina, SOLiD, Life/PGM, Roche/454, and similar instruments. <code>vasp/5.4.1-05feb16-p2/intel-2015-update2</code> The VASP Quantum Chemistry package, version 5.4.1-05feb16 with patches 1 and 2. <code>vasp/5.4.1-24jun15-p2-vtst-r160/intel-2015-update2</code> Adds VASP 5.4.1 built with VTST r160 to your environment. <code>vasp/5.4.1-24jun15-p08072015/intel-2015-update2</code> The VASP Quantum Chemistry package, version 5.4.1-24jun15 with patch 08072015. <code>vasp/5.4.4-18apr2017-libbeef-vtst198/intel-2017-update1</code> Adds VASP 5.4.4 with BEEF-vdW functionals and VTST to your environment. <code>vasp/5.4.4-18apr2017-libbeef/intel-2017-update1</code> Adds VASP 5.4.4 with BEEF-vdW functionals to your environment. <code>vasp/5.4.4-18apr2017-vtst-r178/intel-2017-update1</code> Adds VASP 5.4.4 built with VTST r178 to your environment. <code>vasp/5.4.4-18apr2017/intel-2017-update1</code> Adds VASP 5.4.4 to your environment. <code>vasp/5.4.4-18apr2017/intel-2019</code> Adds VASP 5.4.4 to your environment. Built with Intel 2019. <code>vasp/6.1.2-01dec2020/intel-2019-update5</code> Adds VASP 6.1.2 to your environment. <code>vasp/6.2.1-19Jan2022/intel-2019-update5</code> Adds VASP 6.2.1 to your environment. <code>vasp/6.2.1-19Jan2022/nvidia-22.1-gpu</code> Adds VASP 6.2.1 FOR GPUS ONLY to your environment. <code>vasp/6.3.0-24Jan2022/intel-2019-update5</code> Adds VASP 6.3.0 to your environment. <code>vasp/6.3.0-24Jan2022/nvidia-22.1-gpu</code> Adds VASP 6.3.0 FOR GPUS ONLY to your environment. <code>vcftools/0.1.15/gnu-4.9.2</code> Adds VCFtools version 0.1.15 to your environment. Tools for working with VCF files. <code>velvet/1.2.10</code> Adds Velvet 1.2.10 to your environment. <code>vep/95.0</code> Adds VEP 95.0 to your environment. <code>vesta/3.4.6-bindist</code> VESTA is a 3D visualization program for structural models, volumetric data such as electron/nuclear densities, and crystal morphologies. <code>vg/1.11.0</code> This is a module with no description string. <code>viennarna/2.1.9/gnu-4.9.2</code> Adds viennarna 2.1.9 to your environment. <code>vinalc/1.1.2/gnu-4.9.2</code> Adds VinaLC to your environment. <code>visit/2.9.2</code> This package adds VisIt 2.9.2 to your environment. VisIt is a distributed, parallel visualization and graphical analysis tool for data defined on two- and three-dimensional (2D and 3D) meshes. Visit will create a ~/.visit directory and a ~/Scratch/visit directory. Jobfiles created by the GUI will go in the latter. Legion hostfile: /shared/ucl/apps/visit/2.9.2/gnu-4.9.2/host_legion.xml Web: https://wci.llnl.gov/simulation/computer-codes/visit/ <code>vmd/1.9.3/GL-CUDA</code> VMD is a molecular visualization program for displaying, animating, and analyzing large biomolecular systems using 3-D graphics and built-in scripting. <code>vmd/1.9.3/text-only</code> The binary, text only version of VMD 1.9.3 <code>vmd/1.9.4/a55-gl-cuda</code> Adds VMD 1.9.4 Development Version a55 to your environment. <code>vt/2018-08-01/gnu-4.9.2</code> [ref:f6d2b5dab73c] A tool set for short variant discovery in genetic sequence data. <code>xmds/2.2.2</code> Adds XMDS 2.2.2 (GNU/ATLAS/Intel MPI/FFTW toolchain) to your environment. <code>xmds/3.0.0</code> Adds XMDS 3.0.0 (GNU/ATLAS/Intel MPI/FFTW toolchain) to your environment. <code>xtalopt/r12.1/gnu-4.9.2</code> Adds XtalOpt r12.1 to your environment. <code>xulrunner/3.6.28/gnu-4.9.2</code> Adds XULRunner 3.6.28 to your environment. XULRunner is a Mozilla runtime package that can be used to bootstrap XUL+XPCOM applications. This version was built including javaxpcom. <code>xulrunner/10.0.2</code> Adds the XULRunner 3.6.28 64-bit runtime binaries to your environment. XULRunner is a Mozilla runtime package that can be used to bootstrap XUL+XPCOM applications. <code>yambo/4.1.4/intel-2017</code> This is a module with no description string. <code>yambo/5.2.3/intel-2022</code> This is a module with no description string."},{"location":"Installed_Software_Lists/module-packages/#libraries","title":"Libraries","text":"<p>Modules in this section set up your environment to use specific C, C++, or Fortran libraries. This can include being able to use them with other languages, like Python.</p> Module Description <code>apr-util/1.5.4</code> adds APR-util 1.5.4 to your environment variables <code>apr-util/1.6.1</code> adds APR-util 1.6.1 to your environment variables <code>apr/1.5.2</code> adds APR 1.5.2 to your environment variables <code>apr/1.7.0</code> adds APR 1.7.0 to your environment variables <code>argtable/2.13</code> Adds argtable 2.13 to your environment. <code>armadillo/7.400.3/intel-2015-update2</code> Adds armadillo 7.400.3 to your environment. Armadillo is a linear alebra library for C++, aiming to balance speed and ease of use. <code>armadillo/10.4.0/gnu-10.2.0</code> Adds Armadillo Version 10.4.0 C++ library to your environment. <code>arpack-ng/3.4.0/intel-2015-update2</code> Adds arpack-ng 3.4.0 to your environment. ARPACK-NG is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. <code>arpack-ng/3.5.0/gnu-4.9.2-serial</code> Adds arpack-ng 3.5.0 to your environment. ARPACK-NG is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. <code>arpack-ng/3.5.0/gnu-4.9.2-threaded</code> Adds arpack-ng 3.5.0 to your environment. ARPACK-NG is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. <code>arpack-ng/3.5.0/intel-2017</code> Adds arpack-ng 3.5.0 to your environment. ARPACK-NG is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. <code>arpack-ng/3.8.0-threaded/gnu-10.2.0</code> Adds arpack-ng 3.8.0 to your environment. ARPACK-NG is a collection of Fortran77 subroutines designed to solve large scale eigenvalue problems. <code>atlas/3.10.2/gnu-4.9.2</code> adds ATLAS 3.10.2 for GCC 4.9.2 compilers to your environment variables <code>atlas/3.10.2/intel-2015-update2</code> adds ATLAS 3.10.2 for Intel 15 compilers to your environment variables <code>bambamc/0.0.50/gnu-4.9.2</code> Adds bambamc 0.0.50 to your environment. bambamc is a lightweight C implementation of the read name collation code from the larger libmaus/biobambam C++ project. <code>boost/1.75.0/gnu-4.9.2</code> Boost is a collection of miscellaneous C++ libraries. This build does not include Python bindings or MPI support, and is multi-threaded. <code>boost/1_54_0/gnu-4.9.2</code> Adds Boost 1.54.0 with Python libraries to your environment. <code>boost/1_54_0/mpi/gnu-4.9.2</code> Adds Boost 1.54.0 with Python and MPI libraries to your environment. <code>boost/1_54_0/mpi/gnu-4.9.2-ompi-1.10.1</code> Adds Boost 1.54.0 with Python and MPI libraries to your environment. <code>boost/1_54_0/mpi/intel-2015-update2</code> Adds Boost 1.54.0 with Python and MPI libraries to your environment. <code>boost/1_63_0/gnu-4.9.2</code> Adds Boost 1.63.0 with Python libraries to your environment. <code>boost/1_63_0/mpi/gnu-4.9.2</code> Adds Boost 1.63.0 with Python and MPI libraries to your environment. <code>boost/1_63_0/mpi/intel-2017-update1</code> Adds Boost 1.63.0 with Python and Intel MPI libraries to your environment. <code>cernlib/2006-35</code> Adds the CERN Program library to your environment. 2006-35 EL6 RPM binaries. <code>cernlib/2006/gnu-4.9.2</code> Adds the CERN Program library to your environment <code>cfitsio/3370/gnu-4.9.2</code> Adds cfitsio 3370 to your environment. <code>cfitsio/3370/intel-2015-update2</code> Adds cfitsio 3370 to your environment. <code>cgal/4.9/gnu-4.9.2</code> Adds CGAL 4.9 with Qt5 to your environment. The Computational Geometry Algorithms Library. <code>clusteringsuite/2.6.6/bindist</code> Adds clusteringsuite 2.6.6 to your environment. Clustering Suite is a set of tools to automatically expose the main performance trends in applications' computation structure. <code>cppunit/1.15.1/gnu-4.9.2</code> Adds CppUnit 1.15.1 to your environment. CppUnit is a package for writing and running unit tests in C++. <code>cppunit/1.15.1/gnu-10.2.0</code> Adds CppUnit 1.15.1 built wuth GNU 10.2.0 to your environment. CppUnit is a package for writing and running unit tests in C++. <code>cubelib/4.4/intel-2022</code> Cube is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. <code>cubelib/4.8.2/intel-2022</code> Cube is a generic tool for displaying a multi-dimensional performance space consisting of the dimensions (i) performance metric, (ii) call path, and (iii) system resource. <code>cubew/4.8.2/intel-2022</code> CubeW is a high performance C writer library for the Cube4 data format. <code>cudnn/5.1/cuda-7.5</code> Adds cuDNN to your environment. <code>cudnn/5.1/cuda-8.0</code> Adds cuDNN to your environment. <code>cudnn/6.0/cuda-7.5</code> Adds cuDNN to your environment. <code>cudnn/6.0/cuda-8.0</code> Adds cuDNN to your environment. <code>cudnn/7.0.4/cuda-8.0</code> Adds cuDNN to your environment. <code>cudnn/7.1.4/cuda-9.0</code> Adds cuDNN to your environment. <code>cudnn/7.4.2.24/cuda-9.0</code> Adds cuDNN to your environment. <code>cudnn/7.4.2.24/cuda-10.0</code> Adds cuDNN to your environment. <code>cudnn/7.5.0.56/cuda-10.0</code> Adds cuDNN to your environment. <code>cudnn/7.5.0.56/cuda-10.1</code> Adds cuDNN to your environment. <code>cudnn/7.6.5.32/cuda-10.0</code> Adds cuDNN to your environment. <code>cudnn/7.6.5.32/cuda-10.1</code> Adds cuDNN to your environment. <code>cudnn/8.1.0.77/cuda-11.2</code> Adds cuDNN 8.1.0 to your environment. <code>cudnn/8.2.1.32/cuda-11.3</code> Adds cuDNN 8.2.1 to your environment. <code>cudnn/9.2.0.82/cuda-11</code> The NVIDIA CUDA\u00ae Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. This version is for CUDA 11. <code>cudnn/9.2.0.82/cuda-12</code> The NVIDIA CUDA\u00ae Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. This version is for CUDA 12. <code>cunit/2.1-3/gnu-4.9.2</code> Adds cunit 2.1-3 to your environment. CUnit is a package for writing and running unit tests in C. <code>cvmfs/2.2.1/gnu-4.9.2</code> Adds libcvmfs 2.2.1 to your environment. <code>dyninst/9.3.2/gnu-4.9.2</code> Adds dyninst 9.3.2 to your environment. DynInst is a library for performing dynamic instrumentation of executables. <code>eigen/3.2.5/gnu-4.9.2</code> adds Eigen for GCC 4.9.2 compilers to your environment variables <code>eigen/3.3.9/gnu-4.9.2</code> Adds Eigen 3.3.9 to your environment. Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. <code>eigen/3.4.0/gnu-10.2.0</code> Adds Eigen 3.4.0 to your environment. Eigen is a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms. <code>elfutils/0.170/gnu-4.9.2</code> Adds elfutils 0.170 to your environment. Elfutils provides utilities for manipulating binary ELF files, and is one possible provider of libelf. <code>fftw/2.1.5/gnu-4.9.2</code> adds FFTW 2.1.5 for GCC 4.9.2 compilers to your environment variables <code>fftw/2.1.5/intel-2015-update2</code> adds FFTW 2.1.5 for Intel compilers to your environment variables <code>fftw/3.3.4-impi/gnu-4.9.2</code> Adds fftw 3.3.4 (built with Intel MPI) to your environment. <code>fftw/3.3.4-impi/intel-2017-update1</code> Adds fftw 3.3.4 (built with Intel MPI) to your environment. <code>fftw/3.3.4-ompi-1.10.1/gnu-4.9.2</code> Adds fftw 3.3.4 (built with OpenMPI) to your environment. <code>fftw/3.3.4-ompi/gnu-4.9.2</code> Adds fftw 3.3.4 (built with OpenMPI) to your environment. <code>fftw/3.3.4-threads/gnu-4.9.2</code> adds FFTW 3.3.4 for GCC 4.9.2 compilers to your environment variables <code>fftw/3.3.4/gnu-4.9.2</code> adds FFTW 3.3.4 for GCC 4.9.2 compilers to your environment variables <code>fftw/3.3.4/intel-2015-update2</code> adds FFTW 3.3.4 for Intel compilers to your environment variables <code>fftw/3.3.6-pl2/gnu-4.9.2</code> Adds FFTW 3.3.6 pl2 for GCC 4.9.2 compilers to your environment variables. Includes single and double precision only on Legion, plus long-double and quad on Grace/Thomas. Includes OpenMP and POSIX threads libraries. <code>fftw/3.3.6-pl2/intel-2017</code> Adds FFTW 3.3.6 pl2 for Intel compilers to your environment variables. Includes single and double precision versions on Legion, plus long-double on Grace/Thomas. Includes OpenMP and POSIX threads libraries. <code>fftw/3.3.8-impi/intel-2018</code> Adds fftw fftw (built with Intel MPI) to your environment. <code>fftw/3.3.8-ompi/gnu-4.9.2</code> Adds fftw 3.3.8 (built with OpenMPI) to your environment. <code>fftw/3.3.8/gnu-7.3.0</code> Adds FFTW 3.3.8 for GCC 7.3.0 compilers to your environment variables. Includes single and double precision, plus long-double and quad. Includes OpenMP and POSIX threads libraries. <code>fftw/3.3.8/gnu-9.2.0</code> Adds FFTW 3.3.8 for GCC 9.2.0 compilers to your environment variables. Includes single and double precision, plus long-double and quad. Includes OpenMP and POSIX threads libraries. <code>fftw/3.3.9/gnu-10.2.0</code> Adds FFTW 3.3.9 for GCC 10.2.0 compilers to your environment variables. Includes single and double precision, plus long-double and quad. Includes OpenMP and POSIX threads libraries. <code>fftw/3.3.10-impi/intel-2022</code> Adds fftw fftw (built with Intel 2022 Comilers and MPI) to your environment. <code>fftw/3.3.10/nvidia-22.1</code> Adds fftw 3.3.10 to your environment. <code>forge/1.0.0/gnu-4.9.2</code> Adds forge 1.0.0 to your environment. <code>freeimage/3.17.0/gnu-4.9.2</code> Adds FreeImage 3.17.0 to your environment. <code>freetype/2.8.1/gnu-4.9.2</code> Adds freetype 2.8.1 to your environment. FreeType is a freely available software library to render fonts. <code>ga/5.7-8BInts/intel-2018</code> Global Arrays (GA) is a library that provides a Partitioned Global Address Space (PGAS) programming model. This version has been compiled with 8-byte integers in the Fortran code. <code>ga/5.7/intel-2018</code> Global Arrays (GA) is a library that provides a Partitioned Global Address Space (PGAS) programming model. <code>gcc-libs/4.9.2</code> adds GCC 4.9.2 runtime to your evironment. <code>gcc-libs/7.3.0</code> Base module for gcc 7.3.0 -- does not set the standard compiler environment variables. The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). <code>gcc-libs/8.3.0</code> Base module for gcc 8.3.0 -- does not set the standard compiler environment variables. The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). <code>gcc-libs/9.2.0</code> Base module for gcc 9.2.0 -- does not set the standard compiler environment variables. The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). <code>gcc-libs/10.2.0</code> Base module for gcc 10.2.0 -- does not set the standard compiler environment variables. The GNU Compiler Collection includes front ends for C, C++, Objective-C, and Fortran, as well as libraries for these languages (libstdc++,...). Patch 95889 for __has_include applied. <code>geos/3.5.0/gnu-4.9.2</code> Adds geos 3.5.0 to your environment. GEOS (Geometry Engine, Open Source) is a library for performing various spatial operations, especially for boolean operations on GIS data. Note this version does not include the SWIG, Python, Ruby, or PHP bindings. <code>geos/3.8.1/gnu-9.2.0</code> Adds geos 3.8.1 to your environment. GEOS (Geometry Engine, Open Source) is a library for performing various spatial operations, especially for boolean operations on GIS data. Note this version does not include the SWIG, Python, Ruby, or PHP bindings. <code>geos/3.9.1/gnu-10.2.0</code> Adds geos 3.9.1 to your environment. GEOS (Geometry Engine, Open Source) is a library for performing various spatial operations, especially for boolean operations on GIS data. Note this version does not include the SWIG, Python, Ruby, or PHP bindings. <code>gflags/2.2.1</code> Adds Google gflags 2.2.1 to your environment. <code>giflib/5.1.1</code> Adds giflib 5.1.1 to your environment. A library and utilities for processing gifs. <code>glbinding/2.1.2/gnu-4.9.2</code> Adds glbinding 2.1.2 to your environment. <code>glew/1.13.0/gnu-4.9.2</code> Adds GLEW The OpenGL Extension Wrangler Library 1.11.0 to your environment. <code>glew/2.1.0/gnu-4.9.2</code> Adds GLEW -- The OpenGL Extension Wrangler Library -- 2.1.0 to your environment. <code>glfw/3.2.1/gnu-4.9.2</code> Adds GLFW 3.2.1 to your environment. <code>glog/0.3.5</code> Adds Google glog 0.3.5 to your environment. <code>glpk/4.60/gnu-4.9.2</code> Adds the GNU Linear Programming Kit Version 4.60 for GCC 4.9.2 to your environment. <code>gsl/1.16/gnu-4.9.2</code> adds GSL 1.16 for GCC 4.9.2 to your environment. <code>gsl/1.16/intel-2015-update2</code> Adds gsl 1.16 to your environment. <code>gsl/2.4/gnu-4.9.2</code> adds GSL 2.4 for GCC 4.9.2 to your environment. <code>gsl/2.4/intel-2017</code> adds GSL 2.4 for Intel 2017 to your environment. <code>gsl/2.6/gnu-9.2.0</code> adds GSL 2.6 for GCC 9.2.0 to your environment. <code>gsl/2.7/gnu-10.2.0</code> adds GSL 2.7 for GCC 10.2.0 to your environment. <code>gstreamer/1.12.0</code> GStreamer is a library for constructing graphs of media-handling components, including codecs for various audio and video formats. <code>gulp/4.5/libgulp/intel-2018</code> Adds GULP 4.5 library version to your environment. Built libgulp only, without FoX, for programs such as ChemShell to link. GULP is a materials simulation code. <code>h5py/2.10.0-ompi/gnu-4.9.2</code> Adds h5py 2.10.0-ompi for Python 3.7 to your environment. <code>harminv/1.4.1/gnu-4.9.2</code> Adds harminv 1.4.1 to your environment. <code>harminv/1.4/gnu-4.9.2</code> Adds harminv 1.4 to your environment. <code>hdf/5-1.8.15-p1-impi/intel-2015-update2</code> Adds hdf5 1.8.5-p1 (built with Fortran and IntelMPI options) to your environment. <code>hdf/5-1.8.15-p1-ompi/gnu-4.9.2</code> Adds hdf5 1.8.5-p1 (built with Fortran and OpenMPI options) to your environment. <code>hdf/5-1.8.15/gnu-4.9.2</code> adds HDF5 1.8.15 (Serial) for GCC 4.9.2 to your environment. <code>hdf/5-1.8.15/intel-2015-update2</code> adds HDF5 1.8.15 (Serial) for Intel 2015 to your environment. <code>hdf/5-1.10.2-impi/intel-2018</code> adds HDF5 1.10.2 (Parallel) for Intel 2018 to your environment. <code>hdf/5-1.10.2/intel-2018</code> adds HDF5 1.10.2 (Serial) for Intel 2018 to your environment. <code>hdf/5-1.10.5-ompi/gnu-4.9.2</code> Adds hdf 5-1.10.5-ompi to your environment. Built with OpenMPI and GNU. <code>hdf/5-1.10.5/gnu-4.9.2</code> Adds hdf 5-1.10.5 to your environment. Serial version built with GNU. <code>hdf/5-1.10.5/gnu-9.2.0</code> Adds hdf 5-1.10.5 to your environment. Serial version built with GNU. <code>hdf/5-1.10.6/gnu-10.2.0</code> Adds hdf 5-1.10.6 to your environment. Serial version built with GNU 10.2.0. <code>hdf/5-1.12.3-impi/intel-2022</code> adds HDF5 1.12.3 (Parallel) for Intel 2022 to your environment. <code>htslib/1.2.1</code> This module adds the HTSlib 1.2.1 package to your environment. HTSlib is an implementation of a unified C library for accessing common file formats, such as SAM, CRAM and VCF, used for high-throughput sequencing data. <code>htslib/1.3.1</code> This module adds the HTSlib 1.3.1 package to your environment. HTSlib is an implementation of a unified C library for accessing common file formats, such as SAM, CRAM and VCF, used for high-throughput sequencing data. <code>htslib/1.7</code> This module adds the HTSlib 1.7 package to your environment. HTSlib is an implementation of a unified C library for accessing common file formats, such as SAM, CRAM and VCF, used for high-throughput sequencing data. <code>htslib/1.11/gnu-4.9.2</code> A C library for reading/writing high-throughput sequencing data. <code>hwloc/1.11.12</code> The Portable Hardware Locality (hwloc) software package provides a portable abstraction (across OS, versions, architectures, ...) of the hierarchical topology of modern architectures, including NUMA memory nodes, sockets, shared caches, cores and simultaneous multithreading. This installation includes the optional libnuma dependency. <code>hypre/2.11.2/openmpi-3.0.0/intel-2017</code> Adds HYPRE 2.11.2 to your environment. <code>hypre/2.11.2/openmpi-3.1.1/intel-2018</code> Adds HYPRE 2.11.2 to your environment. <code>ipopt/3.14.2/intel-2018</code> Ipopt (Interior Point OPTimizer, pronounced eye-pea-Opt) is a software package for large-scale nonlinear optimization. <code>jansson/2.11</code> This is a module with no description string. <code>json-c/0.12/gnu-4.9.2</code> Adds json-c 0.12 to your environment. JSON-C is a library for converting between JSON-formatted strings and C representations of the equivalent objects. <code>lapack/3.8.0/gnu-4.9.2</code> LAPACK is a reference library of routines for Linear Algebra. It is not recommended for use, as its ABI is replicated in the much higher-performance libraries OpenBLAS, MKL, or ATLAS instead. <code>leptonica/1.74.4</code> Adds Leptonica 1.74.4 to your environment. <code>leveldb/1.20</code> Adds Google leveldb 1.20 to your environment. <code>libbdwgc/7.4.2/gnu-4.9.2</code> Adds libbdwgc (a garbage-collector library) to your environment. <code>libbeef/0.1.3/intel-2018</code> Library for Bayesian error estimation functionals for use in density functional theory codes: libbeef 0.1.3 commit 2822afe <code>libctl/3.2.2/gnu.4.9.2</code> Adds libctl (built using Intel compilers) to your environment. <code>libctl/4.3.0/gnu-4.9.2</code> Adds libctl 4.3.0 to your environment. <code>libdwarf/20170709/gnu-4.9.2</code> Adds libdwarf 20170709 to your environment. libdwarf is a library for interacting with debugging info in the DWARF 2, 3, and 5 formats. <code>libelf/0.8.13/gnu-4.9.2</code> Adds libelf 0.8.13 to your environment. <code>libetsfio/1.0.4/gnu-4.9.2</code> Adds libetsfio 1.0.4 to your environment. <code>libetsfio/1.0.4/intel-2015-update2</code> Adds libetsfio 1.0.4 to your environment. <code>libflac/1.3.1/gnu-4.9.2</code> Adds libflac 1.3.1 to your environment. libFLAC is the Xiph library for handling their lossless audio codec. <code>libgd/2.1.1/gnu-4.9.2</code> Adds libgd 2.1.1 to your environment. <code>libgd/2.1.1/intel-2015-update2</code> Adds libgd 2.1.1 to your environment. <code>libgdsii/0.21/gnu-4.9.2</code> Adds libgdsii 0.21 to your environment. C++ library and command-line utility for reading GDSII geometry files. <code>libint/1.1.4/gnu-4.9.2</code> Adds libint 1.1.4 to your environment.  Libint is required for CP2K. <code>libmatheval/1.1.11</code> Adds libmatheval 1.1.11 to your environment. GNU libmatheval is a library (callable from C and Fortran) to parse and evaluate symbolic expressions input as text. <code>libpng/1.6.37/gnu-9.2.0</code> Adds libpng 1.6.37 to your environment. <code>libsodium/1.0.6/gnu-4.9.2</code> Adds libsodium 1.0.6 to your environment. libsodium is a crypto library primarily used by ZeroMQ. <code>libsox/14.4.2/gnu-4.9.2</code> Adds libsox 14.4.2 to your environment. SoX is a library for reading, writing, and converting a variety of sound file formats. If you require support for a file format that is not installed, contact rc-support and the library can be rebuilt. <code>libuuid/1.0.3/gnu-4.9.2</code> Adds a static libuuid 1.0.3 to your environment. <code>libxc/2.1.2/intel-2015-update2</code> Adds libxc 2.1.2 to your environment. <code>libxc/2.2.2/gnu-4.9.2</code> Adds libxc 2.2.2 to your environment. <code>libxc/2.2.2/intel-2015-update2</code> Adds libxc 2.2.2 to your environment. <code>libxc/3.0.0/gnu-4.9.2</code> Adds libxc 3.0.0 to your environment. <code>libxc/3.0.0/intel-2015-update2</code> Adds libxc 3.0.0 to your environment. <code>libxc/4.2.3/intel-2018</code> libxc is a library of routines implementing a range of exchange-correlation functionals for density-functional theory calculations. <code>libxc/6.2.2/intel-2022</code> libxc is a library of routines implementing a range of exchange-correlation functionals for density-functional theory calculations. <code>libxml2/2.9.4/gnu-4.9.2</code> Adds libxml2 2.9.4 to your environment. Libxml2 is an XML C parser and toolkit. Includes Python (2.7.9) bindings. <code>llvm/3.3</code> This module adds the LLVM 3.3 package to your environment. The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs. <code>llvm/3.9.1</code> This module adds the LLVM 3.9.1 package to your environment. The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs. <code>llvm/6.0.1</code> This module adds the LLVM 6.0.1 package to your environment. The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. The LLVM Core libraries provide a modern source- and target-independent optimizer, along with code generation support for many popular CPUs. <code>llvm/8.0.0/gnu-4.9.2</code> The LLVM Project is a collection of modular and reusable compiler and toolchain technologies. This installation includes clang, a C compiler based on LLVM. <code>lmdb/0.9.22</code> Adds LMDB 0.9.22 to your environment. <code>lz4/1.8.3</code> This is a module with no description string. <code>magma/2.4.0</code> This is a module with no description string. <code>med/4.0.0/gnu-4.9.2</code> Adds med 4.0.0 to your environment. Allows reading and writing of MED format files. <code>med/4.0.0/gnu-9.2.0</code> Adds med 4.0.0 to your environment. Allows reading and writing of MED format files. <code>mesa/6.5/gnu-4.9.2</code> Adds mesa 6.5 to your environment. Mesa is an open-source implementation of the OpenGL specification - a system for rendering interactive 3D graphics. This is an old version installed to satisfy a particular dependency: please do not use for new builds. <code>mesa/10.6.3</code> Adds Mesa 10.6.3 to your environment. Mesa is an open-source implementation of the OpenGL specification. (Built for offscreen rendering: OSMesa, Xlib GLX, no Gallium, no EGL, no llvm, no DRI). <code>mesa/10.6.9/gnu-4.9.2</code> Adds Mesa 10.6.9 to your environment. Mesa is an open-source implementation of the OpenGL specification. (Built for offscreen rendering: OSMesa, Xlib GLX, no Gallium, no EGL, no llvm, no DRI). <code>mesa/13.0.6/gnu-4.9.2</code> Adds Mesa 13.0.6 to your environment. Mesa is an open-source implementation of the OpenGL specification. (Built options: Gallium, LLVM, no EGL, no DRI, no GLX). The default driver is llvmpipe. You can use \"export GALLIUM_DRIVER\" to explicitly choose llvmpipe, softpipe, or swr <code>metis/5.1.0/gnu-4.9.2</code> Adds metis 5.1.0 to your environment. METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. <code>metis/5.1.0/intel-2015-update2</code> Adds metis 5.1.0 to your environment. METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. <code>metis/5.1.0/intel-2018</code> Adds metis 5.1.0 to your environment. METIS is a set of serial programs for partitioning graphs, partitioning finite element meshes, and producing fill reducing orderings for sparse matrices. <code>mpi/intel/2013/update1/intel</code> adds Intel MPI 4.1.3.048 to your environment variables <code>mpi/intel/2015/update3/gnu-4.9.2</code> adds Intel MPI to your environment variables <code>mpi/intel/2015/update3/intel</code> adds Intel MPI to your environment variables <code>mpi/intel/2017/update1/gnu-4.9.2</code> adds Intel MPI to your environment variables <code>mpi/intel/2017/update1/intel</code> adds Intel MPI to your environment variables <code>mpi/intel/2017/update2/gnu-4.9.2</code> adds Intel MPI to your environment variables configured to use GCC 4.9.2 <code>mpi/intel/2017/update2/intel</code> adds Intel MPI to your environment variables <code>mpi/intel/2017/update3/gnu-4.9.2</code> [Intel MPI/2017.3.196] This is Intel's MPI implementation, version 2017.3.196, which is bundled with compiler package version 2017.Update4. This module sets up the compiler wrappers to use GCC 4.9.2 underneath. <code>mpi/intel/2017/update3/intel</code> [Intel MPI/2017.3.196] This is Intel's MPI implementation, version 2017.3.196, which is bundled with compiler package version 2017.Update4. <code>mpi/intel/2018/update3/intel</code> [Intel MPI/2018.3.222] This is Intel's MPI implementation, version 2018.3.222, which is bundled with compiler package version 2018.Update3. <code>mpi/intel/2019/update4/intel</code> [Intel MPI/2019.4.243] This is Intel's MPI implementation, version 2019.4.243, which is bundled with compiler package version 2019.Update4. <code>mpi/intel/2019/update5/intel</code> [Intel MPI/2019.5.281] This is Intel's MPI implementation, version 2019.5.281, which is bundled with compiler package version 2019.Update5. <code>mpi/intel/2019/update6/intel</code> [Intel MPI/2019.6.166] This is Intel's MPI implementation, version 2019.6.166, which is bundled with compiler package version 2020. <code>mpi/intel/2021.6.0/intel</code> [Intel MPI/2021.6.0] This is Intel's MPI implementation, version 2021.6.0, which is bundled with compiler package version 2022.2. <code>mpi/intel/2021.11/intel</code> [Intel MPI/2021.11] This is Intel's MPI implementation, version 2021.11, which is bundled with compiler package version 2024.0.1. <code>mpi/openmpi/1.8.4/gnu-4.9.2</code> adds OpenMPI 1.8.4 for GCC 4.9.2 compilers to your environment variables <code>mpi/openmpi/1.8.4/intel-2015-update2</code> adds OpenMPI 1.8.4 for Intel 2015 update 2 compilers to your environment variables <code>mpi/openmpi/1.10.1/gnu-4.9.2</code> adds OpenMPI 1.10.1 for GCC 4.9.2 compilers to your environment variables <code>mpi/openmpi/1.10.1/intel-2015-update2</code> adds OpenMPI 1.10.1 for Intel 2015 update 2 compilers to your environment variables <code>mpi/openmpi/2.0.2/gnu-4.9.2</code> Adds openmpi 2.0.2 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/2.0.2/intel-2017</code> Adds openmpi 2.0.2 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/2.1.2/gnu-4.9.2</code> Adds openmpi 2.1.2 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/2.1.2/intel-2017</code> Adds openmpi 2.1.2 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/3.0.0/gnu-4.9.2</code> Adds openmpi 3.0.0 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/3.0.0/intel-2017</code> Adds openmpi 3.0.0 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/3.1.1/gnu-4.9.2</code> Adds openmpi 3.1.1 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/3.1.1/intel-2018</code> Adds openmpi 3.1.1 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/3.1.4/gnu-4.9.2</code> Adds openmpi 3.1.4 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/3.1.4/gnu-7.3.0</code> Adds openmpi 3.1.4 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/3.1.4/intel-2018</code> Adds openmpi 3.1.4 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/3.1.5/gnu-9.2.0</code> Adds openmpi 3.1.5 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/3.1.6/gnu-4.9.2</code> Adds openmpi 3.1.6 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/4.0.3/gnu-4.9.2</code> Adds openmpi 4.0.3 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/4.0.5/gnu-10.2.0</code> Adds openmpi 4.0.5 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi/openmpi/4.1.1/gnu-4.9.2</code> Adds openmpi 4.1.1 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. Note: we set OMPI_MCA_btl=vader on OmniPath clusters for this version. As a result it will work multi-node but be slower than other transports. <code>mpi/openmpi/4.1.1/intel-2022</code> Adds openmpi 4.1.1 to your environment. The Open MPI Project is an open source Message Passing Interface implementation. <code>mpi4py/2.0.0/python2</code> Adds Python2 mpi4py 2.0.0 to your environment. MPI for Python. <code>mpi4py/2.0.0/python3</code> Adds Python3 mpi4py 2.0.0 to your environment. MPI for Python. <code>mpi4py/3.0.0/python3</code> Adds Python3 mpi4py 3.0.0 to your environment. MPI for Python. <code>mpi4py/3.0.2/gnu-4.9.2</code> Adds mpi4py 3.0.2 for Python 3.7 to your environment. <code>mpi4py/3.1.4/gnu-4.9.2</code> Adds mpi4py 3.1.4 for Python 3.9 to your environment. <code>multinest/3.10b/gnu-4.9.2</code> Adds MultiNest 3.10b to your environment. <code>mumps-thirdparty/3.0.0/intel-2018</code> The COIN-OR Tools project ThirdParty-Mumps can be used to download the MUMPS code and build a MUMPS library that is recognized by Ipopt. This version of ThirdParty-Mumps retrieves and builds MUMPS 5.4.0. <code>mumps/5.2.1/gnu-9.2.0</code> Adds mumps 5.2.1 to your environment. Sequential (threaded) version built with GNU, OpenBLAS and METIS. <code>mumps/5.2.1/intel-2018</code> Adds mumps 5.2.1 to your environment. Sequential version built with Intel and METIS. <code>mysql-connector-python/2.0.4/python-3.5.2</code> Adds mysql-connector-python 2.0.4 to your environment. This is Oracle's python-only MySQL connector for Python3 <code>mysql-connector-python/2.0.4/python-3.6.3</code> Adds mysql-connector-python 2.0.4 to your environment. This is Oracle's python-only MySQL connector for Python3 <code>mysql-connector-python/2.0.4/python-3.7.4</code> Adds mysql-connector-python 2.0.4 to your environment. This is Oracle's python-only MySQL connector for Python3 <code>mysql-connector-python/2.0.4/python-3.8.0</code> Adds mysql-connector-python 2.0.4 to your environment. This is Oracle's python-only MySQL connector for Python3 <code>mysql-connector-python/8.0.22/python-3.8.6</code> Adds mysql-connector-python 8.0.22 to your environment. This is Oracle's python-only MySQL connector for Python3 <code>mysql-connector-python/8.0.22/python-3.9.0</code> Adds mysql-connector-python 8.0.22 to your environment. This is Oracle's python-only MySQL connector for Python3 <code>mysql-connector-python/8.0.22/python-3.9.6</code> Adds mysql-connector-python 8.0.22 to your environment. This is Oracle's python-only MySQL connector for Python3 <code>mysql-connector-python/8.0.28/python-3.9.10</code> Adds mysql-connector-python 8.0.28 to your environment. This is Oracle's python-only MySQL connector for Python3 <code>nag/fortran/mark22/gnu-4.9.2</code> Adds NAG Fortran Library Mark 22 for GCC to your environment. <code>nag/fortran/mark24/gnu-4.9.2</code> Adds NAG Fortran Library Mark 24 for GCC to your environment. <code>nag/fortran/mark24/nag-6.0.1044</code> Adds NAG Fortran Library Mark 24 for NAG Fortran to your environment. <code>nag/fortran/mark25/intel-2015-update2</code> Adds NAG Fortran Library Mark 25 for Intel 2015 to your environment. <code>nag/fortran/mark26/gnu-4.9.2</code> Adds NAG Fortran Library Mark 26 for GCC to your environment. <code>nag/fortran/mark26/intel-2017</code> Adds NAG Fortran Library Mark 26 for Intel 2017 to your environment. <code>nag/fortran/mark26/nag-6.1.6106</code> Adds NAG Fortran Library Mark 26 for NAG Fortran to your environment. <code>nag/fortran/mark26/nag-6.2.6223</code> Adds NAG Fortran Library Mark 26 for NAG Fortran to your environment. <code>nag/mark27/intel-2019</code> Adds NAG Library Mark 27 for Intel 2019 to your environment. <code>nag/mark30/intel-2022</code> Adds NAG Library Mark 30 for Intel 2022 to your environment. <code>netcdf-c++/4.2/gnu-4.9.2</code> adds NetCDF C++ 4.2 for GCC to your environment. <code>netcdf-c++/4.2/intel-2015-update2</code> adds NetCDF C++ 4.2 for Intel 2015 to your environment. <code>netcdf-c++4/4.2/gnu-4.9.2</code> adds NetCDF C++ 4.2 for GCC to your environment. <code>netcdf-c++4/4.2/intel-2015-update2</code> adds NetCDF C++ 4.2 for Intel 2015 to your environment. <code>netcdf-fortran/4.4.1/gnu-4.9.2</code> adds NetCDF 4.4.1 for GCC to your environment. <code>netcdf-fortran/4.4.1/intel-2015-update2</code> adds NetCDF 4.4.1 for Intel 2015 to your environment. <code>netcdf-fortran/4.5.4/intel-2018-update3</code> adds NetCDF Fortran 4.5.4 for Intel 2018 to your environment. <code>netcdf-fortran/4.6.1/gnu-10.2.0</code> adds NetCDF 4.6.1 for GCC to your environment. <code>netcdf-fortran/4.6.1/intel-2022</code> adds NetCDF 4.6.1 for Intel 2022 to your environment. <code>netcdf/4.3.3.1/gnu-4.9.2</code> adds NetCDF 4.3.3.1 for GCC 4.9.2 to your environment. <code>netcdf/4.3.3.1/intel-2015-update2</code> adds NetCDF 4.3.3.1 for Intel 2015 to your environment. <code>netcdf/4.7.4/gnu-9.2.0</code> adds NetCDF 4.7.4 for GCC 9.2.0 to your environment. <code>netcdf/4.8.1/gnu-10.2.0</code> adds NetCDF 4.8.1 for GCC 10.2.0 to your environment. <code>netcdf/4.9.0/intel-2018-update3</code> adds NetCDF 4.9.0 for Intel 2018 to your environment. <code>netcdf/4.9.2/gnu-10.2.0</code> adds NetCDF 4.9.2 for GCC 10.2.0 to your environment. <code>netcdf/4.9.2/intel-2022</code> adds NetCDF 4.9.2 for Intel 2022.2 compilers to your environment. <code>numactl/2.0.12</code> numactl provides NUMA policy support, as well as tools and a library to display NUMA allocation statistics and debugging information. <code>opari2/2.0.8/intel-2022</code> OPARI2 is a source-to-source instrumentation tool for OpenMP and hybrid codes. <code>openblas/0.2.14-threads/gnu-4.9.2</code> adds OpenBLAS 0.2.14 for GCC 4.9.2 compilers to your environment variables <code>openblas/0.2.14/gnu-4.9.2</code> adds OpenBLAS 0.2.14 for GCC 4.9.2 compilers to your environment variables <code>openblas/0.2.14/intel-2015-update2</code> adds OpenBLAS 0.2.14 for Intel 2015 update 2compilers to your environment variables <code>openblas/0.3.2-native-threads/gnu-4.9.2</code> This is a module with no description string. <code>openblas/0.3.2-openmp/gnu-4.9.2</code> This is a module with no description string. <code>openblas/0.3.2-serial/gnu-4.9.2</code> This is a module with no description string. <code>openblas/0.3.7-native-threads/gnu-4.9.2</code> This is a module with no description string. <code>openblas/0.3.7-native-threads/gnu-9.2.0</code> OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. <code>openblas/0.3.7-openmp/gnu-4.9.2</code> This is a module with no description string. <code>openblas/0.3.7-openmp/gnu-9.2.0</code> OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. <code>openblas/0.3.7-serial/gnu-4.9.2</code> This is a module with no description string. <code>openblas/0.3.7-serial/gnu-9.2.0</code> OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. <code>openblas/0.3.13-native-threads/gnu-10.2.0</code> OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. <code>openblas/0.3.13-openmp/gnu-10.2.0</code> OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. <code>openblas/0.3.13-serial/gnu-10.2.0</code> OpenBLAS is an optimized BLAS library based on GotoBLAS2 1.13 BSD version. <code>openjpeg/2.4.0/gnu-4.9.2</code> OpenJPEG is an open-source JPEG 2000 codec written in C language. <code>openslide/3.4.1/gnu-4.9.2</code> adds OpenSlide library 3.4.1 to your environment variables <code>openssl/1.1.1t</code> OpenSSL is a widely-used toolkit for general-purpose cryptography and secure communication. <code>openssl/1.1.1u</code> OpenSSL is a widely-used toolkit for general-purpose cryptography and secure communication. <code>otf2/3.0.3/intel-2022</code> The Open Trace Format 2 is a highly scalable, memory efficient event trace data format plus support library. <code>papi/5.5.1/gnu-4.9.2</code> Adds PAPI 5.5.1 to your environment. PAPI is a library for working with performance counters, often used in profiling applications. <code>papi/7.1.0/intel-2022</code> The Performance Application Programming Interface (PAPI) provides tool designers and application engineers with a consistent interface and methodology for the use of low-level performance counter hardware found across the entire compute system. <code>parmetis/4.0.3/intel-2015-update2</code> Adds parmetis 4.0.3 to your environment. ParMETIS is an MPI-based parallel library that implements a variety of algorithms for partitioning unstructured graphs, meshes, and for computing fill-reducing orderings of sparse matrices. <code>pcre2/10.21/gnu-4.9.2</code> Adds pcre2 10.21 to your environment. PCRE (Perl-compatible regular expressions) is a C library implementing regular expression pattern-matching using the same semantics as Perl 5. <code>pcre2/10.35/gnu-9.2.0</code> Adds pcre2 10.35 to your environment. PCRE (Perl-compatible regular expressions) is a C library implementing regular expression pattern-matching using the same semantics as Perl 5. <code>pcre2/10.37/gnu-10.2.0</code> Adds pcre2 10.37 to your environment. PCRE (Perl-compatible regular expressions) is a C library implementing regular expression pattern-matching using the same semantics as Perl 5. <code>pgplot/5.2.2/intel-2017</code> Adds PGPlot 5.2.2 to your environment. <code>pgplot/5.2.2/intel-2018</code> Adds PGPlot 5.2.2 to your environment. <code>pillow-simd/6.0.0.post0/python-3.7.4</code> Adds Pillow-SIMD to your environment. <code>protobuf/3.5.1/gnu-4.9.2</code> adds Google Protocol Buffers for GCC 4.9.2 to your environment. <code>protobuf/3.14.0/gnu-9.2.0</code> adds Google Protocol Buffers for GCC 9.2.0 to your environment. <code>protobuf/3.17.3/gnu-10.2.0</code> adds Google Protocol Buffers for GCC 10.2.0 to your environment. <code>protobuf/12-2017/gnu-4.9.2</code> adds Google Protocol Buffers for GCC 4.9.2 to your environment. <code>psm2/11.2.185/gnu-4.9.2</code> Adds psm2 11.2.185 to your environment. The PSM2 messaging API, libpsm2. A low-level user-level communications interface for the Intel(R) OPA. family of products. <code>pstreams/1.0.1/gnu-4.9.2</code> Adds pstreams 1.0.1 to your environment. PStreams is a C++ wrapper for process control and streaming using popen and pclose. <code>pygsl/2.1.1-python3.6/gnu-4.9.2</code> Adds pygsl 2.1.1 to your environment. PyGSL provides Python bindings for the GNU Scientific Library. <code>pyngl/1.4.0</code> Adds PyNGL to your environment. <code>pynio/1.4.1</code> Adds PyNIO to your environment. <code>quip/18c5440-threads/gnu-4.9.2</code> Adds QUIP to your environment.  QUIP is required for CP2K. <code>quip/18c5440/gnu-4.9.2</code> Adds QUIP to your environment.  QUIP is required for CP2K. <code>quip/c6359e1/gnu-10.2.0</code> Adds QUIP to your environment built with GNU 10.2.0 for recent versions of LAMMPS <code>qutip/4.1.0/python-2.7.12</code> Adds qutip to your environment. <code>scalapack/2.0.2/gnu-4.9.2/openblas</code> Adds ScaLAPACK 2.0.2 to your environment, built with GCC, OpenBLAS and OpenMPI. Static and shared libraries. <code>scalapack/2.0.2/gnu-4.9.2/openblas-0.3.2</code> Adds ScaLAPACK 2.0.2 to your environment, built with GCC, OpenBLAS and OpenMPI. Static and shared libraries. <code>scalapack/2.0.2/gnu-4.9.2/openblas-0.3.7</code> Adds ScaLAPACK 2.0.2 to your environment, built with GCC, OpenBLAS and OpenMPI. Static and shared libraries. <code>scalapack/2.1.0/gnu-9.2.0/openblas-0.3.7</code> ScaLAPACK is a library of high-performance linear algebra routines for parallel distributed memory machines. ScaLAPACK solves dense and banded linear systems, least squares problems, eigenvalue problems, and singular value problems. <code>snappy/1.1.7</code> Adds Google snappy 1.1.7 to your environment. <code>spark/3.1.1-bin-hadoop2.7</code> Apache Spark is an analytics engine for data processing. <code>sparskit2/2009.11.18/gnu-4.9.2</code> Adds sparskit2 2009.11.18 to your environment. <code>sparskit2/2009.11.18/intel-2015-update2</code> Adds sparskit2 2009.11.18 to your environment. <code>spectral/3.4.0/bindist</code> Adds spectral 3.4.0 to your environment. Spectral is a set of tools for performing spectral analysis on traces produced by the BSC profiling toolkit. <code>spglib/1.7.4/gnu-4.9.2</code> Adds spglib 1.7.4 to your environment. Spglib is a library for finding and handling crystal symmetries written in C. <code>squid/1.9g/gnu-4.9.2</code> Adds squid 1.9g to your environment. <code>suitesparse/4.5.5/gnu-4.9.2-serial</code> Adds suitesparse 4.5.5 to your environment. SuiteSparse is a suite of sparse matrix algorithms. <code>suitesparse/4.5.5/gnu-4.9.2-threaded</code> Adds suitesparse 4.5.5 to your environment. SuiteSparse is a suite of sparse matrix algorithms. <code>suitesparse/4.5.5/intel-2017-update1</code> Adds suitesparse 4.5.5 to your environment. SuiteSparse is a suite of sparse matrix algorithms. <code>superlu-dist/5.1.0/intel-2015-update2</code> Adds superlu-dist 5.1.0 to your environment. SuperLU_DIST is the distributed-memory parallel version of SuperLU, a general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations. <code>superlu/5.2.1/gnu-10.2.0</code> Adds superlu 5.2.1 to your environment. SuperLU is a general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations. <code>superlu/5.2.1/intel-2015-update2</code> Adds superlu 5.2.1 to your environment. SuperLU is a general purpose library for the direct solution of large, sparse, nonsymmetric systems of linear equations. <code>szip/2.1</code> Adds szip to your environment. <code>ucx/1.8.0/gnu-4.9.2</code> Adds ucx 1.8.0 to your environment. Unified Communication X (UCX) provides an optimized communication layer for Message Passing (MPI), PGAS/OpenSHMEM libraries and RPC/data-centric applications. <code>ucx/1.9.0/gnu-4.9.2</code> Adds ucx 1.9.0 to your environment. Unified Communication X (UCX) provides an optimized communication layer for Message Passing (MPI), PGAS/OpenSHMEM libraries and RPC/data-centric applications. <code>ucx/1.9.0/gnu-10.2.0</code> Adds ucx 1.9.0 to your environment. Unified Communication X (UCX) provides an optimized communication layer for Message Passing (MPI), PGAS/OpenSHMEM libraries and RPC/data-centric applications. <code>udunits/2.2.19</code> Adds udunits to your environment. <code>udunits/2.2.20/gnu-4.9.2</code> adds the UDUNITS-2 package to your environment. <code>udunits/2.2.26/gnu-4.9.2</code> adds the UDUNITS-2 package to your environment. <code>udunits/2.2.26/gnu-9.2.0</code> adds the UDUNITS-2 package to your environment. <code>udunits/2.2.28/gnu-10.2.0</code> adds the UDUNITS-2 package to your environment. <code>unixodbc/2.3.7</code> Unix ODBC driver <code>vtk/5.10.1/gnu-4.9.2</code> adds VTK 5.10.1  for GCC 4.9.2 to your environment. <code>vtk/6.2.0/gnu-4.9.2</code> adds VTK 6.2.0  for GCC 4.9.2 to your environment. <code>wavpack/5.1.0/gnu-4.9.2</code> WavPack is a completely open audio compression format providing lossless, high-quality lossy, and a unique hybrid compression mode. <code>webkitgtk/2.2.4-1</code> Adds the webkitgtk-1 with webkitgtk-devel library to your environment. 2.2.4-1 EL7 RPM binaries. <code>webkitgtk/2.4.9-1</code> Adds the webkitgtk with webkitgtk-devel library to your environment. 2.4.9-1 EL7 RPM binaries. <code>zeromq/4.1.4/gnu-4.9.2</code> Adds zeromq 4.1.4 to your environment. ZeroMQ is a distributed messaging library that supports many message-passing patterns and methods."},{"location":"Installed_Software_Lists/module-packages/#compilers","title":"Compilers","text":"<p>These modules set up your environment to be able to use specific versions of C, C++, Go, or Rust compilers.</p> Module Description <code>compilers/chapel/1.26.0</code> Chapel 1.26.0 for GCC <code>compilers/clang/8.0.0</code> The Clang project provides a language front-end and tooling infrastructure for languages in the C language family (C, C++, Objective C/C++, OpenCL, CUDA, and RenderScript) for the LLVM project. This module sets variables to make the clang compiler binary the default C and C++ compiler in your environment. <code>compilers/gnu/4.9.2</code> adds GCC 4.9.2 compilers to your environment variables <code>compilers/gnu/7.3.0</code> The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). <code>compilers/gnu/8.3.0</code> The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). <code>compilers/gnu/9.2.0</code> The GNU Compiler Collection includes front ends for C, C++, Objective-C, Fortran, Ada, and Go, as well as libraries for these languages (libstdc++,...). <code>compilers/gnu/10.2.0</code> The GNU Compiler Collection includes front ends for C, C++, Objective-C, and Fortran, as well as libraries for these languages (libstdc++,...). <code>compilers/go/1.7.3</code> Adds go 1.7.3 to your environment. Go is an open-source, compiled, statically-typed language with garbage collection, limited structural typing, memory safety features and CSP-style concurrent programming features added. <code>compilers/go/1.8</code> Adds go 1.8 to your environment. Go is an open-source, compiled, statically-typed language with garbage collection, limited structural typing, memory safety features and CSP-style concurrent programming features added. <code>compilers/go/1.12.4</code> Adds go 1.12.4 to your environment. Go is an open-source, compiled, statically-typed language with garbage collection, limited structural typing, memory safety features and CSP-style concurrent programming features added. <code>compilers/go/1.15.2</code> Go is an open-source, compiled, statically-typed language with garbage collection, limited structural typing, memory safety features and CSP-style concurrent programming features added. <code>compilers/go/1.16.3</code> Go is an open-source, compiled, statically-typed language with garbage collection, limited structural typing, memory safety features and CSP-style concurrent programming features added. <code>compilers/go/1.16.5</code> Go is an open-source, compiled, statically-typed language with garbage collection, limited structural typing, memory safety features and CSP-style concurrent programming features added. <code>compilers/go/1.20.4</code> Go is an open-source, compiled, statically-typed language with garbage collection, limited structural typing, memory safety features and CSP-style concurrent programming features added. <code>compilers/go/1.20.6</code> Go is an open-source, compiled, statically-typed language with garbage collection, limited structural typing, memory safety features and CSP-style concurrent programming features added. <code>compilers/go/1.22.0</code> Go is an open-source, compiled, statically-typed language with garbage collection, limited structural typing, memory safety features and CSP-style concurrent programming features added. <code>compilers/intel/2013.1.046</code> [intel-compilers/2013.1.046] This is the package for the Intel C, C++, and Fortran compilers. <code>compilers/intel/2015/update2</code> adds Intel compilers to your environment variables <code>compilers/intel/2016.0.109</code> [intel-compilers/2016.0.109] This is the package for the Intel C, C++, and Fortran compilers. <code>compilers/intel/2017/update1</code> [intel-compilers/2017.Update1] This is the package for the Intel C, C++, and Fortran compilers. <code>compilers/intel/2017/update3</code> [intel-compilers/2017.Update3] This is the package for the Intel C, C++, and Fortran compilers. <code>compilers/intel/2017/update4</code> [intel/2017.Update4] This is the package for the Intel C, C++, and Fortran compilers. <code>compilers/intel/2018/update3</code> [intel/2018.Update3] This is the package for the Intel C, C++, and Fortran compilers. <code>compilers/intel/2019/update4</code> [intel/2019.Update4] This is the package for the Intel C, C++, and Fortran compilers. <code>compilers/intel/2019/update5</code> [intel/2019.Update5] This is the package for the Intel C, C++, and Fortran compilers. <code>compilers/intel/2020/release</code> [intel/2020] This is the package for the Intel C, C++, and Fortran compilers. <code>compilers/intel/2022.2</code> [intel/2022.2] Intel's suite of compilers, performance libraries, frameworks, and analysis and debug tools. Contains the Intel oneAPI Base Toolkit 2022.2.0.262 and Intel oneAPI HPC Toolkit 2022.2.0.191. Intel compiler version is 2022.1.0 and corresponding Intel MPI version is 2021.6.0 <code>compilers/intel/2024.0.1</code> [intel/2024.0.1] Intel's suite of compilers, performance libraries, frameworks, and analysis and debug tools. Contains the Intel oneAPI Base Toolkit 2024.0.1.46 and Intel oneAPI HPC Toolkit 2024.0.1.38. Intel compiler version is 2024.0 and corresponding Intel MPI version is 2021.11 <code>compilers/nag/6.0.1044</code> adds NAG Fortran compiler V6.0 Build 1044 to your environment. <code>compilers/nag/6.1.6106</code> adds NAG Fortran compiler V6.1 Build 6106 to your environment. <code>compilers/nag/6.2.6214</code> adds NAG Fortran compiler V6.1 Build 6106 to your environment. <code>compilers/nag/6.2.6223</code> adds NAG Fortran compiler V6.2 Build 6223 to your environment. <code>compilers/nag/7.0.7020</code> adds NAG Fortran compiler V7.0 Build 7020 to your environment. <code>compilers/nag/7.1.7114</code> adds NAG Fortran compiler V7.1 Build 7114 to your environment. <code>compilers/nag/7.2</code> adds NAG Fortran compiler V7.2 to your environment. <code>compilers/nvidia/hpc-sdk/20.9</code> Adds Nvidia's HPC SDK to your environment.  This provides a full set of compilers, and MPI. <code>compilers/nvidia/hpc-sdk/21.3</code> Adds Nvidia's HPC SDK to your environment.  This provides a full set of compilers, and MPI. <code>compilers/nvidia/hpc-sdk/21.11</code> Adds Nvidia's HPC SDK to your environment.  This provides a full set of compilers, and MPI. <code>compilers/nvidia/hpc-sdk/22.1</code> Adds Nvidia's HPC SDK to your environment.  This provides a full set of compilers, and MPI. <code>compilers/nvidia/hpc-sdk/22.2</code> Adds Nvidia's HPC SDK to your environment.  This provides a full set of compilers, and MPI. <code>compilers/nvidia/hpc-sdk/22.3</code> Adds Nvidia's HPC SDK to your environment.  This provides a full set of compilers, and MPI. <code>compilers/nvidia/hpc-sdk/22.9</code> Adds Nvidia's HPC SDK to your environment.  This provides a full set of compilers, and MPI. <code>compilers/pgi/2012.10</code> adds Portland Group compilers to your environment. <code>compilers/pgi/2015.4</code> adds Portland Group compilers to your environment. <code>compilers/pgi/2015.7</code> adds Portland Group compilers to your environment. <code>compilers/pgi/2017.3</code> adds Portland Group compilers to your environment. <code>compilers/pgi/2018.5</code> adds Portland Group compilers to your environment. <code>compilers/pgi/2018.5-llvm</code> adds Portland Group compilers to your environment. <code>compilers/pgi/2018.10</code> The PGI Compilers are primarily installed to build Gaussian, which is not compatible with other compilers. Please do not use these over the Intel or GNU compilers unless you have a good reason for doing so. <code>compilers/pgi/2018.10-llvm</code> The PGI Compilers are primarily installed to build Gaussian, which is not compatible with other compilers. Please do not use these over the Intel or GNU compilers unless you have a good reason for doing so. <code>compilers/rust/1.18.0</code> Adds Rust 1.18.0 to your environment. <code>compilers/rust/1.46.0</code> The toolchain for the Rust programming language. <code>compilers/rust/1.58.1</code> The toolchain for the Rust programming language. <code>Miniconda</code> To make conda operate correctly, please run: <code>Miniconda</code> To make conda operate correctly, please run:"},{"location":"Installed_Software_Lists/module-packages/#development-tools","title":"Development Tools","text":"<p>This section is for modules for programs that are used in software development, profiling, or troubleshooting.</p> <p>It also contains language interpreters, like Python, Ruby, and Java.</p> Module Description <code>armforge/20.1.2</code> ArmForge is a collection of parallel profiling and debugging tools for x86_64 and ARM processors. <code>autoconf/2.69</code> Adds GNU Autoconf Version 2.69 to your environment. <code>autogen/5.18.12/gnu-4.9.2</code> AutoGen is a tool designed to simplify the creation and maintenance of programs that contain large amounts of repetitious text. <code>automake/1.16.1</code> Adds GNU Automake Version 1.16.1 to your environment. <code>bazel/0.7.0</code> Adds bazek to your environment. <code>bazel/0.14.1/gnu-4.9.2</code> Adds bazek to your environment. <code>bazel/0.21.0/gnu-4.9.2</code> Adds bazek to your environment. <code>bazel/0.24.0/gnu-4.9.2</code> Adds bazek to your environment. <code>bazel/0.24.1/gnu-4.9.2</code> Adds bazek to your environment. <code>bazel/0.26.1/gnu-4.9.2</code> Adds bazek to your environment. <code>bazel/0.27.1/gnu-4.9.2</code> Adds bazek to your environment. <code>bazel/4.2.1</code> Adds bazek to your environment. <code>binutils/2.29.1/gnu-4.9.2</code> Adds binutils 2.29.1 to your environment. The GNU binutils are a collection of tools for working with binary files and assembling and disassembling machine instructions. <code>binutils/2.36.1/gnu-10.2.0</code> Adds binutils 2.36.1 to your environment. The GNU binutils are a collection of tools for working with binary files and assembling and disassembling machine instructions. <code>bison/3.0.4/gnu-4.9.2</code> Adds Bison 3.0.4 to your environment. Bison is a general-purpose parser generator. <code>chicken/4.13.0</code> adds Chicken 4.13.0 to your environment variables <code>clojure/1.10.0.411</code> This is a module with no description string. <code>clojure/1.10.3.1069</code> This is a module with no description string. <code>cmake/3.2.1</code> adds Cmake 3.2.1 compilers to your environment variables <code>cmake/3.7.2</code> adds Cmake 3.7.2 compilers to your environment variables <code>cmake/3.13.3</code> adds Cmake 3.13.3 compilers to your environment variables <code>cmake/3.19.1</code> adds Cmake 3.19.1 to your environment variables <code>cmake/3.21.1</code> adds Cmake 3.21.1 to your environment variables <code>cmake/3.27.3</code> CMake is an open-source, cross-platform family of tools designed to build, test and package software. <code>cmdstan/2.24.1/gnu-4.9.2</code> Adds CmdStan 2.24.1 to your environment. <code>cmdstan/2.35.0/gnu-10.2.0</code> Adds CmdStan 2.35.0 to your environment. <code>cuda/7.5.18/gnu-4.9.2</code> Adds cuda 7.5.18 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. <code>cuda/8.0.61-patch2/gnu-4.9.2</code> Adds cuda 8.0.61 patch2 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. <code>cuda/9.0.176-patch4/gnu-4.9.2</code> Adds cuda 9.0.176 patch4 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. <code>cuda/10.0.130/gnu-4.9.2</code> Adds cuda 10.0.130 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. <code>cuda/10.1.243/gnu-4.9.2</code> Adds cuda 10.1.243 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. <code>cuda/10.1.243/gnu-7.3.0</code> Adds cuda 10.1.243 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. <code>cuda/11.1.1/gnu-10.2.0</code> Adds cuda 11.1.1 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. <code>cuda/11.2.0/gnu-10.2.0</code> Adds cuda 11.2.0 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. <code>cuda/11.3.1/gnu-10.2.0</code> Adds cuda 11.3.1 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. <code>cuda/11.8.0/gnu-10.2.0</code> Adds cuda 11.8.0 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. <code>cuda/12.2.2/gnu-10.2.0</code> Adds cuda 12.2.2 to your environment. This is for compiling CUDA code to run on NVIDIA GPUs. <code>ddt/6.0.4</code> This module adds DDT 6.0.4 with MIC support to your environment. <code>depot_tools/788d9e0d</code> adds depot_tools to your environment variables <code>depot_tools/c03a9cf</code> adds depot_tools to your environment variables <code>dimemas/5.3.3/bindist</code> Adds dimemas 5.3.3 to your environment. Dimemas is an abstracted network simulator for message-passing programs. <code>dotnet-sdk/7.0.203</code> Microsoft .NET SDK <code>doxygen/1.8.14</code> This is a module with no description string. <code>emacs/24.5</code> Adds Emacs 24.5 to your environment. An extensible text editor. <code>emacs/26.3</code> Adds Emacs 26.3 to your environment. An extensible text editor. <code>emacs/28.1</code> Adds Emacs 28.1 to your environment. An extensible text editor. <code>extrae/3.5.2/intel-2017</code> Adds extrae 3.5.2 to your environment. Extrae is an instrumentation framework to generate execution traces of the most used parallel runtimes. <code>f2c/2013-09-26/gnu-4.9.2</code> Adds f2c 2013-09-26 to your environment. f2c is a source-to-source translator from Fortran 77 to C. It is not standards-compliant and is not recommended for use under any circumstances. <code>flex/2.5.39</code> adds Flex 2.4.39 to your environment variables <code>git/2.3.5</code> adds Git 2.3.5 to your environment variables <code>git/2.10.2</code> adds Git 2.10.2 to your environment variables <code>git/2.19.1</code> adds Git 2.19.1 to your environment variables <code>git/2.32.0</code> adds Git 2.32.0 to your environment variables <code>git/2.41.0-lfs-3.3.0</code> Git is a free and open source distributed version control system. This installation includes git-lfs pre-configured. <code>gperf/3.0.4/gnu-4.9.2</code> Adds gperf 3.0.4 to your environment. GNU gperf is a perfect hash function generator. <code>guile/2.0.11/gnu-4.9.2</code> Adds guile 2.0.11 to your environment. <code>haskellplatform/2014.2.0.0</code> adds Haskell Platform to your environment variables <code>htop/1.0.3/gnu-4.9.2</code> Adds htop 1.0.3 to your environment. <code>htop/3.2.2</code> htop is a cross-platform interactive process viewer. <code>java/1.8.0_45</code> adds Oracle JDK 1.8.0_45 compilers to your environment variables <code>java/1.8.0_92</code> adds Oracle JDK 1.8.0_92 compilers to your environment variables <code>java/21.0.4</code> adds Oracle JDK 21.0.4 compilers to your environment variables <code>java/openjdk-8/8u212/hotspot</code> adds Oracle JDK 8 compilers to your environment variables <code>java/openjdk-8/8u212/openj9</code> adds Oracle JDK 8 compilers to your environment variables <code>java/openjdk-11/11.0.1</code> adds Oracle JDK 11.0.1 compilers to your environment variables <code>java/openjdk-11/11.0.3u7/hotspot</code> adds Oracle JDK 11.0.3 compilers to your environment variables <code>java/openjdk-11/11.0.3u7/openj9</code> adds Oracle JDK 11.0.3 compilers to your environment variables <code>java/semeru-8/8u322-b06</code> adds IBM Semeru 8u322-b06 compilers to your environment variables <code>java/semeru-11/11.0.14.1_1</code> adds IBM Semeru 11.0.14.1+1 compilers to your environment variables <code>java/semeru-17/17.0.2_8</code> adds IBM Semeru 17.0.2+8 compilers to your environment variables <code>java/temurin-8/8u322-b06</code> adds Adoptium Temurin 8u322-b06 compilers to your environment variables <code>java/temurin-11/11.0.14.1_1</code> adds Adoptium Temurin 11.0.14.1+1 compilers to your environment variables <code>java/temurin-11/11.0.14_9</code> adds Adoptium Temurin 11.0.14+9 compilers to your environment variables <code>java/temurin-17/17.0.2_8</code> adds Adoptium Temurin 17.0.2+8 compilers to your environment variables <code>julia/0.3.10</code> adds Julia 0.3.10 to your environment variables <code>julia/0.4.0</code> adds Julia 0.4.0 to your environment variables <code>julia/0.4.7</code> adds Julia 0.4.7 to your environment variables <code>julia/0.5.0</code> adds Julia 0.5.0 to your environment variables <code>julia/0.6.0</code> adds Julia 0.6.0 to your environment variables <code>julia/0.7.0</code> adds Julia 0.7.0 to your environment variables <code>julia/1.0.0</code> adds Julia 1.0.0 to your environment variables <code>julia/1.1.0</code> adds Julia 1.1.0 to your environment variables <code>julia/1.2.0</code> adds Julia 1.2.0 to your environment variables <code>julia/1.3.1</code> adds Julia 1.3.1 to your environment variables <code>julia/1.5.0</code> adds Julia 1.5.0 to your environment variables <code>julia/1.6.0</code> adds Julia 1.6.0 to your environment variables <code>julia/1.6.2</code> adds Julia 1.6.2 to your environment variables <code>julia/1.7.0</code> adds Julia 1.7.0 to your environment variables <code>julia/1.7.1</code> adds Julia 1.7.1 to your environment variables <code>julia/1.8.5</code> adds Julia 1.8.5 to your environment variables <code>julia/1.9.0</code> adds Julia 1.9.0 to your environment variables <code>julia/1.9.1</code> adds Julia 1.9.1 to your environment variables <code>julia/1.9.2</code> adds Julia 1.9.2 to your environment variables <code>julia/1.9.3</code> adds Julia 1.9.3 to your environment variables <code>julia/1.10.0</code> adds Julia 1.10.0 to your environment variables <code>julia/1.10.1</code> adds Julia 1.10.1 to your environment variables <code>julia/1.11.1</code> adds Julia 1.11.1 to your environment variables <code>libtool/2.4.6</code> Adds libtool 2.4.6 to your environment. GNU libtool is a generic library support script. Libtool hides the complexity of using shared libraries behind a consistent, portable interface. <code>libxkbcommon/1.5.0</code> Adds xkbcommon to your environment. <code>linaroforge/23.1.1</code> Linaro Forge (previously ArmForge) is a collection of parallel profiling and debugging tools for x86_64 and ARM processors. <code>ltrace/0.7.3/gnu-4.9.2</code> Adds ltrace 0.7.3 to your environment. <code>lua/5.3.1</code> This module adds the Lua 5.3.1 package to your environment. Lua is a powerful, fast, lightweight, embeddable scripting language. <code>mc/4.8.14</code> This module adds Midnight Commander 4.8.14 to your environment. <code>mono/3.12.1</code> adds Mono 3.12.1 compilers to your environment variables <code>mono/5.20.1.27/gnu-4.9.2</code> This is a module with no description string. <code>nano/2.4.2</code> Adds nano 2.4.2 to your environment. A simple text editor. <code>nano/4.9</code> The nano text editor. <code>nano/6.1</code> The nano text editor. <code>nasm/2.13.01</code> The Netwide Assembler, NASM, is an 80x86 and x86-64 assembler. <code>ncl/6.0.0</code> adds NCL 6.0.0 to your environment variables <code>ncl/6.3.0</code> adds NCL 6.3.0 to your environment variables <code>nedit/5.6-aug15</code> Adds the NEdit GUI text editor to your environment. <code>netlogo/6.1.0</code> adds NetLogo tooklit compilers to your environment variables <code>ninja/1.11.1</code> Adds Ninja Version 1.11.1 to your environment. <code>paraver/4.6.4.rc1/bindist</code> Adds paraver 4.6.4.rc1 to your environment. Paraver is a trace visualizer for post-mortem trace analysis. <code>perl/5.16.0</code> This module adds adds Perl 5.16.0 to your environment. <code>perl/5.22.0</code> This module adds adds Perl 5.22.0 to your environment. <code>perlbrew/0.73</code> This module adds the Perlbrew 0.73 package to your environment. Use Perlbrew to manage your own Perls and Perl modules <code>pigz/2.4</code> pigz is a fully functional replacement for gzip that exploits multiple processors and multiple cores when compressing data. <code>pkg-config/0.29.2</code> Adds pkg-config 0.29.2 to your environment variables. <code>pycuda/2017.1/python2</code> Adds Python2 PyCuda to your environment. MPI for Python. <code>pycuda/2017.1/python3</code> Adds Python3 PyCuda to your environment. MPI for Python. <code>pypy3/6.0.0/gnu-4.9.2</code> Pypy is a JIT-ing interpreter for the Python language. This is the version intended to be compatible with CPython 3.5. <code>python/2.7.9</code> adds Python 2.7.9 with pip and virtualenv to your environment variables <code>python/2.7.12</code> adds Python 2.7.12 with pip and virtualenv to your environment variables <code>python/3.4.3</code> adds Python 3.4.3 with pip and virtualenv to your environment variables <code>python/3.5.2</code> adds Python 3.5.2 with pip and virtualenv to your environment variables <code>python/3.6.1/gnu-4.9.2</code> Adds Python 3.6.1 with pip and virtualenv to your environment variables. <code>python/3.6.3</code> Adds Python 3.6.3 with pip and virtualenv to your environment variables. <code>python/3.7.0</code> Adds Python 3.7.0 with pip and virtualenv to your environment variables. <code>python/3.7.2</code> Adds Python 3.7.2 with pip and virtualenv to your environment variables. <code>python/3.7.4</code> Adds Python 3.7.4 with pip and virtualenv to your environment variables. <code>python/3.8.0</code> Adds Python 3.8.0 with pip and virtualenv to your environment variables. <code>python/3.8.6</code> Adds Python 3.8.6 with pip and virtualenv to your environment variables. <code>python/3.9.0</code> Adds Python 3.9.0 with pip and virtualenv to your environment variables. <code>python/3.9.1</code> Adds Python 3.9.1 with pip and virtualenv to your environment variables. <code>python/3.9.6</code> Adds Python 3.9.6 with pip and virtualenv to your environment variables. <code>python/3.9.6-gnu-10.2.0</code> Adds Python 3.9.6 compiled with GNU 10.2.0 with pip and virtualenv to your environment variables. <code>python/3.9.10</code> Adds Python 3.9.10 with pip and virtualenv to your environment variables. <code>python/3.11.3</code> Adds Python 3.11.3 with pip and virtualenv to your environment variables. <code>python/3.11.4</code> Adds Python 3.11.4 with pip and virtualenv to your environment variables. <code>python/idp3/2019/3.6.8</code> Adds Intel Distribution for Python to your environment variables. <code>python/miniconda3/4.5.11</code> Adds Miniconda 4.5.11 to your environment variables. <code>python/miniconda3/4.10.3</code> Adds Miniconda 4.10.3 to your environment variables. <code>python/miniconda3/24.3.0-0</code> Adds Miniconda 24.3.0-0 to your environment variables. <code>qt/4.8.6/gnu-4.9.2</code> Adds Qt 4.8.6 to your environment. Qt is a cross-platform development tool. <code>qt/5.4.2/gnu-4.9.2</code> Adds Qt 5.4.2 to your environment. Qt is a cross-platform development tool. <code>qt/5.12.1/gnu-4.9.2</code> Adds Qt 5.12.1 to your environment. Qt is a cross-platform development tool. <code>qt/5.15.2/gnu-7.3.0</code> Adds Qt 5.15.2 to your environment. Qt is a cross-platform development tool. <code>qwt/6.1.4/gnu-4.9.2</code> Adds Qwt 6.1.4 to your environment. <code>qwt/6.2.0/gnu-4.9.2</code> Adds Qwt 6.2.0 to your environment. <code>racket/6.8</code> Adds Racket 6.8 to your enviroment. <code>rappture/20130903</code> Adds the Rappture toolkit to your environment. <code>rstudio-ide/1.4.1717</code> RStudio is an integrated development environment (IDE) for R <code>ruby/2.2.2</code> Ruby 2.2.2 with RubyGems 2.4.8 and libffi 3.2.1 <code>ruse/1.0.1</code> A command-line utility to periodically measure the memory use of a process and its subprocesses. <code>ruse/2.0</code> Ruse is a command-line utility that periodically measures the resource use of a process and its subprocesses. <code>sbcl/1.3.19</code> Adds Steelbank Common LISP 1.3.19 to your environment. <code>sbcl/2.1.6</code> Adds Steelbank Common LISP 2.1.6 to your environment. <code>sbt/1.6.1</code> This is a module with no description string. <code>scala/2.13.8</code> This is a module with no description string. <code>scala/3.1.1</code> This is a module with no description string. <code>scalasca/2.6.1/intel-2022</code> Scalasca is a software tool that supports the performance optimization of parallel programs by measuring and analyzing their runtime behavior. <code>scons/2.3.4</code> adds scons 2.3.4 to your environment variables <code>scorep/8.4/intel-2022</code> The Score-P measurement infrastructure is a tool suite for profiling and event tracing of HPC applications. <code>shellcheck/0.8.0</code> Shellcheck is a tool for performing static analysis of shell scripts. <code>shellcheck/0.10.0-bindist</code> Shellcheck is a tool for performing static analysis of shell scripts. <code>strace/4.12</code> Adds strace 4.12 to your environment. Trace system calls and signals. <code>strace/6.9</code> strace is a diagnostic, debugging and instructional userspace utility for Linux. <code>subversion/1.8.13</code> adds Subversion 1.8.13 to your environment variables <code>subversion/1.14.1</code> adds Subversion 1.14.1 to your environment variables <code>swig/3.0.5/gnu-4.9.2</code> This module adds the SWIG 3.0.5 package to your environment. SWIG is an interface compiler that connects programs written in C and C++ with scripting languages such as Perl, Python, Ruby, and Tcl. <code>swig/3.0.7/gnu-4.9.2</code> This module adds the SWIG 3.0.7 package to your environment. SWIG is an interface compiler that connects programs written in C and C++ with scripting languages such as Perl, Python, Ruby, and Tcl. <code>tcl/8.6.8</code> This is a modulefile for Tcl/Tk 8.6.8 <code>v8/3.15</code> adds v8 to your environment variables <code>v8/5.6</code> adds v8 to your environment variables <code>valgrind/3.11.0/gnu-4.9.2</code> Adds valgrind 3.11.0 to your environment. Valgrind is a framework for building dynamic analysis tools. It includes the memgrind and cachegrind tools. <code>xbae/4.60.4</code> Adds the Xbae Matrix Widget to your environment. <code>xcb-util/0.4.0</code> Adds xcb-util packages to your environment. <code>xorg-utils/X11R7.7</code> Adds xorg-utils from X11R7.7 to your environment. Includes util-macros-1.17, makedepend-1.0.5 libXdmcp-1.1.1 and libXScrnSaver-1.2.2 and imake-1.0.7."},{"location":"Installed_Software_Lists/module-packages/#core-modules","title":"Core Modules","text":"<p>These modules refer to groups of system tools, rather than applications. They're intended to help you use the system, and some are loaded by default.</p> Module Description <code>apptainer/1.2.4-1</code> Sets up the environment and cache directories for the Apptainer container runtime. <code>gerun</code> adds gerun wrapper to your environment variables <code>lm-utils/1.0</code> adds utilities to check license manager status to your environment. <code>mrxvt/0.5.4</code> Adds Mrxvt a multi-tabbed xterm replacement to your environment. <code>ops-tools/1.0.0</code> Tools for Ops work <code>ops-tools/1.1.0</code> Tools for Ops work <code>ops-tools/2.0.0</code> Tools for Ops work <code>pipe-gifts/1.0.0</code> A tool for transferring files between users on the same node: 'pipe-give' and 'pipe-receive'. <code>pv/1.6.6</code> Pipe Viewer (pv) is a tool for monitoring the progress of data through a pipeline. <code>rcps-core/1.0.0</code> adds a core set of applications and libraries to your environment. <code>rlwrap/0.43</code> adds rlwrap 0.43 to your environment variables <code>screen/4.2.1</code> adds Screen 4.2.1 to your environment variables <code>screen/4.8.0-ucl1</code> adds Screen 4.8.0 to your environment variables <code>screen/4.9.0</code> adds Screen 4.9.0 to your environment variables <code>singularity-env/1.0.0</code> Sets up the environment and cache directories for the Singularity container runtime. <code>userscripts/1.0.0</code> Adds userscripts dir to your path. Provides jobhist among other utilities. <code>userscripts/1.1.0</code> Adds userscripts dir to your path. Provides jobhist among other utilities. <code>userscripts/1.2.0</code> Adds userscripts dir to your path. Provides jobhist among other utilities. <code>userscripts/1.3.0</code> Adds userscripts dir to your path. Provides jobhist among other utilities. <code>userscripts/1.4.0</code> Adds userscripts dir to your path. Provides jobhist among other utilities. <code>userscripts/1.5.0</code> Adds user tools dirs to your path. Provides jobhist among other utilities. <code>GATK</code> Some GATK tools require conda and associated libraries."},{"location":"Installed_Software_Lists/module-packages/#beta-modules","title":"Beta Modules","text":"<p>This section is for modules we're still trying out. They may or may not work with applications from other sections.</p> Module Description <code>namd/2.13/intel-2018-update3/testing</code> Adds NAMD 2.13 to your environment <code>namd/2.13/plumed/intel-2018-update3/testing</code> Adds NAMD 2.13 to your environment <code>rcps-core-beta/gnu-7.3.0</code> adds a core set of applications and libraries to your environment. <code>rcps-core-beta/gnu-8.3.0</code> adds a core set of applications and libraries to your environment. <code>rcps-core-beta/gnu-9.2.0</code> adds a core set of applications and libraries to your environment. <code>rcps-core-beta/gnu-10.2.0</code> adds a core set of applications and libraries to your environment. <code>sac/102.0-test/gnu-10.2.0</code> Adds SAC 102.0 to your environment. <code>test-stack/2025-02</code> Adds the test-stack/2025-02 module space to module avail. This is under development and liable to change. Please report any issues to rc-support."},{"location":"Installed_Software_Lists/module-packages/#workaround-modules","title":"Workaround Modules","text":"<p>Sometimes we'll find a problem that can't be fixed properly, but can be worked-around by doing something that can be loaded as a module. That kind of module goes in this section.</p> Module Description <code>bazel-compiler-helpers/intel-2018</code> Adds bazel compiler wrappers to your environment. <code>getcwd-autoretry</code> This module uses LD_PRELOAD to shadow the getcwd function with a version that retries on failure, and is intended to workaround a bug in the Lustre filesystem."},{"location":"Installed_Software_Lists/python-packages/","title":"Python Packages","text":"<p>We provide a collection of installed Python packages for each minor version of Python, as a bundle module. This page lists the packages for the current <code>recommended</code> Python 3 bundle.</p> <p>This can be loaded using:</p> <pre><code>module load python3/recommended\n</code></pre> <p>The version of Python 3 provided with this bundle is currently Python 3.9.10.</p> <p>Note that some packages we do not provide this way, because they have complicated non-Python dependencies. These are usually provided using the normal application modules system. This includes TensorFlow.</p> <p>The following list was last updated at 08:05:03 (+0100) on 04 Apr 2025.</p> Module Version Description <code>acora</code> 2.3 Fast multi-keyword search engine for text strings <code>appdirs</code> 1.4.4 A small Python module for determining appropriate platform-specific dirs, e.g. a \"user data dir\". <code>args</code> 0.1.0 Command Arguments for Humans. <code>ase</code> 3.22.1 Atomic Simulation Environment <code>astor</code> 0.8.1 Read/rewrite/write Python ASTs <code>astropy</code> 5.0.1 Astronomy and astrophysics core library <code>asttokens</code> 2.0.5 Annotate AST trees with source code positions <code>attrs</code> 21.4.0 Classes Without Boilerplate <code>backcall</code> 0.2.0 Specifications for callback functions passed in to an API <code>biopython</code> 1.79 Freely available tools for computational molecular biology. <code>bitarray</code> 2.3.6 efficient arrays of booleans -- C extension <code>black</code> 22.1.0 The uncompromising code formatter. <code>BoltzTraP2</code> 20.7.1 band-structure interpolator and transport coefficient calculator <code>certifi</code> 2021.10.8 Python package for providing Mozilla's CA Bundle. <code>cffi</code> 1.15.0 Foreign Function Interface for Python calling C code. <code>cftime</code> 1.5.2 Time-handling functionality from netcdf4-python <code>charset-normalizer</code> 2.0.11 The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. <code>click</code> 8.0.3 Composable command line interface toolkit <code>clint</code> 0.5.1 Python Command Line Interface Tools <code>colorama</code> 0.4.4 Cross-platform colored terminal text. <code>coloredlogs</code> 15.0.1 Colored terminal output for Python's logging module <code>colormath</code> 3.0.0 Color math and conversion library. <code>comm</code> 0.2.2 Jupyter Python Comm implementation, for usage in ipykernel, xeus-python etc. <code>commonmark</code> 0.9.1 Python parser for the CommonMark Markdown spec <code>cryptography</code> 36.0.1 cryptography is a package which provides cryptographic recipes and primitives to Python developers. <code>cutadapt</code> 3.5 trim adapters from high-throughput sequencing reads <code>cvxopt</code> 1.2.7 Convex optimization package <code>cycler</code> 0.11.0 Composable style cycles <code>Cython</code> 0.29.27 The Cython compiler for writing C extensions for the Python language. <code>cyvcf2</code> 0.30.14 fast vcf parsing with cython + htslib <code>deap</code> 1.3.1 Distributed Evolutionary Algorithms in Python <code>debugpy</code> 1.8.1 An implementation of the Debug Adapter Protocol for Python <code>decorator</code> 5.1.1 Decorators for Humans <code>distlib</code> 0.3.4 Distribution utilities <code>dnaio</code> 0.7.1 Read and write FASTA and FASTQ files efficiently <code>ecdsa</code> 0.17.0 ECDSA cryptographic signature library (pure python) <code>emcee</code> 3.1.1 The Python ensemble sampling toolkit for MCMC <code>ephem</code> 4.1.3 Compute positions of the planets and stars <code>executing</code> 0.8.2 Get the currently executing AST node of a frame, and other information <code>filelock</code> 3.4.2 A platform independent file lock. <code>fonttools</code> 4.29.1 Tools to manipulate font files <code>funcparserlib</code> 0.3.6 Recursive descent parsing library based on functional combinators <code>future</code> 0.18.2 Clean single-source support for Python 3 and 2 <code>greenlet</code> 1.1.2 Lightweight in-process concurrent programming <code>hankel</code> 1.1.0 Hankel Transformations using method of Ogata 2005 <code>humanfriendly</code> 10.0 Human friendly output for text interfaces using Python <code>hy</code> 0.20.0 Lisp and Python love each other. <code>idna</code> 3.3 Internationalized Domain Names in Applications (IDNA) <code>imageio</code> 2.15.0 Library for reading and writing a wide range of image, video, scientific, and volumetric data formats. <code>importlib-metadata</code> 4.10.1 Read metadata from Python packages <code>iniconfig</code> 1.1.1 iniconfig: brain-dead simple config-ini parsing <code>ipykernel</code> 6.29.4 IPython Kernel for Jupyter <code>ipython</code> 8.0.1 IPython: Productive Interactive Computing <code>isal</code> 0.11.1 Faster zlib and gzip compatible compression and decompression by providing python bindings for the ISA-L library. <code>jedi</code> 0.18.1 An autocompletion tool for Python that can be used for text editors. <code>Jinja2</code> 3.0.3 A very fast and expressive template engine. <code>joblib</code> 1.1.0 Lightweight pipelining with Python functions <code>jupyter_client</code> 8.6.2 Jupyter protocol implementation and client libraries <code>jupyter_core</code> 5.7.2 Jupyter core package. A base package on which Jupyter projects rely. <code>kiwisolver</code> 1.3.2 A fast implementation of the Cassowary constraint solver <code>ldap3</code> 2.9.1 A strictly RFC 4510 conforming LDAP V3 pure Python client library <code>llvmlite</code> 0.38.0 lightweight wrapper around basic LLVM functionality <code>lxml</code> 4.7.1 Powerful and Pythonic XML processing library combining libxml2/libxslt with the ElementTree API. <code>lzstring</code> 1.0.4 lz-string for python <code>Mako</code> 1.1.6 A super-fast templating language that borrows the  best ideas from the existing templating languages. <code>Markdown</code> 3.3.6 Python implementation of Markdown. <code>MarkupSafe</code> 2.0.1 Safely add untrusted strings to HTML/XML markup. <code>matplotlib</code> 3.5.1 Python plotting package <code>matplotlib-inline</code> 0.1.3 Inline Matplotlib backend for Jupyter <code>mpmath</code> 1.2.1 Python library for arbitrary-precision floating-point arithmetic <code>multiqc</code> 1.12 Create aggregate bioinformatics analysis reports across many samples and tools <code>mypy-extensions</code> 0.4.3 Experimental type system extensions for programs checked with the mypy typechecker. <code>nest-asyncio</code> 1.6.0 Patch asyncio to allow nested event loops <code>netCDF4</code> 1.5.8 Provides an object-oriented python interface to the netCDF version 4 library. <code>networkx</code> 2.6.3 Python package for creating and manipulating graphs and networks <code>nibabel</code> 3.2.2 Access a multitude of neuroimaging data formats <code>ninja</code> 1.10.2.3 Ninja is a small build system with a focus on speed <code>nose</code> 1.3.7 nose extends unittest to make testing easier <code>numba</code> 0.55.1 compiling Python code using LLVM <code>numpy</code> 1.21.5 NumPy is the fundamental package for array computing with Python. <code>obspy</code> 1.2.2 ObsPy - a Python framework for seismological observatories. <code>packaging</code> 21.3 Core utilities for Python packages <code>pandas</code> 1.4.0 Powerful data structures for data analysis, time series, and statistics <code>parso</code> 0.8.3 A Python Parser <code>pathspec</code> 0.9.0 Utility library for gitignore style pattern matching of file paths. <code>pexpect</code> 4.8.0 Pexpect allows easy control of interactive console applications. <code>pickleshare</code> 0.7.5 Tiny 'shelve'-like database with concurrency support <code>Pillow</code> 9.0.1 Python Imaging Library (Fork) <code>pip</code> 22.0.3 The PyPA recommended tool for installing Python packages. <code>platformdirs</code> 2.5.4 A small Python package for determining appropriate platform-specific dirs, e.g. a \"user data dir\". <code>pluggy</code> 1.0.0 plugin and hook calling mechanisms for python <code>Pmw</code> 2.0.1 Python Mega Widgets <code>prompt-toolkit</code> 3.0.27 Library for building powerful interactive command lines in Python <code>psutil</code> 5.9.8 Cross-platform lib for process and system monitoring in Python. <code>ptyprocess</code> 0.7.0 Run a subprocess in a pseudo terminal <code>pure-eval</code> 0.2.2 Safely evaluate AST nodes without side effects <code>py</code> 1.11.0 library with cross-python path, ini-parsing, io, code, log facilities <code>pyasn1</code> 0.4.8 ASN.1 types and codecs <code>pycparser</code> 2.21 C parser in Python <code>pyerfa</code> 2.0.0.1 Python bindings for ERFA <code>pyfastaq</code> 3.17.0 Script to manipulate FASTA and FASTQ files, plus API for developers <code>Pygments</code> 2.11.2 Pygments is a syntax highlighting package written in Python. <code>PyMySQL</code> 1.1.0 Pure Python MySQL Driver <code>pyparsing</code> 3.0.7 Python parsing module <code>pysam</code> 0.18.0 pysam <code>pyspglib</code> 1.8.3.1 This is the pyspglib module. <code>pytest</code> 7.0.0 pytest: simple powerful testing with Python <code>python-dateutil</code> 2.8.2 Extensions to the standard Python datetime module <code>python-Levenshtein</code> 0.12.2 Python extension for computing string edit distances and similarities. <code>pytz</code> 2021.3 World timezone definitions, modern and historical <code>PyWavelets</code> 1.2.0 PyWavelets, wavelet transform module <code>PyYAML</code> 6.0 YAML parser and emitter for Python <code>pyzmq</code> 26.0.3 Python bindings for 0MQ <code>qutip</code> 4.6.3 QuTiP: The Quantum Toolbox in Python <code>regex</code> 2022.1.18 Alternative regular expression module, to replace re. <code>requests</code> 2.27.1 Python HTTP for Humans. <code>rich</code> 11.2.0 Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal <code>rply</code> 0.7.8 A pure Python Lex/Yacc that works with RPython <code>scikit-image</code> 0.19.1 Image processing in Python <code>scikit-learn</code> 1.0.2 A set of python modules for machine learning and data mining <code>scipy</code> 1.8.0 SciPy: Scientific Library for Python <code>seaborn</code> 0.11.2 seaborn: statistical data visualization <code>setuptools</code> 70.0.0 Easily download, build, install, upgrade, and uninstall Python packages <code>shellingham</code> 1.5.4 Tool to Detect Surrounding Shell <code>simplejson</code> 3.17.6 Simple, fast, extensible JSON encoder/decoder for Python <code>six</code> 1.16.0 Python 2 and 3 compatibility utilities <code>spectra</code> 0.0.11 Color scales and color conversion made easy for Python. <code>spglib</code> 1.16.3 This is the spglib module. <code>SQLAlchemy</code> 1.4.31 Database Abstraction Library <code>sshpubkeys</code> 3.3.1 SSH public key parser <code>stack-data</code> 0.1.4 Extract data from python stack frames and tracebacks for informative displays <code>tabulate</code> 0.8.9 Pretty-print tabular data <code>threadpoolctl</code> 3.1.0 threadpoolctl <code>tifffile</code> 2022.2.2 Read and write TIFF files <code>tomli</code> 2.0.1 A lil' TOML parser <code>tornado</code> 6.4.1 Tornado is a Python web framework and asynchronous networking library, originally developed at FriendFeed. <code>traitlets</code> 5.14.3 Traitlets Python configuration system <code>typer</code> 0.12.3 Typer, build great CLIs. Easy to code. Based on Python type hints. <code>typing_extensions</code> 4.0.1 Backported and Experimental Type Hints for Python 3.6+ <code>urllib3</code> 1.26.8 HTTP library with thread-safe connection pooling, file post, and more. <code>venv-kernel</code> 1.0.11 Create a custom jupyter kernel for your venv. <code>virtualenv</code> 20.13.1 Virtual Python Environment builder <code>wcwidth</code> 0.2.5 Measures the displayed width of unicode strings in a terminal <code>weblogo</code> 3.7.9 WebLogo3 : Sequence Logos Redrawn <code>wheel</code> 0.37.1 A built-package format for Python <code>xlrd</code> 2.0.1 Library for developers to extract data from Microsoft Excel (tm) .xls spreadsheet files <code>XlsxWriter</code> 3.0.2 A Python module for creating Excel XLSX files. <code>xlutils</code> 2.0.0 Utilities for working with Excel files that require both xlrd and xlwt <code>xlwt</code> 1.3.0 Library to create spreadsheet files compatible with MS Excel 97/2000/XP/2003 XLS files, on any platform, with Python 2.6, 2.7, 3.3+ <code>xopen</code> 1.4.0 Open compressed files transparently <code>zipp</code> 3.7.0 Backport of pathlib-compatible object wrapper for zip files"},{"location":"Installed_Software_Lists/r-packages/","title":"R Packages","text":"<p>We provide a collection of installed R packages for each release of R, as a bundle module. This page lists the packages for the current <code>recommended</code> R bundle.</p> <p>This can be loaded using:</p> <pre><code>module load r/recommended\n</code></pre> <p>The version of R provided with this bundle is currently R version 4.2.0 (2022-04-22).</p> <p>The following list was last updated at: 08:05:06 (+0100) on 04 Apr 2025.</p> Module Version Description <code>abc</code> 2.2.1 Tools for Approximate Bayesian Computation (ABC) <code>abc.data</code> 1.0 Data Only: Tools for Approximate Bayesian Computation (ABC) <code>abind</code> 1.4-5 Combine Multidimensional Arrays <code>ade4</code> 1.7-19 Analysis of Ecological Data: Exploratory and Euclidean Methods in Environmental Sciences <code>adegenet</code> 2.1.6 Exploratory Analysis of Genetic and Genomic Data <code>ADGofTest</code> 0.3 Anderson-Darling GoF test <code>admisc</code> 0.27 Adrian Dusa's Miscellaneous <code>afex</code> 1.2-1 Analysis of Factorial Experiments <code>affxparser</code> 1.68.1 Affymetrix File Parsing SDK <code>affy</code> 1.74.0 Methods for Affymetrix Oligonucleotide Arrays <code>affydata</code> 1.44.0 Affymetrix Data for Demonstration Purpose <code>affyio</code> 1.66.0 Tools for parsing Affymetrix data files <code>affylmGUI</code> 1.70.0 GUI for limma Package with Affymetrix Microarrays <code>affyPLM</code> 1.72.0 Methods for fitting probe-level models <code>akima</code> 0.6-3.4 Interpolation of Irregularly and Regularly Spaced Data <code>annaffy</code> 1.68.0 Annotation tools for Affymetrix biological metadata <code>annmap</code> 1.38.0 Genome annotation and visualisation package pertaining to Affymetrix arrays and NGS analysis. <code>annotate</code> 1.74.0 Annotation for microarrays <code>AnnotationDbi</code> 1.58.0 Manipulation of SQLite-based annotations in Bioconductor <code>AnnotationFilter</code> 1.20.0 Facilities for Filtering Bioconductor Annotation Resources <code>AnnotationForge</code> 1.38.0 Tools for building SQLite-based annotation data packages <code>AnnotationHub</code> 3.4.0 Client to access AnnotationHub resources <code>ape</code> 5.6-2 Analyses of Phylogenetics and Evolution <code>arm</code> 1.12-2 Data Analysis Using Regression and Multilevel/Hierarchical Models <code>aroma.affymetrix</code> 3.2.0 Analysis of Large Affymetrix Microarray Data Sets <code>aroma.apd</code> 0.6.0 A Probe-Level Data File Format Used by 'aroma.affymetrix' [deprecated] <code>aroma.core</code> 3.2.2 Core Methods and Classes Used by 'aroma.*' Packages Part of the Aroma Framework <code>aroma.light</code> 3.26.0 Light-Weight Methods for Normalization and Visualization of Microarray Data using Only Basic R Data Types <code>arrangements</code> 1.1.9 Fast Generators and Iterators for Permutations, Combinations, Integer Partitions and Compositions <code>arsenal</code> 3.6.3 An Arsenal of 'R' Functions for Large-Scale Statistical Summaries <code>askpass</code> 1.1 Safe Password Entry for R, Git, and SSH <code>assertthat</code> 0.2.1 Easy Pre and Post Assertions <code>backports</code> 1.4.1 Reimplementations of Functions Introduced Since R-3.0.0 <code>bamsignals</code> 1.28.0 Extract read count signals from bam files <code>base</code> 4.2.0 The R Base Package <code>base64</code> 2.0 Base64 Encoder and Decoder <code>base64enc</code> 0.1-3 Tools for base64 encoding <code>BaSTA</code> 1.9.4 Age-Specific Survival Analysis from Incomplete Capture-Recapture/Recovery Data <code>BatchJobs</code> 1.9 Batch Computing with R <code>bayesplot</code> 1.9.0 Plotting for Bayesian Models <code>BBmisc</code> 1.12 Miscellaneous Helper Functions for B. Bischl <code>beachmat</code> 2.12.0 Compiling Bioconductor to Handle Each Matrix Type <code>beadarray</code> 2.46.0 Quality assessment and low-level analysis for Illumina BeadArray data <code>beadarrayExampleData</code> 1.34.0 Example data for the beadarray package <code>BeadDataPackR</code> 1.48.0 Compression of Illumina BeadArray data <code>beanplot</code> 1.3.1 Visualization via Beanplots (Like Boxplot/Stripchart/Violin Plot) <code>benchmarkme</code> 1.0.7 Crowd Sourced System Benchmarks <code>benchmarkmeData</code> 1.0.4 Data Set for the 'benchmarkme' Package <code>bezier</code> 1.1.2 Toolkit for Bezier Curves and Splines <code>BH</code> 1.78.0-0 Boost C++ Header Files <code>BiasedUrn</code> 1.07 Biased Urn Model Distributions <code>binom</code> 1.1-1.1 Binomial Confidence Intervals for Several Parameterizations <code>bio3d</code> 2.4-3 Biological Structure Analysis <code>Biobase</code> 2.56.0 Biobase: Base functions for Bioconductor <code>BiocFileCache</code> 2.4.0 Manage Files Across Sessions <code>BiocGenerics</code> 0.42.0 S4 generic functions used in Bioconductor <code>BiocIO</code> 1.6.0 Standard Input and Output for Bioconductor Packages <code>BiocManager</code> 1.30.18 Access the Bioconductor Project Package Repository <code>BiocParallel</code> 1.30.2 Bioconductor facilities for parallel evaluation <code>BiocVersion</code> 3.15.2 Set the appropriate version of Bioconductor packages <code>biomaRt</code> 2.52.0 Interface to BioMart databases (i.e. Ensembl) <code>Biostrings</code> 2.64.0 Efficient manipulation of biological strings <code>biovizBase</code> 1.44.0 Basic graphic utilities for visualization of genomic data. <code>bit</code> 4.0.4 Classes and Methods for Fast Memory-Efficient Boolean Selections <code>bit64</code> 4.0.5 A S3 Class for Vectors of 64bit Integers <code>bitops</code> 1.0-7 Bitwise Operations <code>blob</code> 1.2.3 A Simple S3 Class for Representing Vectors of Binary Data ('BLOBS') <code>blockmodeling</code> 1.0.5 Generalized and Classical Blockmodeling of Valued Networks <code>boot</code> 1.3-28 Bootstrap Functions (Originally by Angelo Canty for S) <code>BradleyTerry2</code> 1.1-2 Bradley-Terry Models <code>brew</code> 1.0-7 Templating Framework for Report Generation <code>brglm</code> 0.7.2 Bias Reduction in Binomial-Response Generalized Linear Models <code>bridgesampling</code> 1.1-2 Bridge Sampling for Marginal Likelihoods and Bayes Factors <code>brio</code> 1.1.3 Basic R Input Output <code>brms</code> 2.18.0 Bayesian Regression Models using 'Stan' <code>Brobdingnag</code> 1.2-9 Very Large Numbers in R <code>broom</code> 0.8.0 Convert Statistical Objects into Tidy Tibbles <code>BSgenome</code> 1.64.0 Software infrastructure for efficient representation of full genomes and their SNPs <code>BSgenome.Hsapiens.UCSC.hg19</code> 1.4.3 Full genome sequences for Homo sapiens (UCSC version hg19, based on GRCh37.p13) <code>bslib</code> 0.3.1 Custom 'Bootstrap' 'Sass' Themes for 'shiny' and 'rmarkdown' <code>bsseq</code> 1.32.0 Analyze, manage and store bisulfite sequencing data <code>bumphunter</code> 1.38.0 Bump Hunter <code>cachem</code> 1.0.6 Cache R Objects with Automatic Pruning <code>callr</code> 3.7.0 Call R from R <code>car</code> 3.0-13 Companion to Applied Regression <code>carData</code> 3.0-5 Companion to Applied Regression Data Sets <code>caret</code> 6.0-92 Classification and Regression Training <code>Category</code> 2.62.0 Category Analysis <code>caTools</code> 1.18.2 Tools: Moving Window Statistics, GIF, Base64, ROC AUC, etc <code>CDM</code> 8.1-12 Cognitive Diagnosis Modeling <code>cellranger</code> 1.1.0 Translate Spreadsheet Cell Ranges to Rows and Columns <code>ChAMP</code> 2.26.0 Chip Analysis Methylation Pipeline for Illumina HumanMethylation450 and EPIC <code>ChAMPdata</code> 2.28.0 Data Packages for ChAMP package <code>checkmate</code> 2.1.0 Fast and Versatile Argument Checks <code>class</code> 7.3-20 Functions for Classification <code>classInt</code> 0.4-3 Choose Univariate Class Intervals <code>cli</code> 3.6.0 Helpers for Developing Command Line Interfaces <code>clipr</code> 0.8.0 Read and Write from the System Clipboard <code>clue</code> 0.3-60 Cluster Ensembles <code>cluster</code> 2.1.3 \"Finding Groups in Data\": Cluster Analysis Extended Rousseeuw et al. <code>cmprsk</code> 2.2-11 Subdistribution Analysis of Competing Risks <code>coda</code> 0.19-4 Output Analysis and Diagnostics for MCMC <code>codetools</code> 0.2-18 Code Analysis Tools for R <code>colorRamps</code> 2.3.1 Builds Color Tables <code>colorspace</code> 2.0-3 A Toolbox for Manipulating and Assessing Colors and Palettes <code>colourpicker</code> 1.1.1 A Colour Picker Tool for Shiny and for Selecting Colours in Plots <code>combinat</code> 0.0-8 combinatorics utilities <code>commonmark</code> 1.8.0 High Performance CommonMark and Github Markdown Rendering in R <code>compiler</code> 4.2.0 The R Compiler Package <code>copula</code> 1.0-1 Multivariate Dependence with Copulas <code>copynumber</code> 1.36.0 Segmentation of single- and multi-track copy number data by penalized least squares regression. <code>corpcor</code> 1.6.10 Efficient Estimation of Covariance and (Partial) Correlation <code>corrplot</code> 0.92 Visualization of a Correlation Matrix <code>cowplot</code> 1.1.1 Streamlined Plot Theme and Plot Annotations for 'ggplot2' <code>cpp11</code> 0.4.2 A C++11 Interface for R's C Interface <code>crayon</code> 1.5.1 Colored Terminal Output <code>credentials</code> 1.3.2 Tools for Managing SSH and Git Credentials <code>crosstalk</code> 1.2.0 Inter-Widget Interactivity for HTML Widgets <code>crul</code> 1.2.0 HTTP Client <code>curl</code> 4.3.2 A Modern and Flexible Web Client for R <code>data.table</code> 1.14.2 Extension of <code>data.frame</code> <code>datasets</code> 4.2.0 The R Datasets Package <code>DBI</code> 1.1.2 R Database Interface <code>dbplyr</code> 2.1.1 A 'dplyr' Back End for Databases <code>DelayedArray</code> 0.22.0 A unified framework for working transparently with on-disk and in-memory array-like datasets <code>DelayedMatrixStats</code> 1.18.0 Functions that Apply to Rows and Columns of 'DelayedMatrix' Objects <code>deldir</code> 1.0-6 Delaunay Triangulation and Dirichlet (Voronoi) Tessellation <code>dendextend</code> 1.15.2 Extending 'dendrogram' Functionality in R <code>DEoptimR</code> 1.0-11 Differential Evolution Optimization in Pure R <code>Deriv</code> 4.1.3 Symbolic Differentiation <code>desc</code> 1.4.1 Manipulate DESCRIPTION Files <code>DescTools</code> 0.99.47 Tools for Descriptive Statistics <code>DESeq2</code> 1.36.0 Differential gene expression analysis based on the negative binomial distribution <code>devtools</code> 2.4.3 Tools to Make Developing R Packages Easier <code>DEXSeq</code> 1.42.0 Inference of differential exon usage in RNA-Seq <code>dichromat</code> 2.0-0.1 Color Schemes for Dichromats <code>diffobj</code> 0.3.5 Diffs for R Objects <code>digest</code> 0.6.29 Create Compact Hash Digests of R Objects <code>diptest</code> 0.76-0 Hartigan's Dip Test Statistic for Unimodality - Corrected <code>distr</code> 2.8.0 Object Oriented Implementation of Distributions <code>distrEx</code> 2.8.0 Extensions of Package 'distr' <code>distributional</code> 0.3.0 Vectorised Probability Distributions <code>DMRcate</code> 2.10.0 Methylation array and sequencing spatial analysis methods <code>DNAcopy</code> 1.70.0 DNA copy number data analysis <code>docopt</code> 0.7.1 Command-Line Interface Specification Language <code>doMC</code> 1.3.8 Foreach Parallel Adaptor for 'parallel' <code>doMPI</code> 0.2.2 Foreach Parallel Adaptor for the Rmpi Package <code>doParallel</code> 1.0.17 Foreach Parallel Adaptor for the 'parallel' Package <code>doRNG</code> 1.8.2 Generic Reproducible Parallel Backend for 'foreach' Loops <code>dotCall64</code> 1.0-1 Enhanced Foreign Function Interface Supporting Long Vectors <code>dparser</code> 1.3.1-9 Port of 'Dparser' Package <code>dplyr</code> 1.0.9 A Grammar of Data Manipulation <code>DSS</code> 2.44.0 Dispersion shrinkage for sequencing data <code>DT</code> 0.23 A Wrapper of the JavaScript Library 'DataTables' <code>dtplyr</code> 1.2.1 Data Table Back-End for 'dplyr' <code>dygraphs</code> 1.1.1.6 Interface to 'Dygraphs' Interactive Time Series Charting Library <code>dynamicTreeCut</code> 1.63-1 Methods for Detection of Clusters in Hierarchical Clustering Dendrograms <code>DynDoc</code> 1.74.0 Dynamic document tools <code>e1071</code> 1.7-9 Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien <code>easyRNASeq</code> 2.32.0 Count summarization and normalization for RNA-Seq data <code>EBSeq</code> 1.36.0 An R package for gene and isoform differential expression analysis of RNA-seq data <code>edgeR</code> 3.38.1 Empirical Analysis of Digital Gene Expression Data in R <code>effects</code> 4.2-1 Effect Displays for Linear, Generalized Linear, and Other Models <code>ellipse</code> 0.4.2 Functions for Drawing Ellipses and Ellipse-Like Confidence Regions <code>ellipsis</code> 0.3.2 Tools for Working with ... <code>ensembldb</code> 2.20.1 Utilities to create and use Ensembl-based annotation databases <code>Epi</code> 2.46 Statistical Analysis in Epidemiology <code>erer</code> 3.1 Empirical Research in Economics with R <code>estimability</code> 1.3 Tools for Assessing Estimability of Linear Predictions <code>etm</code> 1.1.1 Empirical Transition Matrix <code>evaluate</code> 0.15 Parsing and Evaluation Tools that Provide More Details than the Default <code>evd</code> 2.3-6 Functions for Extreme Value Distributions <code>Exact</code> 3.2 Unconditional Exact Test <code>ExperimentHub</code> 2.4.0 Client to access ExperimentHub resources <code>expm</code> 0.999-6 Matrix Exponential, Log, 'etc' <code>FactoMineR</code> 2.4 Multivariate Exploratory Data Analysis and Data Mining <code>fail</code> 1.3 File Abstraction Interface Layer (FAIL) <code>fansi</code> 1.0.3 ANSI Control Sequence Aware String Functions <code>farver</code> 2.1.0 High Performance Colour Space Manipulation <code>fastcluster</code> 1.2.3 Fast Hierarchical Clustering Routines for R and 'Python' <code>fastICA</code> 1.2-3 FastICA Algorithms to Perform ICA and Projection Pursuit <code>fastmap</code> 1.1.0 Fast Data Structures <code>fastmatch</code> 1.1-3 Fast 'match()' Function <code>FDb.InfiniumMethylation.hg19</code> 2.2.0 Annotation package for Illumina Infinium DNA methylation probes <code>fdrtool</code> 1.2.17 Estimation of (Local) False Discovery Rates and Higher Criticism <code>fields</code> 13.3 Tools for Spatial Data <code>filelock</code> 1.0.2 Portable File Locking <code>flashClust</code> 1.01-2 Implementation of optimal hierarchical clustering <code>flexmix</code> 2.3-17 Flexible Mixture Modeling <code>fontawesome</code> 0.2.2 Easily Work with 'Font Awesome' Icons <code>forcats</code> 0.5.1 Tools for Working with Categorical Variables (Factors) <code>foreach</code> 1.5.2 Provides Foreach Looping Construct <code>foreign</code> 0.8-82 Read Data Stored by 'Minitab', 'S', 'SAS', 'SPSS', 'Stata', 'Systat', 'Weka', 'dBase', ... <code>formatR</code> 1.12 Format R Code Automatically <code>Formula</code> 1.2-4 Extended Model Formulas <code>fpc</code> 2.2-9 Flexible Procedures for Clustering <code>fs</code> 1.5.2 Cross-Platform File System Operations Based on 'libuv' <code>futile.logger</code> 1.4.3 A Logging Utility for R <code>futile.options</code> 1.0.1 Futile Options Management <code>future</code> 1.25.0 Unified Parallel and Distributed Processing in R for Everyone <code>future.apply</code> 1.9.0 Apply Function to Elements in Parallel using Futures <code>gam</code> 1.20.1 Generalized Additive Models <code>gamlss</code> 5.4-3 Generalised Additive Models for Location Scale and Shape <code>gamlss.data</code> 6.0-2 Data for Generalised Additive Models for Location Scale and Shape <code>gamlss.dist</code> 6.0-3 Distributions for Generalized Additive Models for Location Scale and Shape <code>gamlss.mx</code> 6.0-0 Fitting Mixture Distributions with GAMLSS <code>gamlss.nl</code> 4.1-0 Fitting non linear parametric GAMLSS models <code>gargle</code> 1.2.0 Utilities for Working with Google APIs <code>gcrma</code> 2.68.0 Background Adjustment Using Sequence Information <code>gdata</code> 2.18.0.1 Various R Programming Tools for Data Manipulation <code>genefilter</code> 1.78.0 genefilter: methods for filtering genes from high-throughput experiments <code>geneLenDataBase</code> 1.32.0 Lengths of mRNA transcripts for a number of genomes <code>GeneNet</code> 1.2.16 Modeling and Inferring Gene Networks <code>geneplotter</code> 1.74.0 Graphics related functions for Bioconductor <code>generics</code> 0.1.2 Common S3 Generics not Provided by Base R Methods Related to Model Fitting <code>genetics</code> 1.3.8.1.3 Population Genetics <code>GenomeInfoDb</code> 1.32.2 Utilities for manipulating chromosome names, including modifying them to follow a particular naming style <code>GenomeInfoDbData</code> 1.2.8 Species and taxonomy ID look up tables used by GenomeInfoDb <code>genomeIntervals</code> 1.52.0 Operations on genomic intervals <code>GenomicAlignments</code> 1.32.0 Representation and manipulation of short genomic alignments <code>GenomicFeatures</code> 1.48.1 Conveniently import and query gene models <code>GenomicRanges</code> 1.48.0 Representation and manipulation of genomic intervals <code>geojson</code> 0.3.4 Classes for 'GeoJSON' <code>geojsonio</code> 0.9.4 Convert Data from and to 'GeoJSON' or 'TopoJSON' <code>geojsonsf</code> 2.0.2 GeoJSON to Simple Feature Converter <code>geometries</code> 0.2.0 Convert Between R Objects and Geometric Structures <code>GEOquery</code> 2.64.2 Get data from NCBI Gene Expression Omnibus (GEO) <code>gert</code> 1.6.0 Simple Git Client for R <code>getopt</code> 1.20.3 C-Like 'getopt' Behavior <code>GGally</code> 2.1.2 Extension to 'ggplot2' <code>ggforce</code> 0.3.3 Accelerating 'ggplot2' <code>ggplot2</code> 3.3.6 Create Elegant Data Visualisations Using the Grammar of Graphics <code>ggrepel</code> 0.9.1 Automatically Position Non-Overlapping Text Labels with 'ggplot2' <code>ggridges</code> 0.5.3 Ridgeline Plots in 'ggplot2' <code>ggtext</code> 0.1.2 Improved Text Rendering Support for 'ggplot2' <code>gh</code> 1.3.0 'GitHub' 'API' <code>gitcreds</code> 0.1.1 Query 'git' Credentials from 'R' <code>GJRM</code> 0.2-6 Generalised Joint Regression Modelling <code>gld</code> 2.6.6 Estimation and Use of the Generalised (Tukey) Lambda Distribution <code>glmnet</code> 4.1-4 Lasso and Elastic-Net Regularized Generalized Linear Models <code>globals</code> 0.15.0 Identify Global Objects in R Expressions <code>globaltest</code> 5.50.0 Testing Groups of Covariates/Features for Association with a Response Variable, with Applications to Gene Set Testing <code>glue</code> 1.6.2 Interpreted String Literals <code>gmp</code> 0.6-5 Multiple Precision Arithmetic <code>GO.db</code> 3.15.0 A set of annotation maps describing the entire Gene Ontology <code>goftest</code> 1.2-3 Classical Goodness-of-Fit Tests for Univariate Distributions <code>googledrive</code> 2.0.0 An Interface to Google Drive <code>googlesheets4</code> 1.0.0 Access Google Sheets using the Sheets API V4 <code>goseq</code> 1.48.0 Gene Ontology analyser for RNA-seq and other length biased data <code>GOstats</code> 2.62.0 Tools for manipulating GO and microarrays <code>gower</code> 1.0.0 Gower's Distance <code>gplots</code> 3.1.3 Various R Programming Tools for Plotting Data <code>graph</code> 1.74.0 graph: A package to handle graph data structures <code>graphics</code> 4.2.0 The R Graphics Package <code>grDevices</code> 4.2.0 The R Graphics Devices and Support for Colours and Fonts <code>grid</code> 4.2.0 The Grid Graphics Package <code>gridExtra</code> 2.3 Miscellaneous Functions for \"Grid\" Graphics <code>gridtext</code> 0.1.5 Improved Text Rendering Support for 'Grid' Graphics <code>GSEABase</code> 1.58.0 Gene set enrichment data structures and methods <code>gsl</code> 2.1-7.1 Wrapper for the Gnu Scientific Library <code>gsmoothr</code> 0.1.7 Smoothing tools <code>gtable</code> 0.3.0 Arrange 'Grobs' in Tables <code>gtools</code> 3.9.2.1 Various R Programming Tools <code>Gviz</code> 1.40.1 Plotting data and annotation information along genomic coordinates <code>HAC</code> 1.1-0 Estimation, Simulation and Visualization of Hierarchical Archimedean Copulae (HAC) <code>haplo.stats</code> 1.8.7 Statistical Analysis of Haplotypes with Traits and Covariates when Linkage Phase is Ambiguous <code>hardhat</code> 0.2.0 Construct Modeling Packages <code>haven</code> 2.5.0 Import and Export 'SPSS', 'Stata' and 'SAS' Files <code>HDF5Array</code> 1.24.0 HDF5 backend for DelayedArray objects <code>hexbin</code> 1.28.2 Hexagonal Binning Routines <code>hgu95av2.db</code> 3.13.0 Affymetrix Affymetrix HG_U95Av2 Array annotation data (chip hgu95av2) <code>HI</code> 0.5 Simulation from Distributions Supported by Nested Hyperplanes <code>highr</code> 0.9 Syntax Highlighting for R Source Code <code>HistData</code> 0.8-7 Data Sets from the History of Statistics and Data Visualization <code>Hmisc</code> 4.7-0 Harrell Miscellaneous <code>hms</code> 1.1.1 Pretty Time of Day <code>htmlTable</code> 2.4.0 Advanced Tables for Markdown/HTML <code>htmltools</code> 0.5.2 Tools for HTML <code>htmlwidgets</code> 1.5.4 HTML Widgets for R <code>httpcode</code> 0.3.0 'HTTP' Status Code Helper <code>httpuv</code> 1.6.5 HTTP and WebSocket Server Library <code>httr</code> 1.4.3 Tools for Working with URLs and HTTP <code>hugene10stprobeset.db</code> 8.8.0 Affymetrix hugene10 annotation data (chip hugene10stprobeset) <code>hugene10sttranscriptcluster.db</code> 8.8.0 Affymetrix hugene10 annotation data (chip hugene10sttranscriptcluster) <code>hwriter</code> 1.3.2.1 HTML Writer - Outputs R Objects in HTML Format <code>ids</code> 1.0.1 Generate Random Identifiers <code>igraph</code> 1.3.1 Network Analysis and Visualization <code>Illumina450ProbeVariants.db</code> 1.32.0 Annotation Package combining variant data from 1000 Genomes Project for Illumina HumanMethylation450 Bead Chip probes <code>IlluminaHumanMethylation450kanno.ilmn12.hg19</code> 0.6.1 Annotation for Illumina's 450k methylation arrays <code>IlluminaHumanMethylation450kmanifest</code> 0.4.0 Annotation for Illumina's 450k methylation arrays <code>IlluminaHumanMethylationEPICanno.ilm10b4.hg19</code> 0.6.0 Annotation for Illumina's EPIC methylation arrays <code>IlluminaHumanMethylationEPICmanifest</code> 0.3.0 Manifest for Illumina's EPIC methylation arrays <code>illuminaHumanv4.db</code> 1.26.0 Illumina HumanHT12v4 annotation data (chip illuminaHumanv4) <code>illuminaio</code> 0.38.0 Parsing Illumina Microarray Output Files <code>impute</code> 1.70.0 impute: Imputation for microarray data <code>ini</code> 0.3.1 Read and Write '.ini' Files <code>INLA</code> 22.05.18-2 Full Bayesian Analysis of Latent Gaussian Models using Integrated Nested Laplace Approximations <code>inline</code> 0.3.19 Functions to Inline C, C++, Fortran Function Calls from R <code>insight</code> 0.17.1 Easy Access to Model Information for Various Model Objects <code>interactiveDisplayBase</code> 1.34.0 Base package for enabling powerful shiny web displays of Bioconductor objects <code>intervals</code> 0.15.2 Tools for Working with Points and Intervals <code>ipred</code> 0.9-12 Improved Predictors <code>IRanges</code> 2.30.0 Foundation of integer range manipulation in Bioconductor <code>ismev</code> 1.42 An Introduction to Statistical Modeling of Extreme Values <code>isoband</code> 0.2.5 Generate Isolines and Isobands from Regularly Spaced Elevation Grids <code>ISOcodes</code> 2022.01.10 Selected ISO Codes <code>isva</code> 1.9 Independent Surrogate Variable Analysis <code>iterators</code> 1.0.14 Provides Iterator Construct <code>iterpc</code> 0.4.2 Efficient Iterator for Permutations and Combinations <code>JADE</code> 2.0-3 Blind Source Separation Methods Based on Joint Diagonalization and Some BSS Performance Criteria <code>jpeg</code> 0.1-9 Read and write JPEG images <code>jqr</code> 1.2.3 Client for 'jq', a 'JSON' Processor <code>jquerylib</code> 0.1.4 Obtain 'jQuery' as an HTML Dependency Object <code>jsonify</code> 1.2.1 Convert Between 'R' Objects and Javascript Object Notation (JSON) <code>jsonlite</code> 1.8.0 A Simple and Robust JSON Parser and Generator for R <code>karyoploteR</code> 1.22.0 Plot customizable linear genomes displaying arbitrary data <code>KEGGREST</code> 1.36.0 Client-side REST access to the Kyoto Encyclopedia of Genes and Genomes (KEGG) <code>kernlab</code> 0.9-30 Kernel-Based Machine Learning Lab <code>KernSmooth</code> 2.23-20 Functions for Kernel Smoothing Supporting Wand &amp; Jones (1995) <code>knitr</code> 1.39 A General-Purpose Package for Dynamic Report Generation in R <code>kohonen</code> 3.0.11 Supervised and Unsupervised Self-Organising Maps <code>kpmt</code> 0.1.0 Known Population Median Test <code>labeling</code> 0.4.2 Axis Labeling <code>lambda.r</code> 1.2.4 Modeling Data with Functional Programming <code>later</code> 1.3.0 Utilities for Scheduling Functions to Execute Later with Event Loops <code>lattice</code> 0.20-45 Trellis Graphics for R <code>latticeExtra</code> 0.6-29 Extra Graphical Utilities Based on Lattice <code>lava</code> 1.6.10 Latent Variable Models <code>lazyeval</code> 0.2.2 Lazy (Non-Standard) Evaluation <code>lbfgsb3c</code> 2020-3.2 Limited Memory BFGS Minimizer with Bounds on Parameters with optim() 'C' Interface <code>leafem</code> 0.2.0 'leaflet' Extensions for 'mapview' <code>leaflet</code> 2.1.1 Create Interactive Web Maps with the JavaScript 'Leaflet' Library <code>leaflet.providers</code> 1.9.0 Leaflet Providers <code>leafsync</code> 0.1.0 Small Multiples for Leaflet Web Maps <code>leaps</code> 3.1 Regression Subset Selection <code>learnr</code> 0.10.1 Interactive Tutorials for R <code>lifecycle</code> 1.0.1 Manage the Life Cycle of your Package Functions <code>limma</code> 3.52.1 Linear Models for Microarray Data <code>listenv</code> 0.8.0 Environments Behaving (Almost) as Lists <code>lme4</code> 1.1-29 Linear Mixed-Effects Models using 'Eigen' and S4 <code>lmerTest</code> 3.1-3 Tests in Linear Mixed Effects Models <code>lmom</code> 2.9 L-Moments <code>lmtest</code> 0.9-40 Testing Linear Regression Models <code>locfit</code> 1.5-9.5 Local Regression, Likelihood and Density Estimation <code>longitudinal</code> 1.1.13 Analysis of Multiple Time Course Data <code>loo</code> 2.5.1 Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models <code>lotri</code> 0.4.2 A Simple Way to Specify Symmetric, Block Diagonal Matrices <code>lpSolve</code> 5.6.15 Interface to 'Lp_solve' v. 5.5 to Solve Linear/Integer Programs <code>LSD</code> 4.1-0 Lots of Superior Depictions <code>ltm</code> 1.2-0 Latent Trait Models under IRT <code>lubridate</code> 1.8.0 Make Dealing with Dates a Little Easier <code>lumi</code> 2.48.0 BeadArray Specific Methods for Illumina Methylation and Expression Microarrays <code>lwgeom</code> 0.2-8 Bindings to Selected 'liblwgeom' Functions for Simple Features <code>made4</code> 1.70.0 Multivariate analysis of microarray data using ADE4 <code>magic</code> 1.6-0 Create and Investigate Magic Squares <code>magrittr</code> 2.0.3 A Forward-Pipe Operator for R <code>maps</code> 3.4.0 Draw Geographical Maps <code>maptools</code> 1.1-4 Tools for Handling Spatial Objects <code>maptree</code> 1.4-8 Mapping, Pruning, and Graphing Tree Models <code>markdown</code> 1.1 Render Markdown with the C Library 'Sundown' <code>marray</code> 1.74.0 Exploratory analysis for two-color spotted microarray data <code>MASS</code> 7.3-56 Support Functions and Datasets for Venables and Ripley's MASS <code>mathjaxr</code> 1.6-0 Using 'Mathjax' in Rd Files <code>Matrix</code> 1.5-3 Sparse and Dense Matrix Classes and Methods <code>MatrixGenerics</code> 1.8.0 S4 Generic Summary Statistic Functions that Operate on Matrix-Like Objects <code>MatrixModels</code> 0.5-0 Modelling with Sparse and Dense Matrices <code>matrixStats</code> 0.62.0 Functions that Apply to Rows and Columns of Matrices (and to Vectors) <code>mclust</code> 5.4.10 Gaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation <code>mcmc</code> 0.9-7 Markov Chain Monte Carlo <code>memoise</code> 2.0.1 'Memoisation' of Functions <code>MendelianRandomization</code> 0.6.0 Mendelian Randomization Package <code>metadat</code> 1.2-0 Meta-Analysis Datasets <code>metafor</code> 3.4-0 Meta-Analysis Package for R <code>methods</code> 4.2.0 Formal Methods and Classes <code>methylumi</code> 2.42.0 Handle Illumina methylation data <code>mgcv</code> 1.8-40 Mixed GAM Computation Vehicle with Automatic Smoothness Estimation <code>mime</code> 0.12 Map Filenames to MIME Types <code>minfi</code> 1.42.0 Analyze Illumina Infinium DNA methylation arrays <code>miniUI</code> 0.1.1.1 Shiny UI Widgets for Small Screens <code>minpack.lm</code> 1.2-2 R Interface to the Levenberg-Marquardt Nonlinear Least-Squares Algorithm Found in MINPACK, Plus Support for Bounds <code>minqa</code> 1.2.4 Derivative-free optimization algorithms by quadratic approximation <code>missMethyl</code> 1.30.0 Analysing Illumina HumanMethylation BeadChip Data <code>mitools</code> 2.4 Tools for Multiple Imputation of Missing Data <code>mlr</code> 2.19.0 Machine Learning in R <code>mnormt</code> 2.0.2 The Multivariate Normal and t Distributions, and Their Truncated Versions <code>ModelMetrics</code> 1.2.2.2 Rapid Calculation of Model Metrics <code>modelr</code> 0.1.8 Modelling Functions that Work with the Pipe <code>modeltools</code> 0.2-23 Tools and Classes for Statistical Models <code>msm</code> 1.6.9 Multi-State Markov and Hidden Markov Models in Continuous Time <code>mstate</code> 0.3.2 Data Preparation, Estimation and Prediction in Multi-State Models <code>multcomp</code> 1.4-19 Simultaneous Inference in General Parametric Models <code>multtest</code> 2.52.0 Resampling-based multiple hypothesis testing <code>munsell</code> 0.5.0 Utilities for Using Munsell Colours <code>mvtnorm</code> 1.1-3 Multivariate Normal and t Distributions <code>n1qn1</code> 6.0.1-11 Port of the 'Scilab' 'n1qn1' Module for Unconstrained BFGS Optimization <code>nleqslv</code> 3.3.2 Solve Systems of Nonlinear Equations <code>nlme</code> 3.1-157 Linear and Nonlinear Mixed Effects Models <code>nlmixr2</code> 2.0.8 Nonlinear Mixed Effects Models in Population PK/PD <code>nlmixr2data</code> 2.0.7 Nonlinear Mixed Effects Models in Population PK/PD, Data <code>nlmixr2est</code> 2.1.3 Nonlinear Mixed Effects Models in Population PK/PD, Estimation Routines <code>nlmixr2extra</code> 2.0.8 Nonlinear Mixed Effects Models in Population PK/PD, Extra Support Functions <code>nlmixr2plot</code> 2.0.7 Nonlinear Mixed Effects Models in Population PK/PD, Plot Functions <code>nloptr</code> 2.0.2 R Interface to NLopt <code>NLP</code> 0.2-1 Natural Language Processing Infrastructure <code>nnet</code> 7.3-17 Feed-Forward Neural Networks and Multinomial Log-Linear Models <code>nor1mix</code> 1.3-0 Normal aka Gaussian (1-d) Mixture Models (S3 Classes and Methods) <code>npde</code> 3.2 Normalised Prediction Distribution Errors for Nonlinear Mixed-Effect Models <code>numDeriv</code> 2016.8-1.1 Accurate Numerical Derivatives <code>OPE</code> 0.7 Outer-product emulator <code>openssl</code> 2.0.2 Toolkit for Encryption, Signatures and Certificates Based on OpenSSL <code>optextras</code> 2019-12.4 Tools to Support Optimization Possibly with Bounds and Masks <code>optparse</code> 1.7.1 Command Line Option Parser <code>org.Hs.eg.db</code> 3.15.0 Genome wide annotation for Human <code>pander</code> 0.6.5 An R 'Pandoc' Writer <code>parallel</code> 4.2.0 Support for Parallel computation in R <code>parallelly</code> 1.31.1 Enhancing the 'parallel' Package <code>parallelMap</code> 1.5.1 Unified Interface to Parallelization Back-Ends <code>ParamHelpers</code> 1.14 Helpers for Parameters in Black-Box Optimization, Tuning and Machine Learning <code>paran</code> 1.5.2 Horn's Test of Principal Components/Factors <code>pbapply</code> 1.5-0 Adding Progress Bar to '*apply' Functions <code>pbkrtest</code> 0.5.1 Parametric Bootstrap, Kenward-Roger and Satterthwaite Based Methods for Test in Mixed Models <code>pcaPP</code> 2.0-1 Robust PCA by Projection Pursuit <code>pegas</code> 1.1 Population and Evolutionary Genetics Analysis System <code>permute</code> 0.9-7 Functions for Generating Restricted Permutations of Data <code>phangorn</code> 2.8.1 Phylogenetic Reconstruction and Analysis <code>phia</code> 0.2-1 Post-Hoc Interaction Analysis <code>pillar</code> 1.7.0 Coloured Formatting for Columns <code>pixmap</code> 0.4-12 Bitmap Images / Pixel Maps <code>pkgbuild</code> 1.3.1 Find Tools Needed to Build R Packages <code>pkgconfig</code> 2.0.3 Private Configuration for 'R' Packages <code>pkgload</code> 1.2.4 Simulate Package Installation and Attach <code>pkgmaker</code> 0.32.2 Development Utilities for R Packages <code>plinkQC</code> 0.3.4 Genotype Quality Control with 'PLINK' <code>plogr</code> 0.2.0 The 'plog' C++ Logging Library <code>plotly</code> 4.10.0 Create Interactive Web Graphics via 'plotly.js' <code>pls</code> 2.8-0 Partial Least Squares and Principal Component Regression <code>plyr</code> 1.8.7 Tools for Splitting, Applying and Combining Data <code>png</code> 0.1-7 Read and write PNG images <code>poLCA</code> 1.6.0.1 Polytomous Variable Latent Class Analysis <code>polspline</code> 1.1.20 Polynomial Spline Routines <code>polyclip</code> 1.10-0 Polygon Clipping <code>polycor</code> 0.8-1 Polychoric and Polyserial Correlations <code>posterior</code> 1.2.1 Tools for Working with Posterior Distributions <code>poweRlaw</code> 0.70.6 Analysis of Heavy Tailed Distributions <code>prabclus</code> 2.3-2 Functions for Clustering and Testing of Presence-Absence, Abundance and Multilocus Genetic Data <code>pracma</code> 2.3.8 Practical Numerical Math Functions <code>praise</code> 1.0.0 Praise Users <code>PreciseSums</code> 0.5 Accurate Floating Point Sums and Products <code>preprocessCore</code> 1.58.0 A collection of pre-processing functions <code>prettydoc</code> 0.4.1 Creating Pretty Documents from R Markdown <code>prettyunits</code> 1.1.1 Pretty, Human Readable Formatting of Quantities <code>pROC</code> 1.18.0 Display and Analyze ROC Curves <code>processx</code> 3.5.3 Execute and Control System Processes <code>prodlim</code> 2019.11.13 Product-Limit Estimation for Censored Event History Analysis <code>profileModel</code> 0.6.1 Profiling Inference Functions for Various Model Classes <code>progress</code> 1.2.2 Terminal Progress Bars <code>progressr</code> 0.10.0 An Inclusive, Unifying API for Progress Updates <code>promises</code> 1.2.0.1 Abstractions for Promise-Based Asynchronous Programming <code>ProtGenerics</code> 1.28.0 Generic infrastructure for Bioconductor mass spectrometry packages <code>proto</code> 1.0.0 Prototype Object-Based Programming <code>protolite</code> 2.1.1 Highly Optimized Protocol Buffer Serializers <code>proxy</code> 0.4-26 Distance and Similarity Measures <code>ps</code> 1.7.0 List, Query, Manipulate System Processes <code>PSCBS</code> 0.66.0 Analysis of Parent-Specific DNA Copy Numbers <code>pspline</code> 1.0-19 Penalized Smoothing Splines <code>psych</code> 2.2.5 Procedures for Psychological, Psychometric, and Personality Research <code>purrr</code> 0.3.4 Functional Programming Tools <code>qs</code> 0.25.4 Quick Serialization of R Objects <code>quadprog</code> 1.5-8 Functions to Solve Quadratic Programming Problems <code>quanteda</code> 3.2.1 Quantitative Analysis of Textual Data <code>quanteda.corpora</code> 0.9.2 A collection of corpora for quanteda <code>quantmod</code> 0.4.20 Quantitative Financial Modelling Framework <code>quantreg</code> 5.93 Quantile Regression <code>qvalue</code> 2.28.0 Q-value estimation for false discovery rate control <code>qvcalc</code> 1.0.2 Quasi Variances for Factor Effects in Statistical Models <code>R.cache</code> 0.15.0 Fast and Light-Weight Caching (Memoization) of Objects and Results to Speed Up Computations <code>R.devices</code> 2.17.0 Unified Handling of Graphics Devices <code>R.filesets</code> 2.14.0 Easy Handling of and Access to Files Organized in Structured Directories <code>R.huge</code> 0.9.0 Methods for Accessing Huge Amounts of Data [deprecated] <code>R.methodsS3</code> 1.8.1 S3 Methods Simplified <code>R.oo</code> 1.24.0 R Object-Oriented Programming with or without References <code>R.rsp</code> 0.44.0 Dynamic Generation of Scientific Reports <code>R.utils</code> 2.11.0 Various Programming Utilities <code>R2HTML</code> 2.3.3 HTML Exportation for R Objects <code>R2WinBUGS</code> 2.1-21 Running 'WinBUGS' and 'OpenBUGS' from 'R' / 'S-PLUS' <code>R6</code> 2.5.1 Encapsulated Classes with Reference Semantics <code>randomForest</code> 4.7-1.1 Breiman and Cutler's Random Forests for Classification and Regression <code>rapidjsonr</code> 1.2.0 'Rapidjson' C++ Header Files <code>RApiSerialize</code> 0.1.2 R API Serialization <code>rappdirs</code> 0.3.3 Application Directories: Determine Where to Save Data, Caches, and Logs <code>raster</code> 3.5-15 Geographic Data Analysis and Modeling <code>RBGL</code> 1.72.0 An interface to the BOOST graph library <code>rcmdcheck</code> 1.4.0 Run 'R CMD check' from 'R' and Capture Results <code>RColorBrewer</code> 1.1-3 ColorBrewer Palettes <code>Rcpp</code> 1.0.9 Seamless R and C++ Integration <code>RcppArmadillo</code> 0.11.4.2.1 'Rcpp' Integration for the 'Armadillo' Templated Linear Algebra Library <code>RcppEigen</code> 0.3.3.9.2 'Rcpp' Integration for the 'Eigen' Templated Linear Algebra Library <code>RcppGSL</code> 0.3.11 'Rcpp' Integration for 'GNU GSL' Vectors and Matrices <code>RcppParallel</code> 5.1.5 Parallel Programming Tools for 'Rcpp' <code>RcppZiggurat</code> 0.1.6 'Rcpp' Integration of Different \"Ziggurat\" Normal RNG Implementations <code>RCurl</code> 1.98-1.6 General Network (HTTP/FTP/...) Client Interface for R <code>readr</code> 2.1.2 Read Rectangular Text Data <code>readxl</code> 1.4.0 Read Excel Files <code>recipes</code> 0.2.0 Preprocessing and Feature Engineering Steps for Modeling <code>regioneR</code> 1.28.0 Association analysis of genomic regions based on permutation tests <code>registry</code> 0.5-1 Infrastructure for R Package Registries <code>relaimpo</code> 2.2-6 Relative Importance of Regressors in Linear Models <code>rematch</code> 1.0.1 Match Regular Expressions with a Nicer 'API' <code>rematch2</code> 2.1.2 Tidy Output from Regular Expression Matching <code>remotes</code> 2.4.2 R Package Installation from Remote Repositories, Including 'GitHub' <code>renv</code> 0.15.4 Project Environments <code>Repitools</code> 1.42.0 Epigenomic tools <code>reprex</code> 2.0.1 Prepare Reproducible Example Code via the Clipboard <code>reshape</code> 0.8.9 Flexibly Reshape Data <code>reshape2</code> 1.4.4 Flexibly Reshape Data: A Reboot of the Reshape Package <code>restfulr</code> 0.0.13 R Interface to RESTful Web Services <code>rex</code> 1.2.1 Friendly Regular Expressions <code>Rfast</code> 2.0.6 A Collection of Efficient and Extremely Fast R Functions <code>rgdal</code> 1.5-32 Bindings for the 'Geospatial' Data Abstraction Library <code>rgeos</code> 0.5-9 Interface to Geometry Engine - Open Source ('GEOS') <code>rgl</code> 0.108.3.2 3D Visualization Using OpenGL <code>Rgraphviz</code> 2.40.0 Provides plotting capabilities for R graph objects <code>rhdf5</code> 2.40.0 R Interface to HDF5 <code>rhdf5filters</code> 1.8.0 HDF5 Compression Filters <code>Rhdf5lib</code> 1.18.2 hdf5 library as an R package <code>Rhtslib</code> 1.28.0 HTSlib high-throughput sequencing library as an R package <code>Ringo</code> 1.60.0 R Investigation of ChIP-chip Oligoarrays <code>rjson</code> 0.2.21 JSON for R <code>RJSONIO</code> 1.3-1.6 Serialize R Objects to JSON, JavaScript Object Notation <code>rlang</code> 1.0.2 Functions for Base Types and Core R and 'Tidyverse' Features <code>rlecuyer</code> 0.3-5 R Interface to RNG with Multiple Streams <code>rlist</code> 0.4.6.2 A Toolbox for Non-Tabular Data Manipulation <code>rmarkdown</code> 2.14 Dynamic Documents for R <code>Rmpfr</code> 0.8-7 R MPFR - Multiple Precision Floating-Point Reliable <code>Rmpi</code> 0.6-9.2 Interface (Wrapper) to MPI (Message-Passing Interface) <code>rms</code> 6.3-0 Regression Modeling Strategies <code>RMySQL</code> 0.10.23 Database Interface and 'MySQL' Driver for R <code>RNetCDF</code> 2.5-2 Interface to 'NetCDF' Datasets <code>rngtools</code> 1.5.2 Utility Functions for Working with Random Number Generators <code>robustbase</code> 0.95-0 Basic Robust Statistics <code>ROC</code> 1.72.0 utilities for ROC, with microarray focus <code>rootSolve</code> 1.8.2.3 Nonlinear Root Finding, Equilibrium and Steady-State Analysis of Ordinary Differential Equations <code>roxygen2</code> 7.2.0 In-Line Documentation for R <code>rpart</code> 4.1.16 Recursive Partitioning and Regression Trees <code>RPMM</code> 1.25 Recursively Partitioned Mixture Model <code>rprojroot</code> 2.0.3 Finding Files in Project Subdirectories <code>Rsamtools</code> 2.12.0 Binary alignment (BAM), FASTA, variant call (BCF), and tabix file import <code>Rsolnp</code> 1.16 General Non-Linear Optimization <code>RSQLite</code> 2.2.14 SQLite Interface for R <code>rstan</code> 2.21.5 R Interface to Stan <code>rstanarm</code> 2.21.3 Bayesian Applied Regression Modeling via Stan <code>rstantools</code> 2.2.0 Tools for Developing R Packages Interfacing with 'Stan' <code>rstudioapi</code> 0.13 Safely Access the RStudio API <code>Rsubread</code> 2.10.1 Mapping, quantification and variant analysis of sequencing data <code>rtracklayer</code> 1.56.0 R interface to genome annotation files and the UCSC genome browser <code>ruv</code> 0.9.7.1 Detect and Remove Unwanted Variation using Negative Controls <code>rversions</code> 2.1.1 Query 'R' Versions, Including 'r-release' and 'r-oldrel' <code>rvest</code> 1.0.2 Easily Harvest (Scrape) Web Pages <code>Rvmmin</code> 2018-4.17.1 Variable Metric Nonlinear Function Minimization <code>rxode2</code> 2.0.11 Facilities for Simulating from ODE-Based Models <code>rxode2et</code> 2.0.9 Event Table Functions for 'rxode2' <code>rxode2ll</code> 2.0.9 Log-Likelihood Functions for 'rxode2' <code>rxode2parse</code> 2.0.13 Parsing and Code Generation Functions for 'rxode2' <code>rxode2random</code> 2.0.9 Random Number Generation Functions for 'rxode2' <code>s2</code> 1.0.7 Spherical Geometry Operators Using the S2 Geometry Library <code>S4Vectors</code> 0.34.0 Foundation of vector-like and list-like containers in Bioconductor <code>saemix</code> 3.0 Stochastic Approximation Expectation Maximization (SAEM) Algorithm <code>sampling</code> 2.9 Survey Sampling <code>sandwich</code> 3.0-1 Robust Covariance Matrix Estimators <code>sass</code> 0.4.1 Syntactically Awesome Style Sheets ('Sass') <code>scales</code> 1.2.0 Scale Functions for Visualization <code>scam</code> 1.2-12 Shape Constrained Additive Models <code>scatterplot3d</code> 0.3-41 3D Scatter Plot <code>schoolmath</code> 0.4.1 Functions and Datasets for Math Used in School <code>scrime</code> 1.3.5 Analysis of High-Dimensional Categorical Data Such as SNP Data <code>segmented</code> 1.5-0 Regression Models with Break-Points / Change-Points Estimation <code>selectr</code> 0.4-2 Translate CSS Selectors to XPath Expressions <code>sendmailR</code> 1.2-1 send email using R <code>seqinr</code> 4.2-16 Biological Sequences Retrieval and Analysis <code>seqLogo</code> 1.62.0 Sequence logos for DNA sequence alignments <code>sessioninfo</code> 1.2.2 R Session Information <code>sf</code> 1.0-7 Simple Features for R <code>sfheaders</code> 0.4.0 Converts Between R Objects and Simple Feature Objects <code>sfsmisc</code> 1.1-13 Utilities from 'Seminar fuer Statistik' ETH Zurich <code>shape</code> 1.4.6 Functions for Plotting Graphical Shapes, Colors <code>shiny</code> 1.7.1 Web Application Framework for R <code>shinyjs</code> 2.1.0 Easily Improve the User Experience of Your Shiny Apps in Seconds <code>shinystan</code> 2.6.0 Interactive Visual and Numerical Diagnostics and Posterior Analysis for Bayesian Models <code>shinythemes</code> 1.2.0 Themes for Shiny <code>ShortRead</code> 1.54.0 FASTQ input and manipulation <code>siggenes</code> 1.70.0 Multiple Testing using SAM and Efron's Empirical Bayes Approaches <code>sitmo</code> 2.0.2 Parallel Pseudo Random Number Generator (PPRNG) 'sitmo' Header Files <code>slam</code> 0.1-50 Sparse Lightweight Arrays and Matrices <code>sn</code> 2.0.2 The Skew-Normal and Related Distributions Such as the Skew-t and the SUN <code>snow</code> 0.4-4 Simple Network of Workstations <code>SnowballC</code> 0.7.0 Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library <code>snowfall</code> 1.84-6.1 Easier cluster computing (based on snow). <code>softImpute</code> 1.4-1 Matrix Completion via Iterative Soft-Thresholded SVD <code>sourcetools</code> 0.1.7 Tools for Reading, Tokenizing and Parsing R Code <code>sp</code> 1.4-7 Classes and Methods for Spatial Data <code>spam</code> 2.8-0 SPArse Matrix <code>SparseM</code> 1.81 Sparse Linear Algebra <code>sparseMatrixStats</code> 1.8.0 Summary Statistics for Rows and Columns of Sparse Matrices <code>spatial</code> 7.3-15 Functions for Kriging and Point Pattern Analysis <code>spatstat</code> 2.3-4 Spatial Point Pattern Analysis, Model-Fitting, Simulation, Tests <code>spatstat.core</code> 2.4-4 Core Functionality of the 'spatstat' Family <code>spatstat.data</code> 2.2-0 Datasets for 'spatstat' Family <code>spatstat.geom</code> 2.4-0 Geometrical Functionality of the 'spatstat' Family <code>spatstat.linnet</code> 2.3-2 Linear Networks Functionality of the 'spatstat' Family <code>spatstat.random</code> 2.2-0 Random Generation Functionality for the 'spatstat' Family <code>spatstat.sparse</code> 2.1-1 Sparse Three-Dimensional Arrays and Linear Algebra Utilities <code>spatstat.utils</code> 2.3-1 Utility Functions for 'spatstat' <code>spData</code> 2.0.1 Datasets for Spatial Analysis <code>spdep</code> 1.2-4 Spatial Dependence: Weighting Schemes, Statistics <code>splines</code> 4.2.0 Regression Spline Functions and Classes <code>SQUAREM</code> 2021.1 Squared Extrapolation Methods for Accelerating EM-Like Monotone Algorithms <code>stabledist</code> 0.7-1 Stable Distribution Functions <code>StanHeaders</code> 2.21.0-7 C++ Header Files for Stan <code>stars</code> 0.5-5 Spatiotemporal Arrays, Raster and Vector Data Cubes <code>startupmsg</code> 0.9.6 Utilities for Start-Up Messages <code>statmod</code> 1.4.36 Statistical Modeling <code>stats</code> 4.2.0 The R Stats Package <code>stats4</code> 4.2.0 Statistical Functions using S4 Classes <code>stopwords</code> 2.3 Multilingual Stopword Lists <code>stringdist</code> 0.9.8 Approximate String Matching, Fuzzy Text Search, and String Distance Functions <code>stringfish</code> 0.15.7 Alt String Implementation <code>stringi</code> 1.7.6 Character String Processing Facilities <code>stringr</code> 1.4.0 Simple, Consistent Wrappers for Common String Operations <code>SummarizedExperiment</code> 1.26.1 SummarizedExperiment container <code>survey</code> 4.1-1 Analysis of Complex Survey Samples <code>survival</code> 3.3-1 Survival Analysis <code>sva</code> 3.44.0 Surrogate Variable Analysis <code>symengine</code> 0.2.2 Interface to the 'SymEngine' Library <code>sys</code> 3.4 Powerful and Reliable Tools for Running System Commands in R <code>systemfit</code> 1.1-24 Estimating Systems of Simultaneous Equations <code>TAM</code> 4.0-16 Test Analysis Modules <code>tcltk</code> 4.2.0 Tcl/Tk Interface <code>tensor</code> 1.5 Tensor product of arrays <code>tensorA</code> 0.36.2 Advanced Tensor Arithmetic with Named Indices <code>terra</code> 1.5-21 Spatial Data Analysis <code>testthat</code> 3.1.4 Unit Testing for R <code>tgp</code> 2.4-18 Bayesian Treed Gaussian Process Models <code>TH.data</code> 1.1-1 TH's Data Archive <code>threejs</code> 0.3.3 Interactive 3D Scatter Plots, Networks and Globes <code>tibble</code> 3.1.7 Simple Data Frames <code>tictoc</code> 1.0.1 Functions for Timing R Scripts, as Well as Implementations of Stack and List Structures <code>tidyr</code> 1.2.0 Tidy Messy Data <code>tidyselect</code> 1.1.2 Select from a Set of Strings <code>tidyverse</code> 1.3.1 Easily Install and Load the 'Tidyverse' <code>timeDate</code> 3043.102 Rmetrics - Chronological and Calendar Objects <code>tinytex</code> 0.39 Helper Functions to Install and Maintain TeX Live, and Compile LaTeX Documents <code>tkrplot</code> 0.0-26 TK Rplot <code>tm</code> 0.7-8 Text Mining Package <code>tmap</code> 3.3-3 Thematic Maps <code>tmaptools</code> 3.1-1 Thematic Map Tools <code>tmvnsim</code> 1.0-2 Truncated Multivariate Normal Simulation <code>tools</code> 4.2.0 Tools for Package Development <code>topicmodels</code> 0.2-12 Topic Models <code>triebeard</code> 0.3.0 'Radix' Trees in 'Rcpp' <code>truncnorm</code> 1.0-8 Truncated Normal Distribution <code>trust</code> 0.1-8 Trust Region Optimization <code>tseries</code> 0.10-51 Time Series Analysis and Computational Finance <code>TTR</code> 0.24.3 Technical Trading Rules <code>tweenr</code> 1.0.2 Interpolate Data for Smooth Animations <code>TxDb.Hsapiens.UCSC.hg19.knownGene</code> 3.2.2 Annotation package for TxDb object(s) <code>tzdb</code> 0.3.0 Time Zone Database Information <code>ucminf</code> 1.1-4.1 General-Purpose Unconstrained Non-Linear Optimization <code>udunits2</code> 0.13.2.1 Udunits-2 Bindings for R <code>units</code> 0.8-0 Measurement Units for R Vectors <code>UpSetR</code> 1.4.0 A More Scalable Alternative to Venn and Euler Diagrams for Visualizing Intersecting Sets <code>urca</code> 1.3-0 Unit Root and Cointegration Tests for Time Series Data <code>urltools</code> 1.7.3 Vectorised Tools for URL Handling and Parsing <code>usethis</code> 2.1.6 Automate Package and Project Setup <code>utf8</code> 1.2.2 Unicode Text Processing <code>utils</code> 4.2.0 The R Utils Package <code>uuid</code> 1.1-0 Tools for Generating and Handling of UUIDs <code>V8</code> 4.2.0 Embedded JavaScript and WebAssembly Engine for R <code>VariantAnnotation</code> 1.42.1 Annotation of Genetic Variants <code>vctrs</code> 0.4.1 Vector Helpers <code>vegan</code> 2.6-2 Community Ecology Package <code>VGAM</code> 1.1-6 Vector Generalized Linear and Additive Models <code>VineCopula</code> 2.4.4 Statistical Inference of Vine Copulas <code>viridis</code> 0.6.2 Colorblind-Friendly Color Maps for R <code>viridisLite</code> 0.4.0 Colorblind-Friendly Color Maps (Lite Version) <code>vpc</code> 1.2.2 Create Visual Predictive Checks <code>vroom</code> 1.5.7 Read and Write Rectangular Text Data Quickly <code>vsn</code> 3.64.0 Variance stabilization and calibration for microarray data <code>waldo</code> 0.4.0 Find Differences Between R Objects <code>wateRmelon</code> 2.2.0 Illumina 450 and EPIC methylation array normalization and metrics <code>WGCNA</code> 1.71 Weighted Correlation Network Analysis <code>whisker</code> 0.4 {{mustache}} for R, Logicless Templating <code>widgetframe</code> 0.3.1 'Htmlwidgets' in Responsive 'iframes' <code>widgetTools</code> 1.74.0 Creates an interactive tcltk widget <code>withr</code> 2.5.0 Run Code 'With' Temporarily Modified Global State <code>wk</code> 0.6.0 Lightweight Well-Known Geometry Parsing <code>xfun</code> 0.31 Supporting Functions for Packages Maintained by 'Yihui Xie' <code>xgxr</code> 1.1.1 Exploratory Graphics for Pharmacometrics <code>XML</code> 3.99-0.9 Tools for Parsing and Generating XML Within R and S-Plus <code>xml2</code> 1.3.3 Parse XML <code>xopen</code> 1.0.0 Open System Files, 'URLs', Anything <code>xtable</code> 1.8-4 Export Tables to LaTeX or HTML <code>xts</code> 0.12.1 eXtensible Time Series <code>XVector</code> 0.36.0 Foundation of external vector representation and manipulation in Bioconductor <code>yaml</code> 2.3.5 Methods to Convert R Data to YAML and Back <code>zip</code> 2.2.0 Cross-Platform 'zip' Compression <code>zlibbioc</code> 1.42.0 An R packaged zlib-1.2.5 <code>zoo</code> 1.8-10 S3 Infrastructure for Regular and Irregular Time Series (Z's Ordered Observations)"},{"location":"Other_Services/Aristotle/","title":"Aristotle","text":""},{"location":"Other_Services/Aristotle/#overview","title":"Overview","text":"<p>Aristotle is an interactive, Linux-based compute service for teaching, running on four nodes, each with 128 gigabytes of RAM and 16 cores. The nodes run the Red Hat Enterprise Linux operating system (RHEL 7) and have a subset of the RCPS software stack available.</p> <p>The main aim of this service is to allow specific teaching courses to run that need to run Linux/UNIX applications, but it is available to all UCL users.</p> <p>Warning</p> <p>Aristotle is made available but is provided with minimal staff time and no budget. Any user may completely occupy the service and there is no system in place to prevent that.</p>"},{"location":"Other_Services/Aristotle/#access","title":"Access","text":"<p>Anyone with a UCL userid and within the UCL institutional firewall can access Aristotle by connecting via <code>ssh</code> to:</p> <pre><code>aristotle.rc.ucl.ac.uk\n</code></pre> <p>This address can point to more than one actual server (via DNS round-robin). To connect to a specific server from the set, you will  need to know its number: for example, the second server has the address  <code>aristotle02.rc.ucl.ac.uk</code>. When you connect, you should be shown which  one you are connected to on your command line.</p> <p>The userid and password you need to connect with are those provided to you by Information Services Division.</p> <p>If you experience difficulties with your login, please make sure that you are typing your UCL user ID and your password correctly. If you still cannot get access, please contact us at rc-support@ucl.ac.uk.</p> <p>If you are outside the UCL firewall, you will need to connect to the Gateway system first and then SSH in to Aristotle from there.</p>"},{"location":"Other_Services/Aristotle/#user-environment","title":"User Environment","text":"<p>Aristotle runs Red Hat Enterprise Linux 7 and NFS mounts the  RCPS Software Stack. As this machine is intended for teaching, work has focused on getting specific applications required for specific courses to work and these are:</p> <ul> <li>SAC</li> <li>Phon</li> <li>GMT</li> <li>Fortran compilers (of which there are a large variety)</li> </ul> <p>Packages are available through modules and users should consult the relevant modules documentation.</p>"},{"location":"Other_Services/JupyterHub/","title":"JupyterHub","text":""},{"location":"Other_Services/JupyterHub/#overview","title":"Overview","text":"<p>The Research Computing team provide a JupyterHub service (jupyter.org) primarily to support teaching and is part of the Data Science Platform. It uses the central UNIX filestore for user data. </p>"},{"location":"Other_Services/JupyterHub/#access","title":"Access","text":"<p>To access the service go to the following URL:</p> <p>https://jupyter.data-science.rc.ucl.ac.uk/</p> <p>and login with your UCL userid and password.</p> <p>The Data Science Platform and all of its components are only accessible from the UCL network. When off campus, you will need to connect to the UCL VPN first.</p> <p>Please note: The main aim of this service is to support teaching   and the service should not be used for computationally intensive   research. If your use starts to affect the experience of other   users, we reserve the right to terminate sessions without   notice. For computationally intensive   research you should be using the Myriad Cluster.</p>"},{"location":"Other_Services/RStudio/","title":"RStudio","text":""},{"location":"Other_Services/RStudio/#overview","title":"Overview","text":"<p>The Research Computing team currently runs 2 supported instances of RStudio:</p>"},{"location":"Other_Services/RStudio/#httpsrstudiodata-sciencercuclacuk","title":"https://rstudio.data-science.rc.ucl.ac.uk/","text":"<p>This instance is for general teaching use and is part of the Data Science Platform. It uses the central UNIX filestore for user data. Access is blocked by default due to licencing restrictions.</p> <p>Staff who would like to use the service to deliver teaching should email rc-support@ucl.ac.uk to request access (please include your UCL username). To grant access to students, please pre-register the course by emailing the above address and provide the SITS code of the relevant module(s) and a pdf or link to the syllabus. Students who are registered on those SITS modules will then be added automatically.</p> <p>In addition we have a smalll number of Named Researcher licenses for RStudio Pro which also run on this instance. Staff and Research Postgraduates who would like to have access to one of these licenses should email rc-support@ucl.ac.uk to request access explaining why they need to use RStudio.</p> <p>The Data Science Platform and all of its components are only accessible from the UCL network. When off campus, you will need to connect to the UCL VPN first.</p> <p>Please note: The main aim of this service is to support teaching   and the service should not be used for computationally intensive   research. If your use starts to affect the experience of other   users, we reserve the right to terminate sessions without   notice. For computationally intensive   research you should be using the Myriad Cluster.</p>"},{"location":"Other_Services/RStudio/#httpsecon-myriadrcuclacuk","title":"https://econ-myriad.rc.ucl.ac.uk/","text":"<p>This instance is for research use by members of the Economics department. It uses the Myriad filesystem for user data.</p>"},{"location":"Other_Services/RStudio/#installing-r-packages","title":"Installing R Packages","text":"<p>Users can install R packages in their home directories, but we recommend that if you are planning on teaching a course, you make sure the packages you want to use are pre-installed on the system. This saves time and reduces the load on the server.</p> <p>The main version of R used by the RStudio server copies it's packages from the Myriad Cluster, so any package available there should be available in RStudio too. There's an automatically updated list here: R packages. Alternatively, check the list of available packages from within RStudio itself.</p> <p>Requests to install packages can be sent to rc-support@ucl.ac.uk. Please give as much notice as possible when requesting packages as these requests will be handled on a best efforts basis by the research computing applications support team.</p>"},{"location":"Other_Services/RStudio/#troubleshooting-and-problem-pre-emption","title":"Troubleshooting and problem pre-emption","text":"<p>For all of the services, please take care to either run <code>q()</code> in the R window or press the red logout button in the top right hand corner when you are done with the window, DO NOT just close the tab. This decreases the chance of there being stale sessions and future issues with logging in.</p>"},{"location":"Other_Services/RStudio/#not-being-able-to-reach-the-landing-login-page","title":"Not being able to reach the landing (login) page","text":"<p>If you cannot reach the landing page, then please first try getting there using private browsing and if that works then clear your cookies and cache. In most browsers you can do this for a certain time range, though look at the documentation for the browser you are using.</p>"},{"location":"Other_Services/RStudio/#r-session-not-starting-or-rstudio-initialisation-error","title":"R session not starting or RStudio Initialisation Error","text":"<p>If you get an error pop-up RStudio Initialisation Error: Unable to connect to service or an ever-spinning loading screen you can try and resolve the problem using one of the methods below or  get in touch with RC support. </p> <p>There are 2 courses of action for the 2 supported services:</p> <ul> <li>Economics RStudio service: ssh into Myriad and change the name of or delete a folder located at:</li> </ul> <p><code>~/.local/share/rstudio/sessions/</code></p> <ul> <li> <p>Data Science Platform RStudio Pro teaching service: This service  shares home directories with the central Unix services so you need to  do:</p> <ol> <li>login to either Socrates (<code>socrates.ucl.ac.uk</code>) or Aristotle (<code>aristotle.rc.ucl.ac.uk</code>) via SSH. If you don't know how to do this there are instructions here - but make sure the system you are logging  in to is Socrates or Aristotle not Myriad, because they use the same  filesystem as the RStudio machines: </li> </ol> <ul> <li>logging in via SSH</li> <li>Remote access</li> </ul> <ol> <li> <p>Change directory to ~/.local/share:</p> <p><code>cd ~/.local/share</code></p> </li> <li> <p>delete the rstudio folder:</p> <p><code>rm -rf rstudio</code></p> </li> <li> <p>It is worth testing if you can login to RStudio after the last    step. If you still cannot login, delete the folllowing directory as well:</p> <p><code>cd ~/.rstudio</code></p> <p><code>rm -rf sessions</code></p> </li> <li> <p>logout.</p> </li> </ol> </li> </ul> <p>If doing this doesn't resolve your issues,  get in touch with RC support .</p>"},{"location":"Other_Services/RStudio/#issues-uploading-files-or-failure-during-writing-data","title":"Issues uploading files or failure during writing data","text":"<p>You may have run out of space on the Unix Filestore (T: drive).</p> <p>Check your quota:</p> <pre><code>system(\"quota -s\")\n</code></pre> <p>It will tell you the space you are using, your quota and the limit, then the number of files,  the quota for that and the limit for that. If you are using too much space or too many files  you won't be able to add more until you delete some.</p> <p>If this is not the problem, please get in touch with RC support and tell  us:</p> <ul> <li>How large are the files that are failing to upload?</li> <li>What browser are you using to do the upload to RStudio? Does it work better if you use    Firefox instead? (It is also available on Desktop@UCL).</li> </ul> <p>There is a potential workaround when browser uploads are failing but you definitely have space. You can try uploading your data using scp to Socrates or Aristotle and then accessing it from RStudio once it is already there.</p> <ul> <li>Transferring data onto a system</li> <li>Transferring files with remote computers (Moodle)</li> </ul> <p>Make sure the system you are transferring files to is Socrates or Aristotle, not Myriad.</p>"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/","title":"UCL UK e-Science Certificates","text":"<p>UCL Information Services serves as a local Registration Authority for the authentication of applications for e-Science Certificates.</p> <p>A valid e-Science certificate is required to gain access to the resources of the National e-Infrastructure Service (NES) (amongst others).</p> <p>Brief information to help you in applying for an e-Science Certificate is provided below. More detailed information can be found on the NGS Support website.</p>"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#scope-of-the-ucl-registration-authority","title":"Scope of the UCL Registration Authority","text":"<p>In general, the UCL Registration Authority (RA) can only approve personal and server certificate requests for members of UCL and those associated with projects based at UCL. However we have approved personal certificate requests for members of other London institutions without local RAs on request.</p>"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#before-you-apply-for-a-certificate","title":"Before you Apply for a Certificate","text":"<p>The recommended method for applying for a certificate is to use the Certificate Wizard. So:</p> <ul> <li>Download the CertWizard java application or use WebStart from the     Certificate Wizard page     on the NES website.</li> <li>Install the application on your computer.</li> <li>Run the Certificate Wizard application.</li> </ul>"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#applying-for-a-certificate","title":"Applying for a Certificate","text":"<p>You will be asked for a number of items when completing your request including:</p> <ul> <li> <p>Certificate Wizard keystore password.     Your certificates are stored in keystore which is password     protected. The password (or passphrase) you choose should be at     least 8 characters long and should conform to common secure password     guidelines (e.g. include upper and lower-case letters, numbers, and     punctuation symbols).     Note that this password is the only thing that protects the private     key part of your certificate(s) from being compromised, and thus     rendered invalid. Keep this password to yourself, and don't forget     it.     If you forget this password, or if it is compromised, your     certificate(s) will have to be revoked, and you will need to     re-apply for them (at considerable inconvenience to both you and the     CA/RA).</p> </li> <li> <p>Your given name and family name.     You must enter your real name here. Names of roles will be rejected     by the CA, for example you cannot use Biochem GRID. Your first name     must be a word not just your your initial.</p> </li> <li>Registration Authority.     Use UCL EISD. This is the only valid Registration Authority (RA) for     UCL.</li> <li>Your e-mail address.     Make sure you get this right as it will be used by the e-Science CA     to contact you when your certificate is ready.</li> <li>Your PIN. (Not your bank PIN.)     This should be, at minimum, 10 characters long. You will be asked     for your PIN by your Registration Authority so it needs to be     something you can remember or show. It should not be any of your     normal passwords. Using one of these as your PIN (and thus revealing     it to both your RA and the e-Science CA) will compromise its use as     a password.</li> </ul>"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#extra-items-for-server-certificates","title":"Extra items for Server Certificates","text":"<p>There are two extra items for certificates for servers:</p> <ul> <li>Host Name.     The fully qualified DNS name (not numeric IP address) of your     server.</li> <li>Host Admin Email.     To apply for a server certificate you must have a user certificate     for yourself and be responsible for the server.</li> </ul>"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#after-your-request-has-been-submitted","title":"After Your Request has been Submitted","text":"<p>After you have submitted your request, it has to be authenticated by your Registration Authority (RA) before the certificate is issued by the UK e-Science Certification Authority (CA). For authentication the UK e-Science CA requires that you present yourself in person to your RA with an appropriate form of photo-ID and your PIN. You will be asked to explain why you need a UK e-Science Certificate.</p> <p>The RA for UCL is based in Informations Services Division (ISD). To arrange an appointment please email grid-ra AT ucl.ac.uk in the first instance.</p> <p>Valid forms of Photo-ID are any of the following:</p> <ul> <li>Valid UCL ID card (It has to be a complete ID card with photo;     authorisation for an ID card is not sufficient.)</li> <li>Current passport</li> <li>UK style photocard driving licence</li> </ul> <p>We are required to make and log a photocopy of your photo-ID.</p> <p>If you have none of the above forms of photo-ID available, contact us for advice by e-mail at grid-ra AT ucl.ac.uk. Please don't just turn up with an alternative as we may not be able to accept it.</p>"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#extra-requirements-for-students","title":"Extra Requirements for Students","text":"<p>In addition to the above, students should provide a letter (on department paper) from their project supervisor explaining why they need a certificate.</p>"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#extra-requirements-for-servers","title":"Extra Requirements for Servers","text":"<p>In addition to the above, you need to provide a letter from your department explaining that you are responsible for this server. The letter should be on departmental stationary and be signed by your head of department.</p>"},{"location":"Other_Services/UCL_UK_e-Science_Certificates/#obtaining-your-certificate","title":"Obtaining Your Certificate","text":"<p>After your request has been authenticated by your Registration Authority, it is forwarded to the UK e-Science Certification Authority for final creation (this stage is called signing the certificate). Signing is normally done on the same or next working day.</p> <p>When your certificate is ready the CA will e-mail you using the e-mail address that you provided with details of how to download your certificate. If you used the recommend method to request it, then you can download it into the Certificate Wizard application using the Refresh button.</p> <p>You should now make a backup of your certificate using the Export button in the Certificate Wizard application.</p>"},{"location":"Paid-For_Resources/How_to_Use/","title":"Using Paid-For Resources","text":"<p>Paid resources may be in the form of priority access (Gold), dedicated nodes or both.</p> <p>Users with access to Gold have tools for monitoring budgets and usage.</p> <p>Users with access to dedicated nodes have a tool for monitoring current use of  their nodes.</p>"},{"location":"Paid-For_Resources/How_to_Use/#why-we-recommend-priority-access","title":"Why we recommend priority access","text":"<p>We recommend priority access via Gold rather than dedicated nodes in most circumstances because:</p> <ul> <li>The amount of Gold you get is equivalent to the core hours if you ran your node     at 100% utilisation throughout its 3-year lifespan.</li> <li>If you have a dedicated node and aren't currently running anything on it, it is     idle and you cannot recover that time (and it still uses power and incurs     hosting costs).</li> <li>If you aren't currently using your Gold, you have 3 months (or your chosen     allocation period length) to recover that time and use it.</li> </ul>"},{"location":"Paid-For_Resources/How_to_Use/#priority-access-via-gold","title":"Priority access via Gold","text":"<p>If you have priority access, this is managed by a resource called Gold. Gold is divided into allocations, which have start and end dates and a given amount of Gold associated with them. Any unused Gold expires after the allocation period ends.</p> <p>On Myriad, one Gold = one core hour. If you run a job that asks for a wallclock time of 3hrs and 10 cores, then the job costs 30 Gold.</p> <p>Gold is reserved when you submit a job, so the entire 30 will be reserved  and taken away from the available balance when you run <code>qsub</code>. After the  job ends, how long it ran for is checked, and any unused Gold is put back for anyone to use. </p> <p>For example, you asked for 3 hours and 10 cores, but your job finished  in 2 hours. When you submit the job, it will reserve 30 Gold and your budget will go down by 30. When it ends, the final charge is only 20 Gold, so 10 Gold gets put back in your available budget.</p>"},{"location":"Paid-For_Resources/How_to_Use/#how-it-works","title":"How it works","text":"<p>On Myriad, standard jobs that are submitted all start out with a priority of  2.xxxxx once the scheduler has seen them. (Priority 0.00000 means the scheduler hasn't cycled round to looking at this job and assigning it a priority yet).  Gold jobs begin with priority 3.xxxxx and so will be higher in the queue than  any non-Gold jobs.</p> <p>This does not mean they will be scheduled instantly - the resources they are asking for still need to become free, and there could be other Gold jobs also  in the queue. But it greatly increases the likelihood that this will be the next job to run.</p> <p>It is possible in rare circumstances on Myriad for a non-Gold job to reach  priority 3. It is likely that Gold jobs will still have higher priority.</p>"},{"location":"Paid-For_Resources/How_to_Use/#view-your-gold-budgets","title":"View your Gold budgets","text":"<p>To see the Gold budgets available to you, run:</p> <pre><code>budgets\n</code></pre> <p>You will see something like this:</p> <pre><code>Project    Machines  Balance\n---------  --------  -----------\nhpc.999     ANY       124560.00\nhpc.998     ANY       0.00\n</code></pre> <p>The project column shows which budgets you have access to and the balance shows how much is left unused in the current allocation.</p>"},{"location":"Paid-For_Resources/How_to_Use/#jobscript-additions-for-gold-jobs","title":"Jobscript additions for Gold jobs","text":"<p>You choose whether you want a specific job to be a Gold job or a normal priority job. For a Gold job, add these to your jobscript:</p> <pre><code>#$ -P Gold\n#$ -A hpc.xx\n</code></pre> <p>You can also pass these in on the command line to the <code>qsub</code> and <code>qrsh</code> commands: </p> <pre><code>qsub -P Gold -A hpc.xx myscript.sh\n</code></pre>"},{"location":"Paid-For_Resources/How_to_Use/#viewing-allocation-dates","title":"Viewing allocation dates","text":"<p>You can look at all the start and end dates for your allocations:</p> <pre><code>glsalloc -p hpc.xx\n</code></pre> <p>Output will look like this:</p> <pre><code>Id  Account Projects StartTime  EndTime    Amount    Deposited Description    \n--- ------- -------- ---------- ---------- --------- --------- -------------- \n001 01      hpc.999  -infinity  infinity        0.00      0.00 Auto-Generated \n002 01      hpc.999  2021-12-01 2022-03-01 105124.00 205124.00                \n003 01      hpc.999  2022-03-01 2022-06-01 205124.00 205124.00                \n004 01      hpc.999  2022-06-01 2022-09-01 205124.00 205124.00 \n</code></pre> <p>Allocations begin and end at approximately 00:05 on the date mentioned.</p> <ul> <li>'Deposited' is the total amount this allocation had to begin with.</li> <li>'Amount' is the amount it has left just now.</li> </ul>"},{"location":"Paid-For_Resources/How_to_Use/#monitoring-gold-usage","title":"Monitoring Gold usage","text":"<p>You can view some information about when your Gold was used, in which jobs,  and by whom.</p> <pre><code># show help\ngstatement --man\n\n# show statement between the given dates\ngstatement -p hpc.xx -s 2020-12-01 -e 2021-12-01\n\n# give a summary between the given dates\ngstatement -p hpc.xx -s 2020-12-01 -e 2021-12-01 --summarize\n</code></pre>"},{"location":"Paid-For_Resources/How_to_Use/#dedicated-nodes","title":"Dedicated nodes","text":"<p>For dedicated nodes, only members of your project are allowed to run jobs  on your node. Your project is usually set by default so you do not need to specify it in your jobscript. You can check this by looking at <code>qstat -j $JOB_ID</code> for an existing job ID, and looking at the <code>project</code> line near the bottom.</p>"},{"location":"Paid-For_Resources/How_to_Use/#jobscript-additions-for-dedicated-nodes","title":"Jobscript additions for dedicated nodes","text":"<p>If the project is not being set by default, for a job to be eligible to run on your nodes you will need to specify your project in your jobscript:</p> <pre><code>#\u00a0Specify\u00a0project\n#$\u00a0-P\u00a0&lt;project&gt;\n</code></pre> <p>This will allow a job to run on your nodes, but it can also be scheduled on general-use nodes if some are available first. This should be the main way you run jobs.</p> <p>If you need to, you can force jobs to run on your nodes only. This is suitable when you have arranged policies on your nodes that are different from the normal policies (eg. a longer maximum wallclock time), as it means your policies will be in effect instead of the general policies.</p> <pre><code>#\u00a0Specify\u00a0paid\u00a0flag\u00a0to\u00a0force\u00a0running\u00a0on\u00a0paid\u00a0nodes\u00a0only,\u00a0with\u00a0your\u00a0policies\n#$\u00a0-l\u00a0paid=1\n</code></pre>"},{"location":"Paid-For_Resources/How_to_Use/#check-what-is-running-on-your-nodes","title":"Check what is running on your nodes","text":"<p>We have a script named <code>whatsonmynode</code>, that runs <code>qhost -j</code> for all the nodes belonging to your project, so you can see which nodes you have, what is running on them and from which user.</p> <pre><code>module\u00a0load\u00a0userscripts\nwhatsonmynode\u00a0&lt;project&gt;\n</code></pre>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/","title":"Purchasing in Myriad","text":"<p>4 Nov 2024: The Myriad refresh is confirmed.</p> <p>The filesystem will be replaced first and then new nodes will be added. We will be able to bring new paid nodes online in March 2025 and also allow planning for  future purchases. The nodes we will have available are below. </p> <p>Please contact rc-support@ucl.ac.uk for prices and let us know your timescales. </p> <p>Researchers may purchase additional resources to be used as part of the Myriad High Performance Computing cluster if the free service does not meet their needs. These resources can be made available in one of two ways:</p> <ul> <li>Nodes purchased by researchers can be converted into a quarterly allocation of \u201cpriority cycles\u201d equivalent to the amount of computation provided by the nodes, but usable across the whole cluster. We calculate how much time exists on your nodes for three months, and every three months you receive an allocation of that much priority time to use however you want on Myriad (GPU/large memory nodes cost more to use, and give you more if you buy them). We recommend this route.</li> <li>The purchaser may request to buy nodes to be reserved for their group or department, restricting usage to the owned nodes. There may be an additional cost implication to this option (price on application).</li> </ul> <p>Costs will include backend infrastructure \u2013 racks, switches, cables etc \u2013 needed to integrate the compute nodes into the facility. Both options can be costed into research proposals. These costs will be stated in an agreed Statement of Work document giving details of the number of each node type to be purchased, which must be approved by ARC and the purchaser before the purchase can go ahead.</p>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#deadline","title":"Deadline","text":"<p>We will be able to bring new hardware online in March 2025, no hardware can be added to Myriad before then.  If you need your purchase to be receipted before this date, let us know and we may be able to accommodate  this. However, holding the hardware ahead of March will require additional work for us to store it and  would be a last resort. We will not be able to do this for large numbers of nodes.</p> <p>We expect to be able to run regular quarterly rounds again after this.</p> <p>Next round (currently unconfirmed): mid-June 2025. Let us know if you intend to take part in this one.</p>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#node-types","title":"Node types","text":"<p>We have one standard compute nodes, and several variants of NVIDIA and AMD GPU nodes.</p>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#standard-compute-node","title":"Standard compute node","text":""},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#cpu-node","title":"[CPU node]","text":"<p>Lenovo ThinkSystem SD535 V3</p> <ul> <li>1 x AMD EPYC 9554P 64C 360W 3.1GHz (64 cores)</li> <li>768G RAM</li> <li>2x 480GB M.2 7450 PRO NVMe SSD (960G)</li> </ul>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#nvidia-gpu-nodes","title":"NVIDIA GPU nodes","text":""},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#gpu-node-2u-01","title":"[GPU node 2U 01]","text":"<p>Lenovo ThinkSystem SR655 V3 with 2x NVIDIA L40S GPUs</p> <ul> <li>1x AMD EPYC 9354P 32C 280W 3.25GHz </li> <li>768G RAM, 2x 960GB M.2 NVMe (1920GB)</li> <li>2x NVIDIA L40S 48GB</li> </ul>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#gpu-node-2u-02","title":"[GPU node 2U 02]","text":"<p>Lenovo ThinkSystem SR655 V3 with 2x NVIDIA H100</p> <ul> <li>1x AMD EPYC 9354P 32C 280W 3.25GHz</li> <li>768G RAM, 2x 960GB M.2 NVMe (1920GB)</li> <li>2x NVIDIA H100 94GB</li> </ul>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#gpu-node-2u-03","title":"[GPU node 2U 03]","text":"<p>Lenovo ThinkSystem SR655 V3 with 4x NVIDIA L4</p> <ul> <li>1x AMD EPYC 9354P 32C 280W 3.25GHz</li> <li>768G RAM, 2x 960GB M.2 NVMe (1920GB)</li> <li>4x NVIDIA L4 24GB</li> </ul>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#gpu-node-3u-01","title":"[GPU node 3U 01]","text":"<p>Lenovo ThinkSystem SR675 V3 with 4x NVIDIA L40S GPUs</p> <ul> <li>2x AMD EPYC 9354 32C 280W 3.25GHz (64 cores total)</li> <li>1536G RAM, 2x 960GB NVMe (1920GB)</li> <li>4x NVIDIA L40S 48GB</li> </ul>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#gpu-node-3u-02","title":"[GPU node 3U 02]","text":"<p>Lenovo ThinkSystem SR675 V3 with 4x NVIDIA H100 GPUs</p> <ul> <li>2x AMD EPYC 9354 32C 280W 3.25GHz (64 cores total)</li> <li>1536G RAM, 2x 960GB NVMe (1920GB)</li> <li>4x NVIDIA H100 94GB</li> </ul>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#gpu-node-3u-03","title":"[GPU node 3U 03]","text":"<p>Lenovo ThinkSystem SR675 V3 with 8x NVIDIA L40S GPUs</p> <ul> <li>2x AMD EPYC 9354 32C 280W 3.25GHz (64 cores total)</li> <li>1536G RAM, 2x E1.S 5.9mm 7450 PRO 3.84TB NVMe</li> <li>8x NVIDIA L40S 48GB</li> </ul>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#gpu-node-3u-04","title":"[GPU node 3U 04]","text":"<p>Lenovo ThinkSystem SR675 V3 with 8x NVIDIA H100 GPUs</p> <ul> <li>2x AMD EPYC 9354 32C 280W 3.25GHz (64 cores total)</li> <li>1536G RAM, 2x E1.S 5.9mm 7450 PRO 3.84TB NVMe</li> <li>8x NVIDIA H100 94GB</li> </ul>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#amd-gpu-nodes","title":"AMD GPU nodes","text":""},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#gpu-node-3u-amd-01","title":"[GPU node 3U AMD 01]","text":"<p>Lenovo SR675 V3 with 4x AMD Instinct MI210</p> <ul> <li>2x AMD EPYC 9354P 32C 280W 3.25GHz (64 cores total)</li> <li>1536GB RAM,  2x 960GB NVMe (1920GB)</li> <li>4x AMD Instinct MI210</li> </ul>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#gpu-node-3u-amd-02","title":"[GPU node 3U AMD 02]","text":"<p>Lenovo SR675 V3 with 8x AMD Instinct MI210</p> <ul> <li>2x AMD EPYC 9354P 32C 280W 3.25GHz </li> <li>1536GB RAM, 2x E1.S 5.9mm 7450 PRO 3.84TB NVMe</li> <li>8x AMD Instinct MI210</li> </ul>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#faqs","title":"FAQs","text":""},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#can-i-add-the-cost-of-purchasing-nodes-to-a-grant-application","title":"Can I add the cost of purchasing nodes to a grant application?","text":"<p>If you are putting together a grant application and think that you may need to ask for the cost of additional computing resources to be covered, please contact us. We will be able to assess your requirements, recommend appropriate hardware, and then provide an estimate and a supporting statement.</p>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#can-you-give-me-advice-on-what-hardware-to-buy","title":"Can you give me advice on what hardware to buy?","text":"<p>Yes, we\u2019d be happy to discuss this with you. Please contact rc-support@ucl.ac.uk.</p>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#what-type-of-nodes-can-we-purchase","title":"What type of nodes can we purchase?","text":"<p>Current node types.</p> <p>If you require an alternative/custom specification, we can\u2019t guarantee that we will be able to accommodate this on the cluster, but we\u2019re happy to look into it.</p>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#i-want-to-apply-for-more-than-50k-worth-of-equipment-will-we-have-to-go-through-a-tender-process","title":"I want to apply for more than \u00a350k worth of equipment, will we have to go through a tender process?","text":"<p>No. We have a framework agreement with the vendor which covers all hardware purchases.</p>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#i-know-what-hardware-i-need-can-you-send-me-a-quote","title":"I know what hardware I need. Can you send me a quote?","text":"<p>We can send ballpark quotes initially.</p> <p>Before we can send you a final quote, we will need to agree on a detailed specification. Please email rc-support@ucl.ac.uk with the following information:</p> <ul> <li>Budget holder: name and contact details</li> <li>Type and number of nodes you\u2019d like to purchase</li> </ul> <p>We will then either send you a specification to approve, or ask to meet to discuss your requirements further. Once this is agreed, we aim to get back to you with a quote within two weeks.</p>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#how-do-i-manage-who-has-permission-to-use-our-nodespriority-queue","title":"How do I manage who has permission to use our nodes/priority queue?","text":"<p>When you purchase nodes or priority cycles, we will ask you for a list of usernames of people who have permission to use the resource \u2014 access is managed using access control lists on Myriad.  If your list of users is an entire department, we can automatically generate this list nightly. Resource owners or designated resource administrators can request a change of membership of these groups by submitting a request in MyServices or emailing rc-support@ucl.ac.uk. </p>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#what-is-the-difference-between-paying-for-priority-cycles-and-purchasing-dedicated-nodes","title":"What is the difference between paying for priority cycles and purchasing dedicated nodes?","text":"<p>Priority cycles is the better option for most people as it provides greater flexibility: priority cycles can be used across many nodes at once, and there is an entire allocation period to use them. Dedicated hardware however would need to be in use 24/7 in order to get the most out of it. Researchers might want dedicated nodes if they have particular requirements which mean they can only run their work on their own nodes; e.g., they have purchased non-standard nodes, or the software they are using requires a static licence tied to a particular node.</p> <p>Please see Paid-For Resources: How to Use for more information.</p>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#will-my-3-month-priority-cycle-allocation-roll-over-to-the-next-quarter-if-i-dont-use-it","title":"Will my 3-month priority cycle allocation roll over to the next quarter if I don\u2019t use it?","text":"<p>No.</p>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#i-want-the-flexibility-of-priority-cycles-but-my-funder-requires-an-invoice-for-specific-hardware","title":"I want the flexibility of priority cycles, but my funder requires an invoice for specific hardware.","text":"<p>Even if you require an invoice for specific hardware, you can still convert the physical hardware into priority cycles. We will add the nodes you purchased to the general pool and give you the equivalent in priority cycles.</p>"},{"location":"Paid-For_Resources/Purchasing_in_Myriad/#what-happens-to-nodes-i-have-purchased-once-theyve-reached-the-end-of-warranty","title":"What happens to nodes I have purchased once they\u2019ve reached the end of warranty?","text":"<p>Node purchases have a three-year warranty as standard. After the warranty period ends, if the nodes still function  they will be returned to the general use pool (if they were dedicated nodes) and continue running jobs until they  fail, or until they are retired.</p>"},{"location":"Software_Guides/ANSYS/","title":"ANSYS","text":"<p>License checking</p> <p>Current as of Dec 2021, the <code>-ac app=cfx</code> license check does not work after ANSYS renamed all products and licenses. Remove this line from your jobscripts for the time being or jobs will not be able to start.</p> <p>ANSYS/CFX and ANSYS/Fluent are commercial fluid dynamics packages.</p> <p>The most recent version of ANSYS we have installed on the clusters is ANSYS 2023.R1 including ANSYS/CFX, ANSYS/Fluent, ANSYS Mechanical and the ANSYS Electromagnetics Suite (AnsysEM). Some other products included in the ANSYS Campus Agreement are also available including Autodyn.</p> <p>Older versions of some ANSYS products including ANSYS/CFX and ANSYS/Fluent are also available but only on Kathleen at the moment - 2021.R2, 2019.R3, 19.1 for example. However ANSYS Inc changed the way they license their software at the beginning of 2021, causing some products from versions before 2020 to have issues getting a valid license from the lciense server.</p> <p>Before these applications can be run, the user needs to go though a number of set up steps. These are detailed here.</p> <p>To see the versions available, type</p> <pre><code>module avail ansys\n</code></pre> <p>The desired ANSYS module needs to be loaded by issuing a command like:</p> <pre><code>module\u00a0load\u00a0ansys/2023.r1\n</code></pre> <p>This will set up various necessary config directories for you.</p> <p>Module for UCL staff and Students only</p> <p>these ANSYS modules are for UCL staff and Students only as they use the UCL ANSYS License Manager. Users from external Institutions on Young (the MMM Hub cluster) need to use modules specific to their institution instead.</p> <p>The ANSYS applications are intended to be run primarily within batch jobs however you may run short (less than 5 minutes execution time) interactive tests on the Login Nodes and longer (up to two hours) on an interactive session on a compute node using qrsh. Interactive work can be done using the ANSYS interactive tools provided you have X-windows functionality enabled though your ssh connection. See our User Guide for more information about enabling X-windows functionality and using the qrsh command to start interactive sessions.</p> <p>UCL's campus-wide license covers 125 instances with 512 HPC licenses (for  parallel jobs) available for running CFX, Fluent and AnsysEM jobs and in  order to make sure that jobs only run if there are licenses available, it  is necessary for users to request ANSYS licenses with their jobs, by adding  <code>-ac app=cfx</code> to their job submission.</p>"},{"location":"Software_Guides/ANSYS/#ansys-supplied-mpi-used-as-default-does-not-work","title":"ANSYS Supplied MPI used as default does not work","text":"<p>ANSYS 2023.R1 is supplied with a MPI which is used as the default but doesn't work on our clusters. In every parallel job script you need to specify the supplied Intel 2018 MPI alternative. See the example job scripts below.</p>"},{"location":"Software_Guides/ANSYS/#ansyscfx","title":"ANSYS/CFX","text":"<p>CFX handles its own parallelisation, so a number of complex options need to be passed in job scripts to make it run correctly.</p>"},{"location":"Software_Guides/ANSYS/#example-single-node-multi-threaded-ansyscfx-jobscript","title":"Example single node multi-threaded ANSYS/CFX jobscript","text":"<p>Here is an example runscript for running <code>cfx5solve</code> multi-threaded on a given <code>.def</code> file.</p> <pre><code>#!/bin/bash -l\n\n# ANSYS 2023.R1: Batch script to run cfx5solve on the StaticMixer.def example \n# file, single node multi-threaded (12 threads),\n\n# Force bash as the executing shell.\n\n#$ -S /bin/bash\n\n# Request 15 munutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:15:0\n\n# Request 1 gigabyte of RAM per core.\n#$ -l mem=1G\n\n# Set the name of the job.\n#$ -N StaticMixer_thread_12\n\n# Select 12 threads.\n#$ -pe smp 12\n\n# Request ANSYS licences\n# Dec 2021: comment out this check as not currently working\n###$ -ac app=cfx\n\n# Set the working directory to somewhere in your scratch space. In this\n# case the subdirectory cfxtests-2023.R1\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/cfxtests-2023.R1\n\n# Load the ANSYS module to set up your environment\n\nmodule load ansys/2023.r1\n\n# Copy the .def file into the working (current) directory\n\ncp /home/&lt;your userid&gt;/cfx_examples/StaticMixer.def .\n\n# Run cfx5solve - Note: -max-elapsed-time needs to be set to the same\n# time as defined by 2 above.\n\ncfx5solve -max-elapsed-time \"15 [min]\" -def StaticMixer.def -par-local -partition $OMP_NUM_THREADS\n</code></pre> <p>You will need to change the <code>-wd /home/&lt;your_UCL_id&gt;/Scratch/cfxtests-2023.R1</code> location and may need to change the memory, wallclock time, number of threads and job name directives as well. Replace the .def file with your one and modify the -max-elapsed-time value if needed. The simplest form of qsub command can be used to submit the job eg:</p> <pre><code>qsub run-StaticMixer-thr.sh\n</code></pre> <p>Output files will be saved in the job's working directory.</p>"},{"location":"Software_Guides/ANSYS/#example-multi-node-mpi-ansyscfx-jobscript","title":"Example multi-node MPI ANSYS/CFX jobscript","text":"<p>Here is an example runscript for running cfx5solve on more than one node (using MPI) on a given .def file. Note: the need to use the supplied Intel 2018 MPI instead of the default.</p> <pre><code>#!/bin/bash -l\n\n# ANSYS 2023.R1: Batch script to run cfx5solve on the StaticMixer.def example \n# file, distributed parallel (80 cores).\n\n# Using ANSYS 2021 licence manager running on UCL central licence server.\n\n# 1. Force bash as the executing shell.\n#$ -S /bin/bash\n\n# 2. Request one hour of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:60:0\n\n# 3. Request 2 gigabyte of RAM per core.\n#$ -l mem=2G\n\n# 4. Set the name of the job.\n#$ -N StaticMixer_P_dist_80_NLC\n\n# 5. Select the MPI parallel environment and 80 processors.\n#$ -pe mpi 80\n\n# 6. Request ANSYS licences $ not inserted so currently inactive. Job will queue until\n# suficient licenses are available when active.\n# -ac app=cfx\n\n# 7. Set the working directory to somewhere in your scratch space.  In this\n# case the subdirectory cfxtests-2023.R1\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/cfxtests-2023.R1\n\n# 8. Load the ANSYS module to set up your environment\n\nmodule load ansys/2023.r1\n\n# 9. Copy the .def file into the working (current) directory\n\ncp /home/&lt;your_UCL_id&gt;/Software/ANSYS/cfx_examples/StaticMixer.def .\n\n# 10. SGE puts the machine file in $TMPDIR/machines. Use this to generate the \n# string CFX_NODES needed by cfx5solve\n\nexport CFX_NODES=`cfxnodes $TMPDIR/machines`\n\n# 11. Run cfx5solve - Note: -max-elapsed-time needs to be set to the same\n# time as defined by 2 above.\n\n# Run with default MPI.\ncfx5solve -max-elapsed-time \"60 [min]\" -def StaticMixer.def -start-method \"Intel MPI 2018 Distributed Parallel\" -par-dist $CFX_NODES\n</code></pre> <p>Please copy if you wish and edit it to suit your jobs. You will need to change the <code>-wd /home/&lt;your_UCL_userid&gt;/Scratch/cfxtests-2023.R1</code> location and may need to change the memory, wallclock time, number of MPI processors and job name directives as well. Replace the <code>.def</code> file with your one and modify the <code>-max-elapsed-time</code> value if needed. The simplest form of qsub command can be used to submit the job eg:</p> <pre><code>qsub\u00a0run-StaticMixer-par.sh\n</code></pre> <p>Output files will be saved in the job's working directory.</p>"},{"location":"Software_Guides/ANSYS/#running-cfx-with-mpi-on-myriad","title":"Running CFX with MPI on Myriad","text":"<p>On Myriad the maximum number of MPI processors you can request is 36.</p>"},{"location":"Software_Guides/ANSYS/#troubleshooting-cfx","title":"Troubleshooting CFX","text":"<p>If you are getting licensing errors when trying to run a parallel job and you have an older version's <code>~/.ansys/v161/licensing/license.preferences.xml</code> file, delete it. It does not work with the newer license server. (This applies to all older versions, not just <code>v161</code>).</p>"},{"location":"Software_Guides/ANSYS/#ansysfluent","title":"ANSYS/Fluent","text":"<p>Fluent handles its own parallelisation, so a number of complex options need to be passed in job scripts to make it run correctly.</p> <p>The .in file mentioned in the scripts below is a Fluent journal file, giving it the list of commands to carry out in batch mode.</p>"},{"location":"Software_Guides/ANSYS/#example-serial-ansys-fluent-jobscript","title":"Example serial ANSYS Fluent jobscript","text":"<p>Serial ANSYS Fluent not currently working on Myriad</p> <p>Currently we have not been able to get serial ANSYS Fluent jobs working on Myriad with ANSYS 2023.R1.</p> <p>Here is an example jobscript for running Fluent in serial mode (1 core). Note: this is for an older version of ANSYS which is not currently available on Myriad and Kathleen cannot run serial jobs.</p> <pre><code>#!/bin/bash -l\n\n# ANSYS 2021.R2: Batch script to run ANSYS/fluent in serial mode \n# (1 core). \n\n# Request 2 hours of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=2:0:0\n\n# Request 2 gigabytes of RAM.\n#$ -l mem=2G\n\n# Set the name of the job.\n#$ -N Fluent_ser1\n\n# Request ANSYS licences\n# Dec 2021: comment out this check as not currently working\n###$ -ac app=cfx\n\n# Set the working directory to somewhere in your scratch space.  In this\n# case the subdirectory fluent-tests-19.1\n#$ -wd /home/&lt;your_UCL_userid&gt;/Scratch/fluent-tests-2021.R2\n\n# Load the ANSYS module to set up your environment\n\nmodule load ansys/2021.r2\n\n# Copy Fluent input files into the working (current) directory\n\ncp &lt;path to your input files&gt;/test-1.cas .\ncp &lt;path to your input files&gt;/test-1.in .\n\n# Run fluent in 2D single precision (-g no GUI). For double precision use\n# 2ddp. For 3D use 3d or 3ddp. \n\nfluent 2d -g &lt; test-1.in\n</code></pre> <p>Please copy if you wish and edit it to suit your jobs. You will need to change the <code>-wd /home/&lt;your_UCL_id&gt;/Scratch/fluent-tests-2021.R2</code>  location and may need to change the memory, wallclock time, and job name as well. Replace the <code>.cas</code> and <code>.in</code> files with your ones. The simplest form of qsub command can be used to submit the job eg:</p> <pre><code>qsub\u00a0run-ANSYS-fluent-ser.sh\n</code></pre> <p>Output files will be saved in the job's working directory.</p>"},{"location":"Software_Guides/ANSYS/#example-parallel-mpi-ansysfluent-jobscript","title":"Example parallel (MPI) ANSYS/Fluent jobscript","text":"<p>Here is an example runscript for running Fluent in parallel potentially across more than one node. Note: the need to use the supplied Intel 2018 MPI instead of the default.</p> <pre><code>#!/bin/bash -l\n\n# ANSYS 2023.R1: Batch script to run ANSYS/fluent distributed parallel \n# (80 cores). \n\n# Request 2 hours of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=2:0:0\n\n# Request 2 gigabytes of RAM per core.\n#$ -l mem=2G\n\n# Set the name of the job.\n#$ -N Fluent_par80\n\n# Select the MPI parallel environment and 80 processors.\n#$ -pe mpi 80\n\n# Request 25 Gb TMPDIR space (if on a cluster that supports this)\n#$ -l tmpfs=25G\n\n# Request ANSYS licences\n# Dec 2021: comment out this check as not currently working\n###$ -ac app=cfx\n\n# Set the working directory to somewhere in your scratch space.  In this\n# case the subdirectory fluent-tests-2023.R1\n#$ -wd /home/&lt;your_UCL_userid&gt;/Scratch/fluent-tests-2023.R1\n\n# Load the ANSYS module to set up your environment\n\nmodule load ansys/2023.r1\n\n# Copy Fluent input files into the working (current) directory\n\ncp &lt;path to your input files&gt;/test-1.cas .\ncp &lt;path to your input files&gt;/test-1.in .\n\n# Run fluent  in 3D single precision (-g no GUI). For double precision use\n# 3ddp. For 2D use 2d or 2ddp. \n# Do not change -t, -mpi, and -cnf options.\n\nfluent 3ddp -t$NSLOTS -mpi=intel2018 -cnf=$TMPDIR/machines -g &lt; test-1.in\n</code></pre> <p>Please copy if you wish and edit it to suit your jobs. You will need to change the <code>-wd /home/&lt;your_UCL_id&gt;/Scratch/fluent-tests-2023.R1</code> location and may need to change the memory, wallclock time, number of MPI processors and job name as well. Replace the .cas and .in files with your ones. The simplest form of qsub command can be used to submit the job eg: </p> <pre><code>qsub\u00a0run-ANSYS-fluent-par-80.sh\n</code></pre> <p>Output files will be saved in the job's working directory.</p>"},{"location":"Software_Guides/ANSYS/#running-ansys-fluent-with-mpi-on-myriad","title":"Running ANSYS Fluent with MPI on Myriad","text":"<p>On Myriad the maximum number of MPI processors you can request is 36.</p>"},{"location":"Software_Guides/ANSYS/#changes-to-jobscripts-for-ansys-fluent-2021r2","title":"Changes to jobscripts for ANSYS Fluent 2021.R2","text":"<p>The following changes are needed to the example jobscript for the older ANSYS 2021.R2 version. Load this module file instead:</p> <pre><code>module load ansys/2021.r2\n</code></pre> <p>and run fluent with this MPI option:</p> <pre><code>fluent 3ddp -t$NSLOTS -mpi=intel -cnf=$TMPDIR/machines -g &lt; test-1.in\n</code></pre>"},{"location":"Software_Guides/ANSYS/#troubleshooting-fluent","title":"Troubleshooting Fluent","text":"<p>If you are getting licensing errors when trying to run a parallel job and you have an older version's <code>~/.ansys/v161/licensing/license.preferences.xml</code> file, delete it. It does not work with the newer license server. (This applies to all older versions, not just <code>v161</code>).</p> <p>Fluent 14 required <code>-mpi=pcmpi -pinfiniband</code> in the parallel options: if you have older scripts remember to remove this.</p>"},{"location":"Software_Guides/ANSYS/#ansys-mechanical","title":"ANSYS Mechanical","text":"<p>ANSYS Mechanical handles its own parallelisation, and needs an additional setting to work on our clusters. It also only appears to work with Intel MPI. Here is an example jobscript for running in parallel potentially across more than one node, for example on the Kathleen cluster. Note: the need to use the supplied Intel 2018 MPI instead of the default.</p> <pre><code>#!/bin/bash -l\n\n# ANSYS 2023.R1: Batch script to run ANSYS Mechanical solver\n# file, distributed parallel (80 cores).\n\n# Using ANSYS 2021.R2 licence manager running on UCL central licence server.\n\n# Force bash as the executing shell.\n#$ -S /bin/bash\n\n# Request one hour of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=1:00:0\n\n# Request 2 gigabyte of RAM per core. (Must be an integer)\n#$ -l mem=2G\n\n# Set the name of the job.\n#$ -N Mech_P_dist_80\n\n# Select the MPI parallel environment and 80 processors.\n#$ -pe mpi 80\n\n# Request ANSYS licences $ inserted so currently active.Job will queue until\n# suficient licences are available.\n# Dec 2021: comment out this check as not currently working\n###$ -ac app=cfx\n\n# Set the working directory to somewhere in your scratch space.  In this\n# case the subdirectory ANSYS_Mech\n#$ -wd /home/&lt;your_UCL_username&gt;/Scratch/ANSYS_Mech\n\n# Load the ANSYS module to set up your environment\nmodule load ansys/2023.r1\n\n# Copy the .in file into the working (current) directory\ncp ~/ANSYS/steady_state_input_file.dat .\n\n# 10. SGE puts the machine file in $TMPDIR/machines. Use this to generate the \n# string CFX_NODES needed by ansys195 which requires : as the separator.\n\nexport CFX_NODES=`cfxnodes_cs $TMPDIR/machines`\necho $CFX_NODES\n\n# Need to set KMP_AFFINTY to get round error: OMP: System error #22: Invalid argument\n\nexport KMP_AFFINITY=disabled\n\n# Run ansys mechanical\n\nmapdl -mpi intelmpi2018 -dis -b -machines $CFX_NODES -i steady_state_input_file.dat\n</code></pre> <p>Please copy if you wish and edit it to suit your jobs. You will need to change the <code>-wd /home/&lt;your_UCL_id&gt;/Scratch/ANSYS_Mech</code> location and may need to change the memory, wallclock time, number of MPI processors and job name as well. Replace the .dat  file with your one. The simplest form of qsub command can be used to submit the job eg: </p> <pre><code>qsub\u00a0ansys-mech-2023.R1-ex.sh\n</code></pre> <p>Output files will be saved in the job's working directory.</p> <p>If you have access to Kathleen, this test input and jobscript are available at <code>/home/ccaabaa/Software/ANSYS/steady_state_input_file.dat</code> and <code>/home/ccaabaa/Software/ANSYS/ansys-mech-2023.R1-ex.sh</code></p>"},{"location":"Software_Guides/ANSYS/#ansys-electromagnetic-suite-ansysem","title":"ANSYS Electromagnetic Suite (AnsysEM)","text":"<p>The AnsysEM products handle their own parallelisation so a number of complex options need to be passed in job scripts to make it run correctly. Also additional module commands are required.</p> <p>Here is an example jobscript for running in parallel potentially across more than one node, for example on the Kathleen cluster.</p> <pre><code>#!/bin/bash -l\n\n# AnsysEM 2023 R1: Batch script to run one of the Ansys Electromagnetics Products\n# example simulations on Kathleen - distributed parallel (80 cores)\n\n# 1. Force bash as the executing shell.\n#$ -S /bin/bash\n\n# 2. Request one hour of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=1:00:0\n\n# 3. Request 2 gigabyte of RAM per core.\n#$ -l mem=2G\n\n# 4. Set the name of the job.\n#$ -N DiffSL_P_dist_80\n\n# 5. Select the MPI parallel environment and 80 processors - two nodes.\n#$ -pe mpi 80\n\n# 7. Request ANSYS licences $ inserted so currently active. Job will queue until\n# suficient licenses are available. Not currently active.\n###$ -ac app=cfx\n\n# 8. Set the working directory to somewhere in your scratch space.  In this\n# case the directory the job was submitted from,\n#$ -cwd\n\n# 9. Load the ANSYS module to set up your environment\n\nmodule load ansys/2023.r1\nmodule load xorg-utils/X11R7.7\nmodule load giflib/5.1.1\n\n# 10. Run ansysedt\n\nansysedt -ng -Distributed -machinelist num=$NSLOTS -batchoptions 'HFSS/MPIVendor'='Intel' -batchoptions 'TempDirectory'=\"${TMPDIR}\" -batchoptions 'HFSS/HPCLicenseType'='pool'  -batchoptions 'HFSS-IE/HPCLicenseType'='pool' -BatchSolve differential_stripline.aedt\n</code></pre>"},{"location":"Software_Guides/AlphaFold3/","title":"AlphaFold3","text":"<p>The AlphaFold 3 Inference pipeline was developed by Google Deepmind to predict protein structures.</p> <p>It is installed on Myriad as an Apptainer package meaning it is not in environment modules.</p>"},{"location":"Software_Guides/AlphaFold3/#setup","title":"Setup","text":"<p>Important</p> <p>You will need to apply to Google for access to the model parameters which you can do here.</p> <p>Once you have downloaded them, put them in a filesystem you have access to on Myriad.</p> <p>The databases are already downloaded and put in <code>/shared/ucl/apps/AlphaFold3_db</code>.</p> <p>Note on V100s</p> <p>Most of the GPUs on Myriad are V100s, which need to have additional parameters passed to the container, namely setting an environment variable in the contaner (<code>XLA_FLAGS='--xla_disable_hlo_passes=custom-kernel-fusion-rewriter</code>) and passing a command line option to AlphaFold3 itself (<code>--flash_attention_implementation=xla</code>) in order to produce good structures. You can find more details on this on this Github issue.</p> <p>Having added the model weights, you will also need to create input and output directories.</p> <p>You should now have three locations which are unique to you. For ease of reference  in the job script I'm going to set environment variables to them so that the command-line is consistent and so you can change them without messing with the command line options.</p> <pre><code>export AF3_INPUT=/scratch/scratch/uccaoke/af3_input # Replace with your input folder\nexport AF3_INPUT_FILE=fold_input.json # Replace with a file in your input folder\nexport AF3_OUTPUT=/scratch/scratch/uccaoke/af3_output # Replace with your output folder\nexport AF3_WEIGHTS=/scratch/scratch/uccaoke/weights # Replace with the folder you put the weights in\n</code></pre>"},{"location":"Software_Guides/AlphaFold3/#running-alphafold3","title":"Running AlphaFold3","text":"<p>Write a job script that requests GPU nodes:</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run AlphaFold 3 on a V100 job under SGE.\n\n# Request one V100\n#$ -l gpu=1\n#$ -ac allow=EF\n\n# Request 18 cores (half a node)\n#$ -pe smp 18\n\n# Request one hour of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=1:0:0\n\n# Request 5 gigabyte of RAM per core.\n#$ -l mem=5G\n\n# Set the name of the job.\n#$ -N AlphaFold3\n\n# Set the working directory to the current working directory.\n#$ -cwd\n\nexport AF3_INPUT=/scratch/scratch/uccaoke/af3_input # Replace with your input folder\nexport AF3_INPUT_FILE=fold_input.json # Replace with a file in your input folder\nexport AF3_OUTPUT=/scratch/scratch/uccaoke/af3_output # Replace with your output folder\nexport AF3_WEIGHTS=/scratch/scratch/uccaoke/weights # Replace with the folder you put the weights in\n\napptainer exec --nv --bind ${AF3_INPUT}:/root/af_input --bind ${AF3_OUTPUT}:/root/af_output --bind ${AF3_WEIGHTS}:/root/models --bind /shared/ucl/apps/AlphaFold3_db:/root/public_databases --no-home --no-mount bind-paths  /shared/ucl/apps/AlphaFold3/alphafold3.sif sh -c \"XLA_FLAGS='--xla_disable_hlo_passes=custom-kernel-fusion-rewriter' python3 /app/alphafold/run_alphafold.py --json_path=/root/af_input/${AF3_INPUT_FILE} --model_dir=/root/models --db_dir=/root/public_databases --output_dir=/root/af_output --flash_attention_implementation=xla\"\n</code></pre> <p>The flags which are set to work on V100s almost certainly modestly harm performance on A100 so we set this job to request a V100 (the line that says <code>#$ -ac allow=EF</code>). If you delete that line the job will run on any GPU in the cluster.</p> <p>If you wish to queue for an A100, this job should work:</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run AlphaFold 3 on a V100 job under SGE.\n\n# Request one A100\n#$ -l gpu=1\n#$ -ac allow=L\n\n# Request 18 cores (half a node)\n#$ -pe smp 18\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=1:0:0\n\n# Request 5 gigabyte of RAM per core.\n#$ -l mem=5G\n\n# Set the name of the job.\n#$ -N AlphaFold3\n\n# Set the working directory to the current working directory.\n#$ -cwd\n\nexport AF3_INPUT=/scratch/scratch/uccaoke/af3_input # Replace with your input folder\nexport AF3_INPUT_FILE=fold_input.json # Replace with a file in your input folder\nexport AF3_OUTPUT=/scratch/scratch/uccaoke/af3_output # Replace with your output folder\nexport AF3_WEIGHTS=/scratch/scratch/uccaoke/weights # Replace with the folder you put the weights in\n\napptainer exec --nv --bind ${AF3_INPUT}:/root/af_input --bind ${AF3_OUTPUT}:/root/af_output --bind ${AF3_WEIGHTS}:/root/models --bind /shared/ucl/apps/AlphaFold3_db:/root/public_databases --no-home --no-mount bind-paths  /shared/ucl/apps/AlphaFold3/alphafold3.sif sh -c \"python3 /app/alphafold/run_alphafold.py --json_path=/root/af_input/${AF3_INPUT_FILE} --model_dir=/root/models --db_dir=/root/public_databases --output_dir=/root/af_output\"\n</code></pre> <p>For other options you can pass to AlphaFold 3, please consult DeepMind's documentation.</p>"},{"location":"Software_Guides/Installing_Software/","title":"Installing Software","text":""},{"location":"Software_Guides/Installing_Software/#installing-software","title":"Installing software","text":"<p>If you want to request that software be installed centrally, you can email us at  rc-support@ucl.ac.uk. When you send in a request please address the following questions so  that the install can be properly prioitised and planned,</p> <ul> <li>Can you provide some details as to why and give an idea of the timeline you would like us  to build it in?</li> <li>Do you have an idea of the user base for this software within your community? If you are asking  for some software on the MMM machines this can be the wider community and for UCL machines users  in your ecosystem.</li> <li>If the software is only required by you would you be open to trying to install the software in  your home space? We can provide some assistance here if you tell us what problems you are encountering.</li> </ul> <p>The requests will be added to the issues in our buildscripts repository. The buildscripts themselves are there too, so you can see how we  built and installed our central software stack.</p> <p>You can install software yourself in your space on the cluster. Below are some tips for  installing packages for languages such as Python or Perl as well as compiling software.</p>"},{"location":"Software_Guides/Installing_Software/#no-sudo","title":"No sudo!","text":"<p>You cannot install anything using <code>sudo</code> (and neither can we!). If the instructions tell you to do that, read further to see if they also have instructions for installing in user space, or for doing an install from source if they are RPMs.</p> <p>Alternatively, just leave off the <code>sudo</code> from the command they tell you to run and look for an alternative way to give it an install location if it tries to install somewhere that isn't in your space (examples for some common build systems are below).</p>"},{"location":"Software_Guides/Installing_Software/#download-source-code","title":"Download source code","text":"<p>Use wget or curl to download the source code for the software you want to install  to your account on the cluster. You can use <code>tar</code> to extract source archives named  like <code>tar.gz</code> or <code>.tgz</code> or <code>.tar.bz2</code> among others and <code>unzip</code> for <code>.zip</code> files.  <code>xz --decompress</code> will expand <code>.xz</code> files.</p> <pre><code>wget https://www.example.com/program.tar.gz\ntar -xvf program.tar.gz\n</code></pre> <p>You will not be able to use a package manager like <code>yum</code>, and will need to follow  the manual installation instructions for a user-space install (not using <code>sudo</code>).</p>"},{"location":"Software_Guides/Installing_Software/#set-up-modules","title":"Set up modules","text":"<p>Before you start compiling, you need to make sure you have the right compilers,  libraries and other tools available for your software. If you haven't changed  anything, you will have the default modules loaded.</p> <p>Check what the instructions for your software tell you about compiling it. If the  website doesn't say much, the source code will hopefully have a README or INSTALL file.</p> <p>You may want to use a different compiler - the default is the Intel compiler.</p> <p><code>module avail compilers</code> will show you all the compiler modules available. Most  Open Source software tends to assume you're using GCC and OpenMPI (if it uses MPI)  and is most tested with that combination, so if it doesn't tell you otherwise  you  may want to begin there (do check what the newest modules available are - the below  is correct at time of writing):</p> <pre><code># unload your current compiler and mpi modules\nmodule unload -f compilers mpi\n# load the GNU compiler\nmodule load compilers/gnu/4.9.2\n\n# these three modules are only needed on Myriad\nmodule load numactl/2.0.12\nmodule load binutils/2.29.1/gnu-4.9.2\nmodule load ucx/1.8.0/gnu-4.9.2\n\n# load OpenMPI\nmodule load mpi/openmpi/4.0.3/gnu-4.9.2\n</code></pre> <p>Useful resources:</p> <ul> <li>Modules pt 1 (moodle) (UCL users)</li> <li>Modules pt 2 (moodle) (UCL users)</li> <li>Modules pt 1 (mediacentral) (non-UCL users)</li> <li>Modules pt 2 (mediacentral) (non-UCL users)</li> </ul>"},{"location":"Software_Guides/Installing_Software/#newer-versions-of-gcc-and-glibcxx","title":"Newer versions of GCC and GLIBCXX","text":"<p>The software you want to run may require newer compilers or a precompiled binary may say  that it needs a newer GLIBCXX to be able to run. <code>module avail compilers</code> will show you all the available compilers and you can then use them as follows:</p> <pre><code># unload current compiler, mpi and gcc-libs modules\nmodule unload compilers mpi gcc-libs\n# load GCC 10.2.0\nmodule load gcc-libs/10.2.0\nmodule load compilers/gnu/10.2.0\n</code></pre> <p>The <code>gcc-libs</code> module contains the actual compiler and libraries, while the <code>compilers/gnu</code>  module sets environment variables that are likely to be picked up by build systems, telling them what the C, C++ and Fortran compilers are called.</p> <p>The <code>compilers/intel</code> modules will also depend on a certain version of <code>gcc-libs</code>.</p>"},{"location":"Software_Guides/Installing_Software/#glibc-version-error","title":"GLIBC version error","text":"<p>If you get an error saying that a precompiled binary that you are installing needs a newer GLIBC (not GLIBCXX) then this has been compiled on a newer operating system and will not work on our clusters. Look for a binary that was created for CentOS 7 (we have RHEL 7) or build the program from source if possible.</p>"},{"location":"Software_Guides/Installing_Software/#build-systems","title":"Build systems","text":"<p>Most software will use some kind of build system to manage how files are compiled and linked and in what order. Here are a few common ones.</p>"},{"location":"Software_Guides/Installing_Software/#automake-configure","title":"Automake configure","text":"<p>Automake will generate the Makefile for you and hopefully pick up sensible options through configuration. You can give it an install prefix to tell it where to install (or you can build it in place and not use make install at all).</p> <pre><code>./configure\u00a0--prefix=/home/username/place/you/want/to/install\nmake\n#\u00a0if\u00a0it\u00a0has\u00a0a\u00a0test\u00a0suite,\u00a0good\u00a0idea\u00a0to\u00a0use\u00a0it\nmake\u00a0test\u00a0\nmake\u00a0install\n</code></pre> <p>If it has more configuration flags, you can use <code>./configure --help</code> to view them.</p> <p>Usually configure will create a config.log: you can look in there to find if any tests have failed or things you think should have been picked up haven't.</p>"},{"location":"Software_Guides/Installing_Software/#cmake","title":"CMake","text":"<p>CMake is another build system. It will have a CMakeFile or the instructions will ask you to use cmake or ccmake rather than make. It also generates Makefiles for you. <code>ccmake</code> is a terminal-based interactive interface where you can see what variables are set to and change them, then repeatedly configure until everything is correct, generate the Makefile and quit. <code>cmake</code> is the commandline version. The interactive process tends to go like this:</p> <pre><code>ccmake\u00a0CMakeLists.txt\n#\u00a0press\u00a0c\u00a0to\u00a0configure\u00a0-\u00a0will\u00a0pick\u00a0up\u00a0some\u00a0options\n#\u00a0press\u00a0t\u00a0to\u00a0toggle\u00a0advanced\u00a0options\n#\u00a0keep\u00a0making\u00a0changes\u00a0and\u00a0configuring\u00a0until\u00a0no\u00a0more\u00a0errors\u00a0or\u00a0changes\n#\u00a0press\u00a0g\u00a0to\u00a0generate\u00a0and\u00a0exit\nmake\n#\u00a0if\u00a0it\u00a0has\u00a0a\u00a0test\u00a0suite,\u00a0good\u00a0idea\u00a0to\u00a0use\u00a0it\nmake\u00a0test\u00a0\nmake\u00a0install\n</code></pre> <p>The options that you set using ccmake can also be passed on the commandline to cmake with <code>-D</code>. This allows you to script an install and run it again later. <code>CMAKE_INSTALL_PREFIX</code> is how you tell it where to install.</p> <pre><code># making a build directory allows you to clean it up more easily\nmkdir build\ncd build\ncmake .. -DCMAKE_INSTALL_PREFIX=/home/username/place/you/want/to/install\n</code></pre> <p>If you need to rerun cmake/ccmake and reconfigure, remember to delete the <code>CMakeCache.txt</code> file first or it will still use your old options. Turning on verbose Makefiles in cmake is also useful if your code didn't compile first time - you'll be able to see what flags the compiler or linker is actually being given when it fails.</p>"},{"location":"Software_Guides/Installing_Software/#make","title":"Make","text":"<p>Your code may come with a Makefile and have no configure, in which case the generic way to compile it is as follows:</p> <pre><code>make\u00a0targetname\n</code></pre> <p>There's usually a default target, which <code>make</code> on its own will use. <code>make all</code> is also frequently used.  If you need to change any configuration options, you'll need to edit those sections of the Makefile (usually near the top, where the variables/flags are defined).</p> <p>Here are some typical variables you may want to change in a Makefile.</p> <p>These are what compilers/mpi wrappers to use - these are also defined by the compiler modules, so you can see what they should be. Intel would be <code>icc</code>, <code>icpc</code>, <code>ifort</code>, while the GNU compiler would be <code>gcc</code>, <code>g++</code>, <code>gfortran</code>.  If this is a program that can be compiled using MPI and only has a variable for CC,  then set that to mpicc.</p> <pre><code>CC=gcc\nCXX=g++\nFC=gfortran\nMPICC=mpicc\nMPICXX=mpicxx\nMPIF90=mpif90\n</code></pre> <p>CFLAGS and LDFLAGS are flags for the compiler and linker respectively, and there might be LIBS or INCLUDE in the Makefile as well. When linking a library  with the name libfoo, use <code>-lfoo</code>.</p> <pre><code>CFLAGS=\"-I/path/to/include\"\nLDFLAGS=\"-L/path/to/foo/lib\u00a0-L/path/to/bar/lib\"\nLDLIBS=\"-lfoo\u00a0-lbar\"\n</code></pre> <p>Remember to <code>make clean</code> first if you are recompiling with new options. This will delete object files from previous attempts. </p>"},{"location":"Software_Guides/Installing_Software/#blas-and-lapack","title":"BLAS and LAPACK","text":"<p>BLAS and LAPACK are linear algebra libraries that are provided as part of MKL,  OpenBLAS or ATLAS. There are several different OpenBLAS and ATLAS modules for  different compilers. MKL is available as part of each Intel compiler module.</p> <p>Your code may try to link <code>-lblas -llapack</code>: this isn't the right way to use BLAS  and LAPACK with MKL or ATLAS (though our OpenBLAS now has symlinks that mean this  will work).</p>"},{"location":"Software_Guides/Installing_Software/#mkl","title":"MKL","text":"<p>When you have an Intel compiler module loaded, typing </p> <pre><code>echo $MKLROOT\n</code></pre> <p>will show you that MKL is available.</p>"},{"location":"Software_Guides/Installing_Software/#easy-linking-of-mkl","title":"Easy linking of MKL","text":"<p>If you can, try to use <code>-mkl</code> as a compiler flag - if that works, it should get  all the correct libraries linked in the right order. Some build systems do not  work with this however and need explicit linking.</p>"},{"location":"Software_Guides/Installing_Software/#intel-mkl-link-line-advisor","title":"Intel MKL link line advisor","text":"<p>It can be complicated to get the correct link line for MKL, so Intel has provided  a tool which will give you the link line with the libraries in the right order.</p> <ul> <li>https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor</li> </ul> <p>Pick the version of MKL you are using (for the Intel 2018 compiler it should be  Intel(R) MKL 2018.0), and these options:</p> <ul> <li>OS: Linux</li> <li>Pick your compiler. BLAS and LAPACK are Fortran95 interfaces, to select them pick a Fortran compiler.</li> <li>Architecture: Intel(R) 64</li> <li>You can choose what type of linking you prefer. Dynamic linking means the libraries are linked at runtime and use the .so library, while static means they are linked at compile time and use the .a library. The Single Dynamic Library for later MKL versions will mean MKL will do clever things to work out which parts of it you are using.</li> <li>Interface layer: 64-bit integer</li> <li>Threading layer: You probably want sequential threading in most cases.</li> <li>Select additional libraries (ScaLAPACK) if required.</li> <li>Select Intel MPI if required.</li> <li>Select 'Link with Intel MKL libraries explicitly' </li> </ul> <p>You'll get something like this:</p> <pre><code>${MKLROOT}/lib/intel64/libmkl_blas95_ilp64.a ${MKLROOT}/lib/intel64/libmkl_lapack95_ilp64.a -L${MKLROOT}/lib/intel64 -lmkl_scalapack_ilp64 -lmkl_intel_ilp64 -lmkl_sequential -lmkl_core -lmkl_blacs_intelmpi_ilp64 -lpthread -lm -ldl\n</code></pre> <p>and compiler options:</p> <pre><code>-i8 -I${MKLROOT}/include/intel64/ilp64 -I${MKLROOT}/include\n</code></pre> <p>It is a good idea to double check the library locations given by the tool are  correct: do an <code>ls ${MKLROOT}/lib/intel64</code> and make sure the directory exists  and contains the libraries. In the past there have been slight path differences  between tool and install for some versions.</p>"},{"location":"Software_Guides/Installing_Software/#openblas","title":"OpenBLAS","text":"<p>We have native threads, OpenMP and serial versions of OpenBLAS.  Type <code>module avail openblas</code> to see the available versions.</p>"},{"location":"Software_Guides/Installing_Software/#linking-openblas","title":"Linking OpenBLAS","text":"<p>Our OpenBLAS modules now contain symlinks for <code>libblas</code> and <code>liblapack</code> that both  point to <code>libopenblas</code>. This means that the default <code>-lblas -llapack</code> will in fact work.</p> <p>This is how you would normally link OpenBLAS:</p> <pre><code>-L${OPENBLASROOT}/lib -lopenblas\n</code></pre> <p>If code you are compiling requires separate entries for BLAS and LAPACK, set them  both to <code>-lopenblas</code>.</p>"},{"location":"Software_Guides/Installing_Software/#troubleshooting-openmp-loop-warning","title":"Troubleshooting: OpenMP loop warning","text":"<p>If you are running a threaded program and get this warning:</p> <pre><code>OpenBLAS Warning : Detect OpenMP Loop and this application may hang. Please rebuild the library with USE_OPENMP=1 option.\n</code></pre> <p>Then tell OpenBLAS to use only one thread by adding the below to your jobscript  (this overrides <code>$OMP_NUM_THREADS</code> for OpenBLAS only):</p> <pre><code>export OPENBLAS_NUM_THREADS=1\n</code></pre> <p>If it is your own code, you can also set it in the code with the function</p> <pre><code>void openblas_set_num_threads(int num_threads);\n</code></pre> <p>You can avoid this error by compiling with one of the <code>native-threads</code> or <code>serial</code>  OpenBLAS modules instead of the <code>openmp</code> one.</p>"},{"location":"Software_Guides/Installing_Software/#atlas","title":"ATLAS","text":"<p>We would generally recommend using OpenBLAS instead at present, but we do have  ATLAS modules.</p>"},{"location":"Software_Guides/Installing_Software/#dynamic-linking-atlas","title":"Dynamic linking ATLAS","text":"<p>There is one combined library each for serial and threaded ATLAS (in most  circumstances you probably want the serial version).</p> <p>Serial:</p> <pre><code>-L${ATLASROOT}/lib -lsatlas\n</code></pre> <p>Threaded:</p> <pre><code>-L${ATLASROOT}/lib -ltatlas\n</code></pre>"},{"location":"Software_Guides/Installing_Software/#static-linking-atlas","title":"Static linking ATLAS","text":"<p>There are multiple libraries to link.</p> <p>Serial:</p> <pre><code>-L${ATLASROOT}/lib -llapack -lf77blas -lcblas -latlas\n</code></pre> <p>Threaded:</p> <pre><code>-L${ATLASROOT}/lib -llapack -lptf77blas -lptcblas -latlas\n</code></pre>"},{"location":"Software_Guides/Installing_Software/#troubleshooting-libgfortran-or-lifcore-cannot-be-found","title":"Troubleshooting: libgfortran or lifcore cannot be found","text":"<p>If you get a runtime error saying that <code>libgfortran.so</code> cannot be found,  you need to add <code>-lgfortran</code> to your link line.</p> <p>The Intel equivalent is <code>-lifcore</code>.</p> <p>You can do a module show on the compiler module you are using to see where  the Fortran libraries are located if you need to give a full path to them.</p>"},{"location":"Software_Guides/Installing_Software/#installing-additional-packages-for-an-existing-scripting-language","title":"Installing additional packages for an existing scripting language","text":""},{"location":"Software_Guides/Installing_Software/#python","title":"Python","text":"<p>There are <code>python2/recommended</code> and <code>python3/recommended</code> module bundles you will see if you type  <code>module avail python</code>. These use a virtualenv, have a lot of Python packages installed already,  like numpy and scipy (see the Python package list)  and have <code>pip</code> set up for you.</p>"},{"location":"Software_Guides/Installing_Software/#load-the-gnu-compiler","title":"Load the GNU compiler","text":"<p>Our Python installs were built with GCC. You can run them without problems with the default Intel compilers loaded because it also depends on the <code>gcc-libs/4.9.2</code> module. However, when you are  installing your own Python packages you should make sure you have the GNU compiler module loaded. This is to avoid the situation where you build your package with the Intel compiler and then try to run it with our GNU-based Python. If it compiled any C code, it will be unable to find Intel-specific instructions and give you errors.</p> <p>Change your compiler module:</p> <pre><code>module unload compilers\nmodule load compilers/gnu/4.9.2\n</code></pre> <p>If you get an error like this when trying to run something, you built a package with the Intel compiler.</p> <pre><code>undefined symbol: __intel_sse2_strrchr\n</code></pre>"},{"location":"Software_Guides/Installing_Software/#install-your-own-packages-in-the-same-virtualenv","title":"Install your own packages in the same virtualenv","text":"<p>This will use our central virtualenv and the packages we have already installed.</p> <pre><code># for Python 2\npip install --user &lt;python2pkg&gt;\n# for Python 3\npip3 install --user &lt;python3pkg&gt;\n</code></pre> <p>These will install into <code>.python2local</code> or <code>.python3local</code> in your home directory. </p> <p>If your own installed Python packages get into a mess, you can delete (or rename) the whole  <code>.python3local</code> and start again.</p>"},{"location":"Software_Guides/Installing_Software/#using-your-own-virtualenv","title":"Using your own virtualenv","text":"<p>If you need different packages that are not compatible with the centrally installed versions (eg.  what you are trying to install depends on a different version of something we have already installed) then you can create a new virtualenv and only packages you are installing yourself will be in it.</p> <p>In this case, you do not want our virtualenv with our packages to also be active. We have two types of Python modules. If you type <code>module avail python</code> there are  \"bundles\" which are named like <code>python3/3.7</code> - these include our virtualenv and packages. Then there are the base modules for just python itself, like <code>python/3.7.4</code>. </p> <p>When using your own virtualenv, you want to load one of the base python modules.</p> <pre><code># load a base python module (you will always need to do this)\nmodule load python/3.7.4\n# create the new virtualenv, with any name you want\nvirtualenv &lt;DIR&gt;\n# activate it\nsource &lt;DIR&gt;/bin/activate\n</code></pre> <p>Your bash prompt will change to show you that a different virtualenv is active. (This one is called <code>venv</code>).</p> <pre><code>(venv) [uccacxx@login03 ~]$ \n</code></pre> <p><code>deactivate</code> will deactivate your virtualenv and your prompt will return to normal.</p> <p>You only need to create the virtualenv the first time. </p>"},{"location":"Software_Guides/Installing_Software/#error-while-loading-shared-libraries","title":"Error while loading shared libraries","text":"<p>You will always need to load the base python module before activating your virtualenv or you will get an error like this:</p> <pre><code>python3: error while loading shared libraries: libpython3.7m.so.1.0: cannot open shared object file: No such file or directory\n</code></pre>"},{"location":"Software_Guides/Installing_Software/#installing-via-setuppy","title":"Installing via setup.py","text":"<p>If you need to install by downloading a package and using <code>setup.py</code>, you can use the <code>--user</code>  flag and as long as one of our python module bundles are loaded, it will install into the same  <code>.python2local</code> or <code>.python3local</code> as <code>pip</code> does and your packages will be found automatically.</p> <pre><code>python setup.py install --user\n</code></pre> <p>If you want to install to a different directory in your space to keep this package separate, you can use <code>--prefix</code> instead. You'll need to add that location to your <code>$PYTHONPATH</code> and <code>$PATH</code> as well so it can be found. Some install methods won't create the prefix directory you requested for you automatically, so you would need to create it yourself first.</p> <p>This type of install makes it easier for you to only have this package in your paths when you want to use it, which is helpful if it conflicts with something else.</p> <pre><code># add location to PYTHONPATH so Python can find it\nexport PYTHONPATH=/home/username/your/path/lib/python3.7/site-packages:$PYTHONPATH\n# if necessary, create lib/pythonx.x/site-packages in your desired install location\nmkdir -p /home/username/your/path/lib/python3.7/site-packages\n# do the install\npython setup.py install --prefix=/home/username/your/path\n</code></pre> <p>It will tend to tell you at install time if you need to change or create the <code>$PYTHONPATH</code> directory.</p> <p>To use this package, you'll need to add it to your paths in your jobscript or <code>.bashrc</code>. Check that the <code>PATH</code> is where your Python executables were installed.</p> <pre><code>export PYTHONPATH=/home/username/your/path/lib/python3.7/site-packages:$PYTHONPATH\nexport PATH=/home/username/your/path/bin:$PATH\n</code></pre> <p>It is very important that you keep the <code>:$PYTHONPATH</code> or <code>:$PATH</code> at the end of these - you are putting your location at the front of the existing contents of the path. If you leave  them out, then only your package location will be found and nothing else.</p>"},{"location":"Software_Guides/Installing_Software/#troubleshooting-remove-your-pip-cache","title":"Troubleshooting: remove your pip cache","text":"<p>If you built something and it went wrong, and are trying to reinstall it with <code>pip</code> and keep  getting errors that you think you should have fixed, you may still be using a previous cached version.  The cache is in <code>.cache/pip</code> in your home directory, and you can delete it.</p> <p>You can prevent caching entirely by installing using <code>pip3 install --user --no-cache-dir &lt;python3pkg&gt;</code></p>"},{"location":"Software_Guides/Installing_Software/#troubleshooting-python-script-executable-paths","title":"Troubleshooting: Python script executable paths","text":"<p>If you have an executable Python script (eg. something you run using <code>pyutility</code> and not  <code>python pyutility.py</code>) that begins like this: </p> <pre><code>#!/usr/bin/python2.6\n</code></pre> <p>and fails because that Python doesn't exist in that location or isn't the one that has the  additional packages installed, then you should change it so it uses the first Python found  in your environment instead, which will be the one from the Python module you've loaded.</p> <pre><code>#!/usr/bin/env python\n</code></pre>"},{"location":"Software_Guides/Matlab/","title":"MATLAB","text":"<p>MATLAB is a numerical computing environment and proprietary programming language developed by MathWorks.</p> <p>Our MATLAB installs include all the toolboxes included in UCL's Total Academic Headcount-Campus licence plus the MATLAB Parallel Server. We also have the NAG toolbox for Matlab available.</p> <p>You can submit single node multi-threaded MATLAB jobs, single node jobs which use the Parallel Computing Toolbox and the MATLAB Parallel     Server (MPS) and MATLAB GPU jobs. Currently MATLAB jobs can only be run     on Myriad however we are working with MathWorks to allow the     submission of multi-node MPS jobs on Kathleen.</p> <p>You can also submit jobs to Myriad from MATLAB running on your own desktop or laptop.</p>"},{"location":"Software_Guides/Matlab/#setup","title":"Setup","text":"<p>You need to load MATLAB once from a login node before you can submit any jobs. This allows it to set up your <code>~/.matlab</code> directory as a symbolic link to <code>~/Scratch/.matlab</code> so that the compute nodes can write to it.</p> <pre><code># on a login node\nmodule load xorg-utils/X11R7.7\nmodule load matlab/full/r2021a/9.10\n</code></pre> <p>We have other versions of MATLAB installed. You can run <code>module avail matlab</code> to see all the available installed versions.</p>"},{"location":"Software_Guides/Matlab/#single-node-multi-threaded-batch-jobs","title":"Single node multi-threaded batch jobs","text":"<p>This is the simplest way to start using MATLAB on the cluster.</p> <p>You will need a .m file containing the MATLAB commands you want to carry out.</p> <p>Here is an example jobscript which you would submit using the <code>qsub</code> command, after you have loaded the MATLAB module once on a login node as mentioned in Setup:</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run a multi-threaded MATLAB job under SGE.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM per core. \n#$ -l mem=1G\n\n# Request 15 gigabytes of TMPDIR space (default is 10 GB)\n#$ -l tmpfs=15G\n\n# Request a number of threads (which will use that number of cores). \n# On Myriad you can set the number of threads to a maximum of 36. \n#$ -pe smp 36\n\n# Request one MATLAB licence - makes sure your job doesn't start \n# running until sufficient licenses are free.\n#$ -l matlab=1\n\n# Set the name of the job.\n#$ -N Matlab_multiThreadedJob1\n\n# Set the working directory to somewhere in your scratch space.\n# This is a necessary step as compute nodes cannot write to $HOME.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID.\n# This directory must already exist.\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/Matlab_examples\n\n# Your work should be done in $TMPDIR\ncd $TMPDIR\n\nmodule load xorg-utils/X11R7.7\nmodule load matlab/full/r2021a/9.10\n# outputs the modules you have loaded\nmodule list\n\n# Optional: copy your script and any other files into $TMPDIR.\n# This is only practical if you have a small number of files.\n# If you do not copy them in, you must always refer to them using a\n# full path so they can be found, eg. ~/Scratch/Matlab_examples/analyse.m\n\ncp ~/Scratch/Matlab_examples/myMatlabJob.m $TMPDIR\ncp ~/Scratch/Matlab_examples/initialise.m $TMPDIR\ncp ~/Scratch/Matlab_examples/analyse.m $TMPDIR\n\n# These echoes output what you are about to run\necho \"\"\necho \"Running matlab -nosplash -nodisplay &lt; myMatlabJob.m ...\"\necho \"\"\n\nmatlab -nosplash -nodesktop -nodisplay &lt; myMatlabJob.m\n# Or if you did not copy your files:\n# matlab -nosplash -nodesktop -nodisplay &lt; ~/Scratch/Matlab_examples/myMatlabJob.m\n\n# tar up all contents of $TMPDIR back into your space\ntar zcvf $HOME/Scratch/Matlab_examples/files_from_job_${JOB_ID}.tgz $TMPDIR\n\n# Make sure you have given enough time for the copy to complete!\n</code></pre> <p>Alternative syntax: Instead of using Unix input redirection like this:</p> <pre><code>matlab -nosplash -nodesktop -nodisplay &lt; $Matlab_infile\n</code></pre> <p>you can also do:</p> <pre><code>matlab -nosplash -nodesktop -nodisplay -r $Matlab_infile\n</code></pre> <p>NOTE: You should use the <code>-r</code> syntax if your input file contains function definitions as using <code>&lt;</code> treats the script as keyboard input and MATLAB does not allow you to define functions directly on the command line.</p>"},{"location":"Software_Guides/Matlab/#run-without-the-jvm-to-reduce-overhead","title":"Run without the JVM to reduce overhead","text":"<p>You can give the <code>-nojvm</code> option to tell MATLAB to run without the Java Virtual Machine. This will speed up startup time, possibly execution time, and remove some memory overhead, but will prevent you using any tools that require Java (eg, tools that use the Java API for I/O and networking like URLREAD, or call Java object methods). </p>"},{"location":"Software_Guides/Matlab/#run-single-threaded","title":"Run single-threaded","text":"<p>Most of the time, MATLAB will create many threads and use them as it wishes. If you know your job is entirely single-threaded, you can force MATLAB to run with only one thread on one core, which will allow you to have more jobs running at once. </p> <p>To request one core only, set <code>#$ -pe smp 1</code> in your jobscript.</p> <p>Run MATLAB like this:</p> <pre><code>matlab -nosplash -nodesktop -nodisplay -nojvm -singleCompThread &lt; $Matlab_infile\n</code></pre> <p>The <code>-singleCompThread</code> forces MATLAB to run single-threaded, and the <code>-nojvm</code> tells it to run without the Java Virtual Machine, as above. </p>"},{"location":"Software_Guides/Matlab/#using-the-matlab-gui-interactively","title":"Using the MATLAB GUI interactively","text":"<p>You can run MATLAB interactively for short amounts of time on the login nodes (please do not do this if your work will be resource-intensive). You can also run it interactively in a <code>qrsh</code> session on the compute nodes.</p> <p>Launching with <code>matlab</code> will give you the full graphical user interface - you will need to have logged in to the cluster with X-forwarding on for this to work. </p> <p>Launching with <code>matlab -nodesktop -nodisplay</code> will give you the MATLAB terminal.</p>"},{"location":"Software_Guides/Matlab/#submitting-jobs-using-the-matlab-parallel-server-mps","title":"Submitting jobs using the MATLAB Parallel Server (MPS)","text":"<p>You must have loaded the MATLAB module once from a login node as described in Setup before you can submit any MATLAB MPS jobs.</p> <p>MATLAB PCS jobs must be submitted from an interactive or scripted Matlab session which can be running on the cluster login nodes, from another MATLAB job or on your own machine.</p> <p>MATLAB MPS jobs will currently only work inside a single node on our clusters. On Myriad this means a maximum of 36 workers can be used per job.</p>"},{"location":"Software_Guides/Matlab/#importing-the-cluster-profile","title":"Importing the cluster profile","text":"<p>You need to import the cluster profile into your MATLAB environment and set it as the default before you can submit DCS jobs. This only needs doing once. The imported profile will be saved in your MATLAB settings directory.</p> <p>Importing the profile can be done either by calling MATLAB functions or via the graphical interface. The profile is stored here (for R2021a): </p> <pre><code>/shared/ucl/apps/Matlab/toolbox_local/R2021a/myriad_R2021a.mlsettings\n</code></pre>"},{"location":"Software_Guides/Matlab/#import-using-matlab-functions","title":"Import using MATLAB functions","text":"<p>Run these functions from a MATLAB session:</p> <pre><code>profile_Myriad = parallel.importProfile ('/shared/ucl/apps/Matlab/toolbox_local/R2021a/myriad_R2021a.mlsettings');\nparallel.defaultClusterProfile ('myriad_R2021a');\n</code></pre>"},{"location":"Software_Guides/Matlab/#import-from-matlab-gui","title":"Import from MATLAB GUI","text":"<p>To import using the graphical interface instead, do this.</p> <ol> <li>From the Home tab select the Parallel menu and click Create and Manage Clusters....</li> <li>The Cluster Profile Manager window will open. </li> <li>Select Import and in the Import Profiles from file window navigate to the myriad_R2021a.mlsettings file shown above and select Open.</li> <li>Select the resulting myriad_R2021a profile and click Set as Default. The Cluster Profile Manager window should now look like this: </li> </ol> <p>In both cases after you exit MATLAB your cluster profile is saved for future use.</p>"},{"location":"Software_Guides/Matlab/#environment-variables-needed-for-job-submission","title":"Environment variables needed for job submission","text":"<p>We have set up four Grid Engine environment variables to assist with job submission from within MATLAB. These are needed to pass in job resource parameters that aren't supported by the internal MATLAB job submission mechanism.</p> <ul> <li><code>SGE_CONTEXT</code>: a comma-separated list of variables treated as if added via the <code>-ac</code> option, eg. <code>exclusive</code></li> <li><code>SGE_OPT</code>: a comma-separated list of resources treated as if added via the <code>-l</code> option, eg. <code>h_rt=0:10:0,mem=1G,tmpfs=15G</code></li> </ul> <p>and two project and Gold related variables. Most users will not need to use either of these:</p> <ul> <li><code>SGE_PROJECT</code>: a project treated as if added via the <code>-P</code> option.</li> <li><code>SGE_ACCOUNT</code>: a Gold project as if added via the <code>-A</code> option. When using this your <code>SGE_PROJECT</code> needs to be set to <code>Gold</code>.</li> </ul> <p><code>-ac exclusive</code> prevents anything else running on the same node as your job, even if you aren't using all the cores. This is no longer a necessary option for MATLAB jobs.</p> <p>There are two ways to set these:</p> <p>1) Before starting your MATLAB session, using the usual Bash method of exporting environment variables:</p> <pre><code>export SGE_CONTEXT=exclusive\nexport SGE_OPT=h_rt=0:15:0,mem=2G,tmpfs=15G\nexport SGE_PROJECT=&lt;your project ID&gt;\nexport SGE_ACCOUNT=&lt;your Gold project&gt;\n</code></pre> <p>2) Inside your MATLAB session, using MATLAB's <code>setenv</code> function:</p> <pre><code>setenv ('SGE_CONTEXT', 'exclusive');\nsetenv ('SGE_OPT', 'h_rt=0:15:0,mem=2G,tmpfs=15G'); \nsetenv ('SGE_PROJECT', '&lt;your project ID&gt;');\nsetenv ('SGE_ACCOUNT', '&lt;your Gold project&gt;');\n</code></pre>"},{"location":"Software_Guides/Matlab/#example-a-simple-mps-job","title":"Example: a simple MPS job","text":"<p>This submits a job from inside a MATLAB session running on a login node. You need to start MATLAB from a directory in Scratch - jobs will inherit this as their working directory.</p> <p>This is an example where you have only one MATLAB source file.</p> <p>1) Change to the directory in Scratch you want the job to run from and set the SGE environment variables.</p> <pre><code>cd ~/Scratch/Matlab_examples\nexport SGE_OPT=h_rt=0:10:0,mem=2G,tmpfs=15G\n</code></pre> <p>2) Either start the MATLAB GUI:</p> <pre><code>matlab\n</code></pre> <p>or start a MATLAB terminal session:</p> <pre><code>matlab -nodesktop -nodisplay\n</code></pre> <p>3) Inside MATLAB, create a cluster object using the cluster profile:</p> <pre><code>c = parcluster ('myriad_R2021a');\n</code></pre> <p>4) Use your cluster object to create a job object of the type you need. For this example the job is a parallel job with communication between MATLAB workers of type \"Single Program Multiple Data\":</p> <pre><code>myJob = createCommunicatingJob (c, 'Type', 'SPMD');\n</code></pre> <p>5) Set the number of workers:</p> <pre><code>num_workers = 8;\n</code></pre> <p>The maximum value you can set here on Myriad is 36.</p> <p>6) Tell the job the files needed to be made available to each worker - in this example there is only one file:</p> <pre><code>myJob.AttachedFiles = {'colsum.m'};\n</code></pre> <p><code>colsum.m</code> contains the simple magic square example from the MATLAB manual \"Parallel Computing Toolbox User's Guide\". </p> <p>7) Set the minimum and maximum number of workers for the job (we are asking for an exact number here by setting them the same):</p> <pre><code>myJob.NumWorkersRange = [num_workers, num_workers];\n</code></pre> <p>8) Create a MATLAB task to be executed as part of the job. Here it consists of executing the MATLAB function <code>colsum</code>. The other arguments say that the task returns one parameter and there are no input arguments to the <code>colsum</code> function:</p> <pre><code>task = createTask (myJob, @colsum, 1, {});\n</code></pre> <p>9) Submit the job:</p> <pre><code>submit (myJob);\n</code></pre> <p>Your job is now submitted to the scheduler and you can see its queue status in <code>qstat</code> as normal. If you were using the MATLAB GUI you can also monitor jobs by selecting Monitor Jobs from the Parallel menu on the Home tab.</p> <p>10) When the job has completed get the results using:</p> <pre><code>results = fetchOutputs(myJob)\n</code></pre> <p>You can access the job log from MATLAB using:</p> <pre><code>logMess = getDebugLog (c, myJob);\n</code></pre>"},{"location":"Software_Guides/Matlab/#example-a-mps-job-with-more-than-one-input-file","title":"Example: a MPS job with more than one input file","text":"<p>This example has several input files. The job type is \"MATLAB Pool\". A \"Pool\" job runs the specified task function with a MATLAB pool available to run the body of parfor loops or spmd blocks and is the default job type. This example was kindly supplied to assist in testing MATLAB by colleagues from CoMPLEX. </p> <p>The first part of creating the job is the same as the above example apart from the longer runtime and larger amount of memory per core: </p> <p>1)  Change into a directory in Scratch, set the SGE variables and launch MATLAB:</p> <pre><code>cd ~/Scratch/Matlab_examples\nexport SGE_OPT=h_rt=1:0:0,mem=4G,tmpfs=15G\nmatlab\n</code></pre> <p>to launch the GUI or:</p> <pre><code>matlab -nodesktop -nodisplay\n</code></pre> <p>to start a terminal session. </p> <pre><code>c = parcluster ('myriad_R2021a');\n</code></pre> <p>2) Using our cluster object create a job object of type \"Pool\":</p> <pre><code>myJob2 = createCommunicatingJob (c, 'Type', 'Pool');\n</code></pre> <p>3) Set the number of workers and another variable used by the example:</p> <pre><code>num_workers = 8;\nsimulation_duration_ms = 1000;\n</code></pre> <p>4) Tell the job all the input files needed to be made available to each worker as a cell array:</p> <pre><code>myJob2.AttachedFiles = {\n'AssemblyFiniteDifferencesMatrix.m'\n'AssemblyFiniteDifferencesRightHandSide.m'\n'CellModelsComputeIonicCurrents.m'\n'CellModelsGetVoltage.m'\n'CellModelsInitialise.m'\n'CellModelsSetVoltage.m'\n'GetStimuliForTimeStep.m'\n'SubmitMonodomainJob.m'\n'RegressionTest.m'\n'RunAndVisualiseMonodomainSimulation.m'\n'SolveLinearSystem.m'\n'luo_rudy_1991_iionic.m'\n'luo_rudy_1991_time_deriv.m'};\n</code></pre> <p>5) Set the minimum and maximum number of workers for the job:</p> <pre><code>myJob2.NumWorkersRange = [num_workers, num_workers];\n</code></pre> <p>6) Create a MATLAB task to be executed as part of the job. For this example it will consist of executing the MATLAB function <code>RunAndVisualiseMonodomainSimulation</code>. The rest of the arguments indicate that the task returns three parameters and there are five input arguments to the function. These are passed as a cell array:</p> <pre><code>task = createTask (myJob2, @RunAndVisualiseMonodomainSimulation, 3, {5000, simulation_duration_ms, 1.4, 1.4, false});\n</code></pre> <p>7) Submit the job:</p> <pre><code>submit (myJob2);\n</code></pre> <p>As before use <code>fetchOutputs</code> to collect the results.</p> <p>If you closed your session, you can get your results by:</p> <pre><code>c = parcluster ('myriad_R2021a');                  # get a cluster object\njobs = findJob(c)                                  # get a list of jobs submitted to that cluster\njob = jobs(3);                                     # pick a particular job\nresults = fetchOutputs(job)\n</code></pre> <p>You can get other information: <code>diary(job)</code> will give you the job diary, and <code>load(job)</code> will load the workspace. </p>"},{"location":"Software_Guides/Matlab/#further-reading","title":"Further reading","text":"<p>There is a lot more information about using the MATLAB Distributed Computing Server in the MATLAB manual: Parallel Computing Toolbox User\u2019s Guide.</p>"},{"location":"Software_Guides/Matlab/#submitting-matlab-jobs-from-your-workstationlaptop","title":"Submitting MATLAB jobs from your workstation/laptop","text":"<p>You can submit MATLAB jobs to Myriad from MATLAB sessions running on your own desktop workstation or laptop systems provided they are running the same version of MATLAB and your computer is within the UCL firewall. </p> <p>With MATLAB R2021a you can currently submit jobs to Myriad. Support for R2018b is also still available. </p>"},{"location":"Software_Guides/Matlab/#prerequisites","title":"Prerequisites","text":"<ol> <li>You must already have an account on the clusters!</li> <li>Have MATLAB R2021a (or R2018b) installed on your local workstation/laptop. The local version must match the version running jobs. MATLAB R2021a can be downloaded from the UCL Software Database.</li> <li>Your local workstation/laptop installation of MATLAB must include the Parallel Computing toolbox. This is included in the UCL TAH MATLAB license and may be installed automatically.  Home tab &gt; Add-Ons &gt; Get Add-Ons  You will find the Parallel Computing Toolbox in the Workflows category.</li> <li>If your local workstation/laptop is not directly connected to the UCL network (at home for example), you need to have the UCL VPN client installed and running on it.</li> </ol>"},{"location":"Software_Guides/Matlab/#remote-setup","title":"Remote setup","text":"<p>1) On the cluster you are using (Myriad in this case) create a directory to hold remotely submitted job details. For example:</p> <pre><code>mkdir ~/Scratch/Matlab_remote_jobs\n</code></pre> <p>This directory needs to be in your Scratch directory as compute nodes need to be able to write to it. You should not use this directory for anything else.</p> <p>2) On your local workstation/laptop create a directory to hold information about jobs that have been submitted to the cluster. Again you should not use this directory for anything else.</p> <p>3) Download either the the support files for remote submission to Myriad for R2021a or support files for remote submission to Myriad for R2018b. Make sure you download the correct one for your version of MATLAB!</p> <p>4) This step MUST be done while Matlab is shut down. Unzip the archive into MATLAB's local toolbox directory. Default locations for the local toolbox directory are:</p> <ul> <li>Linux: The default local toolbox location is <code>/usr/local/MATLAB/R2021a/toolbox/local</code> for R2021a. Navigate to this directory and use <code>unzip -x archive_name</code>. </li> <li>macOS: The default local toolbox location is <code>/Applications/MATLAB_R2021a.app/toolbox/local</code> for R2021a. In order to view or change the contents of an application package, open <code>/Applications</code> in a Finder window. Then right-click the application and select \"View Package Contents.\" Then navigate to the appropriate directory. Note: if you don't have access to <code>/Applications/MATLAB_R2021a.app/toolbox/local</code>, you can unzip the support files into <code>~/Documents/MATLAB/</code> instead. </li> <li>Windows: The default local toolbox location is <code>C:\\Program Files\\MATLAB\\R2021a\\toolbox\\local</code> for R2021a. Extract the archive here. You can unzip the support files into <code>Documents\\MATLAB\\</code> instead.</li> </ul> <p>5) Download the parallelProfileMyriad function  to your local workstation/laptop. It will need to be unzipped. This function create a cluster profile for Myriad for R2021a or R2018b.</p> <p>6) Start MATLAB, navigate to where you saved the <code>parallelProfileMyriad.m</code> file and run the function by typing:</p> <pre><code>parallelProfileMyriad\n</code></pre> <p>at your MATLAB prompt (in your MATLAB Command Window if running the MATLAB GUI) and answer the questions.</p>"},{"location":"Software_Guides/Matlab/#submitting-a-job-to-the-cluster","title":"Submitting a job to the cluster","text":"<p>1) You need to set the Grid Engine support environment variables on your local computer. Eg. in your MATLAB session set:</p> <pre><code>setenv ('SGE_CONTEXT', 'exclusive');                # optional\nsetenv ('SGE_OPT', 'h_rt=0:15:0,mem=2G,tmpfs=15G'); \nsetenv ('SGE_PROJECT', '&lt;your project ID&gt;');        # optional\nsetenv ('SGE_ACCOUNT', '&lt;your Gold project&gt;');      # optional\n</code></pre> <p>2) In your MATLAB session create a cluster object using the cluster profile created by the <code>parallelProfile...</code> functions. For Myriad:</p> <pre><code>c = parcluster ('myriad_R2021a');\n</code></pre> <p>3) You can now create and submit jobs in a similar way to that shown in the MPS examples above starting from step 4 in the simple MPS job example or step 2 in the MPS job with multiple input files example.</p>"},{"location":"Software_Guides/Matlab/#viewing-your-results","title":"Viewing your results","text":"<p>After submitting your job remotely from your desktop, you can close MATLAB and come back later. To see your jobs:</p> <p>Click \"Parallel &gt; Monitor jobs\"</p> <p>This will bring up the job monitor where you can see the status of your jobs and whether they are finished. MATLAB numbers the jobs sequentially.</p> <p>Right-click on a job and choose \"fetch outputs\".</p> <p>This is what will be executed (for job4 on Myriad):</p> <pre><code>myCluster = parcluster('myriad_R2021a');\njob4 = myCluster.findJob('ID',4);\njob4_output = fetchOutputs(job4);\n</code></pre> <p>The Workspace will show the available data and you can view your results. The data is fetched from the <code>Matlab_remote_jobs</code> directory you created on Myriad (or Legion) in Remote setup step 1, so that will also have files and directories in it called job1, job2 and so on.</p> <p>If you have already fetched the data, you can view the results straight away by selecting that job. If you need to reload everything, you can right-click on the job and the option will be to load variables instead. </p>"},{"location":"Software_Guides/Matlab/#writing-intermediate-results","title":"Writing intermediate results","text":"<p>If you want to explicitly write out intermediate results, you need to provide a full path to somewhere in Scratch otherwise MATLAB will try to write them in your home, which isn't writable by the compute nodes. </p>"},{"location":"Software_Guides/Matlab/#troubleshooting-remote-jobs","title":"Troubleshooting remote jobs","text":"<p>If you get a message like this when retrieving your outputs then something has gone wrong in your job:</p> <pre><code>Task with ID xxx returned 0 outputs but 1 were expected\n</code></pre> <p>You need to retrieve the debug log to find out what happened. Example:</p> <pre><code>myCluster = parcluster('myriad_R2021a');\njob4 = myCluster.findJob('ID',4);\n\njobLog = getDebugLog (myCluster, job4);\njobLog\n</code></pre> <p>There will be a lot of output. Look for lines related to errors happening in your own code.</p>"},{"location":"Software_Guides/Matlab/#running-matlab-on-gpus","title":"Running MATLAB on GPUs","text":"<p>This uses MATLAB's Mandelbrot Set GPU example.</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run a GPU MATLAB job on Myriad.\n\n# Request 15 minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:15:0\n\n# Request 2 gigabytes of RAM per core.\n#$ -l mem=2G\n\n# Request 15 gigabytes of TMPDIR space (default is 10 GB)\n#$ -l tmpfs=15G\n\n# Request 1 GPU\n#$ -l gpu=1\n\n# Request 1 CPU core. (Max on Myriad is 36)\n#$ -pe smp 1\n\n# Request one MATLAB licence - makes sure your job doesn't start \n# running until sufficient licenses are free.\n#$ -l matlab=1\n\n# Set the name of the job.\n#$ -N Matlab_GPU_Job1\n\n# Set the working directory to somewhere in your scratch space.\n# This is a necessary step as compute nodes cannot write to $HOME.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID.\n# This directory must already exist.\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/Matlab_examples\n\n# Your work should be done in $TMPDIR\ncd $TMPDIR\n\n# Optional: Copy your script and any other files into $TMPDIR.\n# If not, you must always refer to them using a full path.\ncp /home/ccaabaa/Software/Matlab/Mandelbrot_GPU.m $TMPDIR\n\nmodule unload compilers mpi\nmodule load compilers/gnu/4.9.2\nmodule load xorg-utils/X11R7.7\nmodule load matlab/full/r2021a/9.10\nmodule list\n\n# These echoes output what you are about to run\necho \"\"\necho \"Running matlab -nosplash -nodisplay &lt; Mandelbrot_GPU.m ...\"\necho \"\"\nmatlab -nosplash -nodesktop -nodisplay &lt; Mandelbrot_GPU.m\n\n# tar up all contents of $TMPDIR back into your space\ntar zcvf $HOME/Scratch/Matlab_examples/files_from_job_${JOB_ID}.tgz $TMPDIR\n\n# Make sure you have given enough time for the copy to complete!\n</code></pre>"},{"location":"Software_Guides/Other_Software/","title":"Other Software","text":"<p>We maintain a large software stack that is available across all our clusters (licenses permitting).</p> <p>We use environment modules to let you manage which specific versions of software packages you are using. </p>"},{"location":"Software_Guides/Other_Software/#general-use-of-environment-modules","title":"General use of environment modules","text":"<p>We have a default set of modules that everyone has loaded when they log in: these include the current default compiler and MPI, some utili ties to make your life easier and some text editors.</p>"},{"location":"Software_Guides/Other_Software/#summary-of-module-commands","title":"Summary of module commands","text":"<pre><code>module avail            # shows available modules\nmodule whatis           # shows available modules with brief explanations\nmodule list             # shows your currently loaded modules\n\nmodule load &lt;module&gt;    # load this module\nmodule unload &lt;module&gt;  # unload this module\nmodule purge            # unload all modules\n\nmodule show &lt;module&gt;    # Shows what the module requires and what it sets up\nmodule help &lt;module&gt;    # Shows a longer text description for the software\n</code></pre>"},{"location":"Software_Guides/Other_Software/#find-out-if-a-software-package-is-installed-and-load-it","title":"Find out if a software package is installed and load it","text":"<p>Generically, the way you find out if a piece of software is installed is to run</p> <pre><code>module avail packagename\n</code></pre> <p>The output will also contain newer versions of GCC and the software that has been built using them.</p> <p>Then <code>module avail</code> gives you a list of all the modules we have that match the name you searched  for. You can then type </p> <pre><code>module show packagename\n</code></pre> <p>and it will show you the other software dependencies this module has: these have to be loaded first. It also shows where the software is installed and what environment variables it sets up.</p> <p>Once you have found the modules you want to load, it is good practice to refer to them using their full name, including the version. If you use the short form (<code>package</code> rather than <code>package/5.1.2/gnu-4.9.2</code>) then a matching module will be loaded, but if we install a different version, your jobs may begin using the new one and you would not know which version created your results. Different software versions may not be compatible or may have different default settings, so this is undesirable.</p> <p>You may need to unload current modules in order to load some requirements (eg different compiler, different MPI).</p> <p>This example switches from Intel compiler and MPI modules to GNU ones.</p> <pre><code>module unload -f compilers mpi\nmodule load compilers/gnu/4.9.2\nmodule load mpi/openmpi/3.1.4/gnu-4.9.2\n</code></pre> <p>You can use the short name when unloading things because there is usually only one match in your current modules.</p> <p>The last part of a module name usually tells you what compiler it was built with and which version  of that compiler. There may be GNU compiler versions and Intel compiler versions of the same  software available.</p> <p>Once the module is loaded, you should have all the usual executables in your path, and can use its commands. You load modules in exactly the same way inside a jobscript.</p> <p>Useful resources:</p> <ul> <li>Modules pt 1 (moodle) (UCL users)</li> <li>Modules pt 2 (moodle) (UCL users)</li> <li>Modules pt 1 (mediacentral) (non-UCL users)</li> <li>Modules pt 2 (mediacentral) (non-UCL users)</li> </ul>"},{"location":"Software_Guides/Other_Software/#notes-on-how-to-run-specific-packages","title":"Notes on how to run specific packages","text":"<p>The packages below have slightly complex commands needed to run them, or different settings needed on our clusters. These are examples of what should be added to your jobscripts. Change the module load command to the version you want to load and check that the dependencies are the same.</p> <p>The top of a jobscript should contain your resource requests. See also examples of full jobscripts .</p>"},{"location":"Software_Guides/Other_Software/#abaqus","title":"ABAQUS","text":"<p>ABAQUS is a commercial software suite for finite element analysis and computer-aided engineering.</p> <p>You must be authorised by the Mech Eng Department before you can be added to the group controlling access to ABAQUS (legabq).</p> <p>A serial interactive analysis can be run on the compute nodes (via a <code>qrsh</code> session) like this:</p> <pre><code>abaqus interactive job=myJobSerial input=myInputFile.inp\n</code></pre> <p>A parallel job can be run like this (fill in your own username):</p> <pre><code>module load abaqus/2017\n\nINPUT_FILE=/home/&lt;username&gt;/ABAQUS/heattransfermanifold_cavity_parallel.inp\nABAQUS_ARGS=\nABAQUS_PARALLELSCRATCH=/home/&lt;username&gt;/Scratch/Abaqus/parallelscratch\n# creates a parallel scratch dir and a new working dir for this job\nmkdir -p $ABAQUS_PARALLELSCRATCH\nmkdir -p $JOB_NAME.$JOB_ID\ncd $JOB_NAME.$JOB_ID\ncp $INPUT_FILE .\n\nINPUT=$(basename $INPUT_FILE)\nabaqus interactive cpus=$NSLOTS mp_mode=mpi job=$INPUT.$JOB_ID input=$INPUT \\\n       scratch=$ABAQUS_PARALLELSCRATCH $ABAQUS_ARGS\n</code></pre>"},{"location":"Software_Guides/Other_Software/#beast","title":"BEAST","text":"<p>BEAST is an application for Bayesian MCMC analysis of molecular sequences orientated towards rooted, time-measured phylogenies inferred using strict or relaxed molecular clock models.</p> <p>Note that FigTree and Tracer are available as standalone modules. The addons DISSECT, MODEL_SELECTION, and SNAPP are installed for BEAST. </p> <pre><code>cd $TMPDIR\n\nmodule load java/1.8.0_45\nmodule load beast/2.3.0\n\nbeast -threads $OMP_NUM_THREADS ~/Scratch/BEAST/gopher.xml\n\n# tar up all contents of $TMPDIR back into your space\ntar zcvf $HOME/Scratch/BEAST/files_from_job_$JOB_ID.tar.gz $TMPDIR\n</code></pre>"},{"location":"Software_Guides/Other_Software/#bowtie","title":"Bowtie","text":"<p>Bowtie 1 and 2 are tools for aligning sequencing reads to their reference sequences.</p> <p>Bowtie 1 and 2 are available. For reads longer than about 50 bp Bowtie 2 is generally faster, more sensitive, and uses less memory than Bowtie 1. For relatively short reads (e.g. less than 50 bp) Bowtie 1 is sometimes faster and/or more sensitive. For further differences, see How is Bowtie 2 different from Bowtie 1?.</p> <p>Bowtie sets <code>$BT1_HOME</code> and Bowtie2 sets <code>$BT2_HOME</code>. You can have both modules loaded at once. </p> <pre><code>cd $TMPDIR\nmodule load bowtie2/2.2.5\n\n# Run Bowtie2 example from getting started guide:\n# http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#getting-started-with-bowtie-2-lambda-phage-example\nbowtie2-build $BT2_HOME/example/reference/lambda_virus.fa lambda_virus\nbowtie2 -x lambda_virus -U $BT2_HOME/example/reads/reads_1.fq -S eg1.sam\n\n# tar up all contents of $TMPDIR back into your space\ntar zcvf $HOME/Scratch/Bowtie2_output/files_from_job_$JOB_ID.tgz $TMPDIR\n</code></pre>"},{"location":"Software_Guides/Other_Software/#castep","title":"CASTEP","text":"<p>CASTEP is a full-featured materials modelling code based on a first-principles quantum mechanical description of electrons and nuclei.</p> <pre><code>module load castep/17.21/intel-2017\n# Gerun is our mpirun wrapper which sets the machinefile and number of \n# processes to the amount you requested with -pe mpi.\ngerun castep.mpi input\n</code></pre> <p>CASTEP 19 has different pre-reqs:</p> <pre><code>module unload -f compilers mpi\nmodule load compilers/intel/2019/update4\nmodule load mpi/intel/2019/update4/intel\nmodule load castep/19.1.1/intel-2019\n# Gerun is our mpirun wrapper which sets the machinefile and number of \n# processes to the amount you requested with -pe mpi.\ngerun castep.mpi input\n</code></pre> <p>If building your own CASTEP, for version 23 onwards module load the newest <code>cmake</code> module we have and use the  newer cmake build system rather than make.</p>"},{"location":"Software_Guides/Other_Software/#cctools","title":"Cctools","text":"<p>Provides the Parrot connector to CVMFS, the CernVM File System. </p> <p>By default, the cctools module sets the following:</p> <pre><code>export PARROT_CVMFS_REPO=&lt;default-repositories&gt; \nexport PARROT_ALLOW_SWITCHING_CVMFS_REPOSITORIES=yes \nexport HTTP_PROXY=DIRECT;\nexport PARROT_HTTP_PROXY=DIRECT;\n</code></pre> <p>Example usage - will list the contents of the repository then exit:</p> <pre><code>module load cctools/7.0.11/gnu-4.9.2\nparrot_run bash\nls /cvmfs/alice.cern.ch\nexit\n</code></pre> <p>That will create the cache in <code>/tmp/parrot.xxxxx</code> on the login nodes when run interactively. To use in a job, you will want to put the cache somewhere in your space that the compute nodes can access. You can set the cache to be in your Scratch, or to <code>$TMPDIR</code> on the nodes if it just needs to exist for the duration of that job.</p> <pre><code>export PARROT_CVMFS_ALIEN_CACHE=&lt;/path/to/cache&gt;\n</code></pre>"},{"location":"Software_Guides/Other_Software/#cfd-ace","title":"CFD-ACE","text":"<p>CFD-ACE+ is a commercial computational fluid dynamics solver developed by ESI Group. It solves the conservation equations of mass, momentum, energy, chemical species and other scalar transport equations using the finite volume method. These equations enable coupled simulations of fluid, thermal, chemical, biological, electrical and mechanical phenomena.</p> <p>The license is owned by the Department of Mechanical Engineering who must give permission for users to be added to the group <code>lgcfdace</code>.</p> <pre><code>module load cfd-ace/2018.0\n\nCFD-SOLVER -model 3Dstepchannel_060414.DTF -num $NSLOTS -wd `pwd` \\ \n   -hosts $TMPDIR/machines -rsh=ssh -decomp -metis -sim 1 -platformmpi -job\n</code></pre>"},{"location":"Software_Guides/Other_Software/#comsol","title":"COMSOL","text":"<p>COMSOL Multiphysics is a cross-platform finite element analysis, solver and multiphysics simulation software.</p> <p>Electrical Engineering have a group license for version 52 and must give permission for users to be added to the group <code>legcomsl</code>. Chemical Engineering have a Departmental License for version 53 and members of that department may be added to the group <code>lgcomsol</code>. </p> <pre><code># Run a parallel COMSOL job\n\n# Versions 52 and 52a have this additional module prerequisite\nmodule load xulrunner/3.6.28/gnu-4.9.2\n\n# pick the version to load\nmodule load comsol/53a\n\n# Parallel multinode options:\n# $NHOSTS gets the number of nodes the job is running on and\n# $TMPDIR/machines is the machinefile that tells it which nodes.\n# These are automatically set up in a \"-pe mpi\" job environment.\ncomsol -nn $NHOSTS -clustersimple batch -f $TMPDIR/machines -inputfile micromixer_batch.mph \\ \n       -outputfile micromixer_batch_output_${JOB_ID}.mph\n\n# On Myriad you need to specify the fabric:\ncomsol batch -f $TMPDIR/machines -np $NSLOTS -mpifabrics shm:tcp \\ \n    -inputfile micromixer_batch.mph -outputfile micromixer_batch_output_${JOB_ID}.mph\n</code></pre>"},{"location":"Software_Guides/Other_Software/#cp2k","title":"CP2K","text":"<p>CP2K performs atomistic and molecular simulations.</p> <p>To see all available versions type</p> <pre><code>module avail cp2k\n</code></pre> <p>To load CP2K 8.2:</p> <pre><code>module unload compilers mpi gcc-libs\nmodule load gcc-libs/10.2.0\nmodule load compilers/gnu/10.2.0\n\n# These three modules only needed on Myriad\nmodule load numactl/2.0.12\nmodule load binutils/2.36.1/gnu-10.2.0\nmodule load ucx/1.9.0/gnu-10.2.0\n\nmodule load mpi/openmpi/4.0.5/gnu-10.2.0\nmodule load openblas/0.3.13-openmp/gnu-10.2.0\nmodule load cp2k/8.2/ompi/gnu-10.2.0\n\n# Gerun is our mpirun wrapper which sets the machinefile and number of \n# processes to the amount you requested with -pe mpi.\ngerun cp2k.popt &lt; input.in &gt; output.out\n</code></pre>"},{"location":"Software_Guides/Other_Software/#crystal","title":"CRYSTAL","text":"<p>CRYSTAL is a general-purpose program for the study of crystalline solids. The CRYSTAL program computes the electronic structure of periodic systems within Hartree Fock, density functional or various hybrid approximations. </p> <p>CRYSTAL is commercial software which is available free of charge to UK academics. You must obtain a license from Crystal Solutions: How to get CRYSTAL - Academic UK license. You need to create an account and then request to be upgraded to Academic UK. Access to CRYSTAL is enabled by being a member of the reserved application group <code>legcryst</code>. For proof of access we accept emails from CRYSTAL saying your account has been upgraded to \"Academic UK\", or a screenshot of your account page showing you have the full download available rather than just the demo version. </p> <pre><code>module unload -f mpi\nmodule load mpi/openmpi/2.1.2/intel-2017\nmodule load crystal17/v1.0.1\n\n# 9. Create a directory for this job and copy the input file into it. \nmkdir test00\ncd test00\ncp ~/Scratch/Crystal17/test_cases/inputs/test00.d12 INPUT\n\n# Gerun is our mpirun wrapper which sets the machinefile and number of\n# processes to the amount you requested with -pe mpi.\n# The CRYSTAL module sets $CRYxx_EXEDIR and $VERSION environment variables.\ngerun $CRY17_EXEDIR/$VERSION/Pcrystal\n</code></pre> <pre><code># Similarly, for Pproperties the command would be\ngerun $CRY17_EXEDIR/$VERSION/Pproperties\n</code></pre> <p>For CRYSTAL 17 v1.0.2, the modules and path are slightly different and you would do this instead:</p> <pre><code>module unload -f compilers mpi\nmodule load compilers/intel/2017/update4\nmodule load mpi/intel/2017/update3/intel\nmodule load crystal17/v1.0.2/intel-2017\n\n# Create a directory for this job and copy the input file into it.\nmkdir test00\ncd test00\ncp ~/Scratch/Crystal17/test_cases/inputs/test00.d12 INPUT\n\n# Gerun is our mpirun wrapper which sets the machinefile and number of\n# processes to the amount you requested with -pe mpi.\n# The CRYSTAL module sets $CRYxx_EXEDIR and $VERSION environment variables.\ngerun $CRY17_EXEDIR/Pcrystal\n</code></pre>"},{"location":"Software_Guides/Other_Software/#freesurfer","title":"FreeSurfer","text":"<p>FreeSurfer is a set of tools for analysis and visualization of structural and functional brain imaging data.</p> <p>Freesurfer can use threads to run on multiple cores in one node: request the number with <code>-pe smp</code> in the resource-request part of your jobscript.</p> <pre><code>#$ -pe smp 4\n\nmodule load xorg-utils/X11R7.7\nmodule load freesurfer/6.0.0\nexport SUBJECTS_DIR=~/Scratch/FreeSurfer_examples/subjects\n\n# -openmp $NSLOTS runs with the number of threads you requested\nrecon-all -openmp $NSLOTS -i sample-001.nii.gz -s bert -all\n</code></pre>"},{"location":"Software_Guides/Other_Software/#gamess","title":"GAMESS","text":"<p>The General Atomic and Molecular Electronic Structure System (GAMESS) is a general ab initio quantum chemistry package. </p> <p>The GAMESS module should be loaded once from a login node before submitting a job - this creates the <code>~/Scratch/gamess/randomLabel</code> directory for you which is used as <code>USERSCR</code> to write some scratch files during the job. If you don't want to keep these files and would prefer them to be written to <code>$TMPDIR</code> instead, you can put <code>export GAMESS_USERSCR=$TMPDIR</code> in your jobscript after the module load command.</p> <p>It will also check whether this is a cluster with <code>$TMPDIR</code> available, and set <code>SCR</code> which is the  binary scratch directory to <code>$TMPDIR</code> if it exists and <code>~/Scratch/gamess/scr.randomLabel</code> if it does not. You can also override this by adding <code>export GAMESS_SCR=</code> to any path you want it to use in your jobscript after the module load command.</p> <pre><code>module unload compilers mpi\nmodule load compilers/intel/2015/update2\nmodule load mpi/intel/2015/update3/intel\nmodule load gamess/5Dec2014_R1/intel-2015-update2\n\n# Optional: set where the USERSCR files go. \n# By default, the module sets it to ~/Scratch/gamess/randomLabel\nexport GAMESS_USERSCR=$TMPDIR\n\n# removes any of the user\u2019s semaphore arrays that were left from previous\n# jobs on this node. Use if you are using a whole number of nodes.\nipcrm --all=sem\n\nrungms exam01.inp 00 $NSLOTS $(ppn)\n\n# removes all of the user\u2019s semaphore arrays. \n# Use if you are using a whole number of nodes.\nipcrm --all=sem\n</code></pre>"},{"location":"Software_Guides/Other_Software/#semaphores","title":"Semaphores","text":"<p>GAMESS uses semaphore arrays and leaves these on the nodes where it has been running. This can  cause problems for your next jobs or for jobs from other users. </p> <p>You will get errors like this if there are not enough semaphores available:</p> <pre><code>DDI Process 12: semget return an error.\nsemget errno=ENOSPC -- check system limit for sysv semaphores.\n</code></pre> <p>These lines in the script above will clean up the semaphores belonging to your user on the node that you are on:</p> <pre><code># removes all of the user\u2019s semaphore arrays.\n# Use if you are using a whole number of nodes.\nipcrm --all=sem\n</code></pre> <p>Putting this before and after your GAMESS run ought to make sure that no semaphores from you are left over, providing your job completes and exits.</p> <p>On clusters where you can run multiple jobs per node, running this will cause you problems  between your own jobs, because it would delete your second job's semaphore arrays too. In that  case, you have two options. You can limit your GAMESS jobs to only running one after the other:</p> <pre><code># submit a job that will only start after job 12345 ends\nqsub -hold_jid=12345\n</code></pre> <p>Or you can request exclusive use of the node you are using so nothing else runs there (if you do  this, do use all of the cores available in the node if possible so they are not wasted):</p> <pre><code># request exclusive use of this node so that no other jobs run on it\n#$ -ac exclusive\n</code></pre>"},{"location":"Software_Guides/Other_Software/#troubleshooting-gamess","title":"Troubleshooting GAMESS","text":"<p>If there's something wrong with the way you are running your model or a mismatch between the number of processes you tell it to use and what it expects, you may get a log that only shows this:</p> <pre><code>Copying input file exam01.inp to your run's scratch directory...\ncp tests/standard/exam01.inp /tmpdir/job/12345.undefined/exam01.F05\nunset echo\n@: Expression Syntax.\n</code></pre> <p>To see what was happening to cause the error, you should take a local copy of <code>rungms</code>. Use  <code>module show</code> to look at the gamess module you are using, and look at the line that sets the PATH.  This shows you where the install is, and where the <code>rungms</code> script is located:</p> <pre><code>module show gamess/5Dec2014_R1/intel-2015-update2\n-------------------------------------------------------------------\n/shared/ucl/apps/modulefiles/applications/gamess/5Dec2014_R1/intel-2015-update2:\n\nmodule-whatis   {Adds GAMESS 5Dec2014_R1 to your environment, built for Intel MPI. Uses ~/Scratch/gamess for USERSCR. You can override by exporting GAMESS_USERSCR as another path.}\nprereq          gcc-libs\nprereq          compilers/intel/2015/update2\nprereq          mpi/intel/2015/update3/intel\nconflict        gamess\nprepend-path    PATH /shared/ucl/apps/gamess/5Dec2014_R1/intel-2015-update2\nprepend-path    CMAKE_PREFIX_PATH /shared/ucl/apps/gamess/5Dec2014_R1/intel-2015-update2\nsetenv          GAMESS_USERSCR ~/Scratch/gamess\n-------------------------------------------------------------------\n</code></pre> <p>Copy <code>rungms</code> from the version you are using to somewhere in your space.</p> <pre><code>cp /shared/ucl/apps/gamess/5Dec2014_R1/intel-2015-update2/rungms ~/Scratch/gamess_test\n</code></pre> <p>Edit it to add <code>-evx</code> to the very first line so it reads:</p> <pre><code>#!/bin/csh -evx\n</code></pre> <p>Now in your jobscript, use your <code>rungms</code> instead of the central one.</p> <pre><code># using rungms in current directory\n./rungms exam01.inp 00 $NSLOTS $(ppn)\n</code></pre> <p>You should get error output showing you what the script is doing. At the end it might look like this:</p> <pre><code>@ PPN2 = $PPN + $PPN\n@: Expression Syntax.\n</code></pre> <p>This can happen if you were running <code>./rungms exam01.inp 00 $NSLOTS</code> so that <code>$PPN</code> was not being  passed to <code>rungms</code> and is undefined.</p>"},{"location":"Software_Guides/Other_Software/#gatk","title":"GATK","text":"<p>The Genome Analysis Toolkit or GATK is a software package developed at the Broad Institute to analyze high-throughput sequencing data. </p> <p>Version 4 of GATK is BSD-licensed so does not require a group to control access to the software. </p> <p>Version 3 of GATK requires you to agree to the GATK license before we can add you to the <code>leggatk</code> group which gives you access: you can do this by downloading GATK 3 from The Broad Institute GATK download page, reading the license, and telling us you agree to it. You may need to create a gatkforums account before you can download.</p> <p>GATK 3 uses Java 1.7 (the system Java) so you do not need to load a Java module. GATK 4 uses 1.8 so you need to load <code>java/1.8.0_92</code> first.</p> <p>GATK 4.2.5.0 or newer uses the newest version of Java 8, so you need to load <code>java/temurin-8</code>.</p> <p>Load the version you want, then to run GATK you should either prefix the .jar you want to run with <code>$GATKPATH</code>:</p> <pre><code>java -Xmx2g -jar $GATKPATH/GenomeAnalysisTK.jar OPTION1=value1 OPTION2=value2...\n</code></pre> <p>Or we provide wrappers, so you can run it one of these ways instead:</p> <pre><code>GenomeAnalysisTK OPTION1=value1 OPTION2=value2...\n</code></pre> <pre><code>gatk OPTION1=value1 OPTION2=value2...\n</code></pre> <p>If you want to use some of the newer tools in GATK 4 which rely on Python/Conda, you must use GATK &gt;= 4.2.5.0 and additionally set up your miniconda environment.  With 4.2.5.0 this means:</p> <pre><code>module load java/temurin-8\nmodule load gatk/4.2.5.0\nmodule load python/miniconda3/4.10.3\nsource $UCL_CONDA_PATH/etc/profile.d/conda.sh \nconda activate $GATK_CONDA\n</code></pre> <p>(For newer versions of GATK it will tell you which version of miniconda to load)</p>"},{"location":"Software_Guides/Other_Software/#gaussian","title":"Gaussian","text":"<p>Access to Gaussian 09 or Gaussian 16 are controlled by membership of separate groups.  UCL has a site license so UCL users can be added on request.</p> <p>Gaussian is too resource-intensive to ever be run on the login nodes.</p>"},{"location":"Software_Guides/Other_Software/#multithreaded-shared-memory-gaussian-jobs","title":"Multithreaded shared memory Gaussian jobs","text":"<p>The main Gaussian executable lets you run jobs that use from 1 core up to a full node. When using more than one core, make sure your input file contains <code>%NProcShared=</code> with the number of cores your job is requesting.</p> <p><code>$GAUSS_SCRDIR</code> is where Gaussian puts temporary files which can use a lot of space. On Myriad in a job this is created inside <code>$TMPDIR</code> by default. On diskless clusters, this  is set this to a directory in your Scratch instead: loading one of the Gaussian  modules will handle this automatically and show where it has created the directory.</p> <pre><code># Example for Gaussian 16\n\n# Set up runtime environment\nmodule load gaussian/g16-a03/pgi-2016.5\nsource $g16root/g16/bsd/g16.profile\n\n# Run g16 job\ng16 input.com\n</code></pre> <pre><code># Example for Gaussian 09\n\n# Setup runtime environment\nmodule load gaussian/g09-d01/pgi-2015.7\nsource $g09root/g09/bsd/g09.profile\n\n# Run g09 job\ng09 input.com\n</code></pre>"},{"location":"Software_Guides/Other_Software/#linda-parallel-gaussian-jobs","title":"Linda parallel Gaussian jobs","text":"<p>Only currently working for Gaussian 09.</p> <p>Gaussian Linda jobs can run across multiple nodes.</p> <pre><code># Select the MPI parallel environment and 80 cores total\n#$ -pe mpi 80\n\n# 8. Select number of threads per Linda worker (value of NProcShared in your\n#     Gaussian input file. This will give 80/40 = 2 Linda workers.\nexport OMP_NUM_THREADS=40\n\n# Setup g09 runtime environment\nmodule load gaussian/g09-d01/pgi-2015.7\nsource $g09root/g09/bsd/g09.profile\n\n# Pre-process g09 input file to include nodes allocated to job\necho \"Running: lindaConv testdata.com $JOB_ID $TMPDIR/machines\"\necho ''\n$lindaConv testdata.com $JOB_ID $TMPDIR/machines\n\n# Run g09 job\n\necho \"Running: g09 \\\"job$JOB_ID.com\\\"\"\n\n# communication needs to be via ssh not the Linda default\nexport GAUSS_LFLAGS='-v -opt \"Tsnet.Node.lindarsharg: ssh\"'\n\ng09 \"job$JOB_ID.com\"\n</code></pre>"},{"location":"Software_Guides/Other_Software/#troubleshooting-memory-errors","title":"Troubleshooting: Memory errors","text":"<p>If you encounter errors like:</p> <pre><code>Out-of-memory error in routine ShPair-LoodLd2 (IEnd= 257724 MxCore= 242934)\n\nUse %mem=48MW to provide the minimum amount of memory required to complete this step.\n</code></pre> <p>Try adding this to your jobscript:</p> <pre><code>export GAUSS_MEMDEF=48000000\n</code></pre> <p>You may need to increase this value even more to allow it to run.</p>"},{"location":"Software_Guides/Other_Software/#troubleshooting-no-space-left-on-device","title":"Troubleshooting: No space left on device","text":"<p>If you get this error</p> <pre><code>  g_write: No space left on device\n</code></pre> <p>The <code>$GAUSS_SCRDIR</code> is probably full - if it was on a cluster that has local  disks and is using <code>$TMPDIR</code> you should increase the amount of <code>tmpfs</code> you are  requesting in your jobscript. Otherwise check <code>lquota</code> for your data usage and potentially request a larger Scratch.</p>"},{"location":"Software_Guides/Other_Software/#gromacs","title":"GROMACS","text":"<p>We have many versions of GROMACS installed, some built with Plumed. The module name will indicate this.</p> <p>Which executable you should run depends on the problem you wish to solve. For both single and double precision version builds, serial binaries and an MPI binary for mdrun (<code>mdrun_mpi</code> for newer versions, <code>gmx_mpi</code> for Plumed and some older versions) are provided. Double precision binaries have a <code>_d</code> suffix (so <code>gmx_d</code>, <code>mdrun_mpi_d</code>, <code>gmx_mpi_d</code> etc). </p> <p>You can see what the executable names are by running <code>module show gromacs/2021.2/gnu-7.3.0</code>  for example and then running the <code>ls</code> command on the <code>bin</code> directory that the module tells you  that version is installed in.</p> <pre><code># Example for GPU gromacs/2021.5/cuda-11.3\nmodule unload compilers mpi gcc-libs\nmodule load gcc-libs/10.2.0\nmodule load compilers/gnu/10.2.0\nmodule load\u00a0python3/3.9-gnu-10.2.0\u00a0\nmodule load cuda/11.3.1/gnu-10.2.0\nmodule load mpi/openmpi/4.0.5/gnu-10.2.0\nmodule load\u00a0gromacs/2021.5/cuda-11.3\n\n# Run GROMACS - the executables are gmx_cuda, gmx_mpi_cuda and mdrun_mpi_cuda\n</code></pre> <pre><code># Example for gromacs/2021.2/gnu-7.3.0\nmodule unload compilers mpi gcc-libs\nmodule load gcc-libs/7.3.0\nmodule load compilers/gnu/7.3.0\nmodule load mpi/openmpi/3.1.4/gnu-7.3.0\nmodule load python3\nmodule load gromacs/2021.2/gnu-7.3.0\n\n# Run GROMACS - replace with mdrun command line suitable for your job!\n\ngerun mdrun_mpi -v -stepout 10000\n</code></pre> <pre><code># Example for gromacs/2019.3/intel-2018\nmodule unload -f compilers mpi\nmodule load compilers/intel/2018/update3\nmodule load mpi/intel/2018/update3/intel\nmodule load gromacs/2019.3/intel-2018\n\n# Run GROMACS - replace with mdrun command line suitable for your job!\n\ngerun mdrun_mpi -v -stepout 10000\n</code></pre> <pre><code># Plumed example for gromacs/2019.3/plumed/intel-2018\nmodule unload -f compilers mpi\nmodule load compilers/intel/2018/update3 \nmodule load mpi/intel/2018/update3/intel \nmodule load libmatheval \nmodule load flex \nmodule load plumed/2.5.2/intel-2018\nmodule load gromacs/2019.3/plumed/intel-2018\n\n# Run GROMACS - replace with mdrun command line suitable for your job!\n\ngerun gmx_mpi -v -stepout 10000\n</code></pre>"},{"location":"Software_Guides/Other_Software/#passing-in-options-to-gromacs-non-interactively","title":"Passing in options to GROMACS non-interactively","text":"<p>Some GROMACS executables like <code>trjconv</code> normally take interactive input. You can't do this in a jobscript, so you need to pass in the input you would normally type in. There are several ways of doing this, mentioned at GROMACS Documentation - Specifying selections from command line. The simplest is to echo the input in and keep your gmx options as they would normally be. If the inputs you would normally type were 3 and 3, then you can do this:</p> <pre><code>echo 3 3 | gmx whatevercommand -options\n</code></pre>"},{"location":"Software_Guides/Other_Software/#checkpoint-and-restart","title":"Checkpoint and restart","text":"<p>GROMACS has built-in checkpoint and restart ability, so you can use this if your runs will not complete in the maximum 48hr wallclock time.</p> <p>Have a look at the GROMACS manual for full details, as there are more options than mentioned here.</p> <p>You can tell GROMACS to write a checkpoint file when it is approaching the maximum wallclock time available, and then exit.</p> <p>In this case, we had asked for 48hrs wallclock. This tells GROMACS to start from the last checkpoint if there is one, and write a new checkpoint just before it reaches 47 hrs runtime.</p> <pre><code>gerun mdrun_mpi -cpi -maxh 47 &lt;options&gt;\n</code></pre> <p>The next job you submit with the same script will carry on from the checkpoint the last job wrote. You could use job dependencies to submit two identical jobs at the same time and have one dependent on the other, so it won't start until the first finishes - have a look at <code>man qsub</code> for the <code>-hold_jid</code> option.</p> <p>You can also write checkpoints at given intervals:</p> <pre><code># Write checkpoints every 120 mins, start from checkpoint if there is one.\ngerun mdrun_mpi -cpi -cpt 120 &lt;options&gt;\n</code></pre>"},{"location":"Software_Guides/Other_Software/#hammock","title":"Hammock","text":"<p>Hammock is a tool for peptide sequence clustering. It is able to cluster extremely large amounts of short peptide sequences into groups sharing sequence motifs. Typical Hammock applications are NGS-based experiments using large combinatorial peptide libraries, e.g. Phage display. </p> <p>Hammock has to be installed in your own space to function, so we provide a hammock module that contains the main dependencies and creates a quick-install alias:</p> <pre><code># on the login nodes\nmodule unload compilers\nmodule load hammock/1.0.5\ndo-hammock-install\n</code></pre> <p>This will install Hammock 1.0.5 in your home, edit settings.prop to use clustal-omega and hmmer from our modules and tell it to write temporary files in your Scratch directory (in the form <code>Hammock_temp_time</code>). </p> <pre><code># in your jobscript\nmodule unload compilers\nmodule load hammock/1.0.5\n\n# This copies the MUSI example that comes with Hammock into your working\n# directory and runs it. The module sets $HAMMOCKPATH for you. \n# You must set the output directory to somewhere in Scratch with -d. \n# Below makes a different outputdir per job so multiple runs don't overwrite files.\n\ncp $HAMMOCKPATH/../examples/MUSI/musi.fa .\noutputdir=~/Scratch/hammock-examples/musi_$JOB_ID\nmkdir -p $outputdir\necho \"Running java -jar $HAMMOCKPATH/Hammock.jar full -i musi.fa -d $outputdir\"\n\njava -jar $HAMMOCKPATH/Hammock.jar full -i musi.fa -d $outputdir\n</code></pre>"},{"location":"Software_Guides/Other_Software/#hopspack","title":"HOPSPACK","text":"<p>HOPSPACK (Hybrid Optimization Parallel Search PACKage) solves derivative-free optimization problems using an open source, C++ software framework.</p> <p>We have versions of HOPSPACK built using the GNU compiler and OpenMPI, and the Intel compiler and MPI. This example shows the GNU version. Serial and parallel versions are available, <code>HOPSPACK_main_mpi</code> and <code>HOPSPACK_main_serial</code>.</p> <pre><code>module unload compilers\nmodule unload mpi\nmodule load compilers/gnu/4.9.2\nmodule load mpi/openmpi/1.8.4/gnu-4.9.2\nmodule load atlas/3.10.2/gnu-4.9.2\nmodule load hopspack/2.0.2/gnu-4.9.2\n\n# Add the examples directory we are using to our path. \n# Replace this with the path to your own executables.\nexport PATH=$PATH:~/Scratch/examples/1-var-bnds-only/\n\n# Run parallel HOPSPACK.\n# Gerun is our mpirun wrapper which sets the machinefile and number of\n# processes to the amount you requested with -pe mpi.\ngerun HOPSPACK_main_mpi ~/Scratch/examples/1-var-bnds-only/example1_params.txt &gt; example1_output.txt\n</code></pre>"},{"location":"Software_Guides/Other_Software/#idl","title":"IDL","text":"<p>IDL is a complete environment and language for the analysis and visualisation of scientific and other technical data. It can be used for everything from quick interactive data exploration to building complex applications. </p> <p>Single-threaded jobscript:</p> <pre><code>cd $TMPDIR\n\nmodule load idl/8.4.1\n\n# Copy IDL source files to $TMPDIR\ncp ~/Scratch/IDL/fib.pro $TMPDIR\ncp ~/Scratch/IDL/run1.pro $TMPDIR\n\nidl -queue -e @run1.pro\n\n# tar up all contents of $TMPDIR back into your space\ntar zcvf $HOME/Scratch/IDL_output/files_from_job_$JOB_ID.tgz $TMPDIR\n</code></pre> <p>Parallel jobscript:</p> <pre><code>cd $TMPDIR\n\nmodule load idl/8.1\n\n# this sets the IDL thread pool: do not change this\nexport IDL_CPU_TPOOL_NTHREADS=$OMP_NUM_THREADS\n\n# Copy IDL source files to $TMPDIR\ncp ~/Scratch/IDL/fib.pro $TMPDIR\ncp ~/Scratch/IDL/run2mp.pro $TMPDIR\n\nidl -queue -e @run2mp.pro\n\n# tar up all contents of $TMPDIR back into your space\ntar zcvf $HOME/Scratch/IDL_output/files_from_job_$JOB_ID.tgz $TMPDIR\n</code></pre>"},{"location":"Software_Guides/Other_Software/#jags","title":"JAGS","text":"<p>JAGS (Just Another Gibbs Sampler) is a program for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation not wholly unlike BUGS. </p> <p>Use this to use JAGS in standalone command line mode:</p> <pre><code>module unload compilers mpi\nmodule load compilers/gnu/4.9.2\nmodule load openblas/0.2.14/gnu-4.9.2\nmodule load jags/4.2.0/gnu.4.9.2-openblas\n</code></pre> <p>We have also added JAGS support to <code>r/recommended</code> using the <code>rjags</code> and <code>R2jags</code> R packages.</p>"},{"location":"Software_Guides/Other_Software/#lammps","title":"LAMMPS","text":"<p>LAMMPS is an open source parallel molecular dynamics code which exhibits good scaling in a wide range of environments. </p> <p>The LAMMPS binaries are called <code>lmp_$cluster</code> and all have an <code>lmp_default</code> symlink which can be used.</p> <p>LAMMPS-8Dec15 and later were built with additional packages <code>kspace</code>, <code>replica</code>, <code>rigid</code>, and <code>class2</code>.</p> <p>The versions from <code>lammps-16Mar18-basic_install</code> onwards (not <code>lammps/16Mar18/intel-2017</code>) have most of the included packages built. There are also <code>userintel</code> and <code>gpu</code> versions from this point.</p> <p>We do not install the LAMMPS user packages as part of our central install, but you can build your own version with the ones that you want in your space.</p> <pre><code>module -f unload compilers mpi\nmodule load compilers/intel/2018\nmodule load mpi/intel/2018\nmodule load lammps/16Mar18/basic/intel-2018\n\n# Gerun is our mpirun wrapper which sets the machinefile and number of\n# processes to the amount you requested with -pe mpi.\ngerun $(which lmp_default) -in inputfile\n</code></pre> <p>For the latest version of LAMMPS we have installed which is 29th September 2021 Update 2 where the binaries are called <code>lmp_mpi</code> for the MPI version and <code>lmp_gpu</code> for the GPU version:</p> <pre><code>module unload compilers mpi gcc-libs\nmodule load gcc-libs/10.2.0\nmodule load compilers/gnu/10.2.0\nmodule load mpi/openmpi/4.0.5/gnu-10.2.0\nmodule load python3/3.9-gnu-10.2.0\nmodule load lammps/29sep21up2/basic/gnu-10.2.0\n\ngerun lmp_mpi -in inputfile\n</code></pre> <p>for the basic MPI version and:</p> <pre><code>module unload compilers mpi gcc-libs\nmodule load gcc-libs/10.2.0\nmodule load compilers/gnu/10.2.0\n\n# these three modules for Myriad only\nmodule load numactl/2.0.12\nmodule load binutils/2.36.1/gnu-10.2.0\nmodule load ucx/1.9.0/gnu-10.2.0\n\nmodule load mpi/openmpi/4.0.5/gnu-10.2.0\nmodule load cuda/11.3.1/gnu-10.2.0\nmodule load python3/3.9-gnu-10.2.0\nmodule load lammps/29sep21up2/gpu/gnu-10.2.0\n\ngerun lmp_gpu -sf gpu -pk gpu 1 -in inputfile\n</code></pre> <p>for the version with GPU support which is only available on clusters with GPUs.  The MPI version is available on all clusters. On Myriad the <code>numactl</code>, <code>binutils</code> and  <code>ucx</code> modules are additionally needed by OpenMPI.</p> <p>LAMMPS 29th September 2021 Update 2 has been built with the GNU compilers, OpenMPI and CUDA for the GPU version. </p> <p>We also have Intel installs:</p> <pre><code># LAMMPS 29 Sep 2021 Update 2 with Intel compilers and INTEL package\nmodule unload -f compilers mpi\nmodule load compilers/intel/2020/release\nmodule load mpi/intel/2019/update6/intel\nmodule load python/3.9.10\nmodule load lammps/29sep21up2/userintel/intel-2020\n\ngerun lmp_mpi -in inputfile\n</code></pre> <pre><code># LAMMPS 29 Sep 2021 Update 2 for GPU with Intel compilers\nmodule unload compilers mpi\nmodule load compilers/intel/2020/release\nmodule load mpi/intel/2019/update6/intel\nmodule load python/3.9.10\nmodule load cuda/11.3.1/gnu-10.2.0\nmodule load lammps/29sep21up2/gpu/intel-2020\n\ngerun lmp_gpu -sf gpu -pk gpu 1 -in inputfile\n</code></pre>"},{"location":"Software_Guides/Other_Software/#mathematica","title":"Mathematica","text":"<p>Wolfram Mathematica: mathematical computation program for areas of technical computing such  as machine learning, statistics, symbolic computation, data manipulation, network analysis,  time series analysis, NLP, optimization, and plotting functions and various types of data.</p> <pre><code>module load xorg-utils/X11R7.7\nmodule load mathematica/13.1.0\n\nwolfram -script input.wl\n# or\nmath -script input.m\n# to redirect output to a different file\nwolfram -script input.wl &gt; output.out\n</code></pre>"},{"location":"Software_Guides/Other_Software/#meme-suite","title":"MEME Suite","text":"<p>MEME Suite: Motif-based sequence analysis tools. This install is for the command-line tools and connects to their website for further analysis. </p> <pre><code>module unload compilers\nmodule unload mpi\nmodule load compilers/gnu/4.9.2\nmodule load mpi/openmpi/1.8.4/gnu-4.9.2\nmodule load perl/5.22.0\nmodule load python2/recommended\nmodule load ghostscript/9.16/gnu-4.9.2\nmodule load meme/4.10.1_4\n</code></pre>"},{"location":"Software_Guides/Other_Software/#mirdeep2","title":"miRDeep2","text":"<p>Discovering known and novel miRNAs from deep sequencing data, miRDeep2 is a completely overhauled tool which discovers microRNA genes by analyzing sequenced RNAs. The tool reports known and hundreds of novel microRNAs with high accuracy in seven species representing the major animal clades. </p> <pre><code>module load squid/1.9g\nmodule load randfold/2.0\nmodule load perl/5.22.0\nmodule load bowtie/1.1.2\nmodule load python/2.7.9\nmodule load viennarna/2.1.9\nmodule load mirdeep/2.0.0.7\n</code></pre>"},{"location":"Software_Guides/Other_Software/#misomisopy","title":"MISO/misopy","text":"<p>MISO (Mixture of Isoforms) is a probabilistic framework that quantitates the expression level of alternatively spliced genes from RNA-Seq data, and identifies differentially regulated isoforms or exons across samples.</p> <p>misopy is available as part of the <code>python2/recommended</code> bundle. </p> <p>MISO can run multithreaded on one node, or can submit multiple independent single-core jobs at once using the <code>--use-cluster</code> option.</p> <p>If you want to use MISO's ability to create and submit jobs itself, you need a MISO settings file like the one shown below. You give your job options as arguments to the qsub command in the <code>cluster_command</code> line. </p> <p>Settings files can be used with the <code>--settings-filename=SETTINGS_FILENAME</code> option. You will also need to put your module unload and load commands in your .bashrc if using MISO's own job submission, because you are no longer including them in a jobscript.</p> <p>Example <code>miso_settings.txt</code>. Multithreaded jobs will use <code>num_processors</code>. <code>num_processors</code> is ignored if <code>--use-cluster</code> is specified:</p> <pre><code>[data]\nfilter_results = True\nmin_event_reads = 20\n\n[cluster]\ncluster_command = \"qsub -l h_rt=00:10:00 -l mem=1GB -wd ~/Scratch\"\n\n[sampler]\nburn_in = 500\nlag = 10\nnum_iters = 5000\nnum_chains = 6\nnum_processors = 4\n</code></pre>"},{"location":"Software_Guides/Other_Software/#molpro","title":"MOLPRO","text":"<p>Molpro is a complete system of ab initio programs for molecular electronic structure calculations. </p> <p>Molpro 2015.1.3 was provided as binary only and supports communication over Ethernet and not Infiniband - use this one on single-node jobs primarily.</p> <p>Molpro 2015.1.5 was built from source with the Intel compilers and Intel MPI, so can be run multi-node.</p> <p>Molpro 2020.1 is a more recent binary install and supports both.</p> <pre><code>module load molpro/2015.1.5/intel-2015-update2\n\n# Example files available in /shared/ucl/apps/molpro/2015.1.5/intel-2015-update2/molprop_2015_1_linux_x86_64_i8/examples/\n# If this is a multi-node job you need to set the wavefunction directory to \n# somewhere in Scratch with -W. For a single-node job -W should be in $TMPDIR.\n# You can use $SGE_O_WORKDIR to refer to the directory you set with -wd in your jobscript.\n# $NSLOTS will use the number of cores you requested with -pe mpi.\n\necho \"Running molpro -n $NSLOTS -W $TMPDIR h2o_scf.com\"\n\nmolpro -n $NSLOTS -W $TMPDIR h2o_scf.com\n</code></pre> <p>On Myriad, if you get this error with the MPI 2015 install, please use the binary 2015.1.3 install.</p> <pre><code>libi40iw-i40iw_ucreate_qp: failed to create QP, unsupported QP type: 0x4\n</code></pre> <p>Output: MOLPRO can end up writing very many small output files, and this is detrimental to the performance of a parallel filesystem like Lustre. If you are running jobs on Myriad then you should set your -I -d and (especially) -W directories to be in $TMPDIR so they can be  accessed quickly and not slow down other jobs. At the end of the job, copy back the data you want to keep into your Scratch.</p> <p>If you are running parallel multi-node jobs and the directories need to be readable by all  the nodes, then you need to write to Scratch.</p> <p>If you sometimes get an error like this:</p> <pre><code>terminate called after throwing an instance of 'std::runtime_error'\n what():  Cannot open cache file tmpfilexNCOMM\n</code></pre> <p>This is because Molpro is opening and closing temporary files very close together and the file  handles do not get freed up immediately, and it has gone over the soft limit of open files of 1024.</p> <p>You can increase this to the hard limit of open files on the compute nodes by adding the below to your jobscript before the <code>molpro</code> command and it will let you have up to 4096 files open at once. Please be careful about the number of jobs you have running concurrently if you do this, as it could  have an impact on the filesystem.</p> <pre><code>ulimit -Sn 4096\n</code></pre>"},{"location":"Software_Guides/Other_Software/#mrtrix","title":"MRtrix","text":"<p>MRtrix provides a set of tools to perform diffusion-weighted MRI white matter tractography in the presence of crossing fibres. </p> <pre><code>module load python3/recommended\nmodule load qt/4.8.6/gnu-4.9.2\nmodule load eigen/3.2.5/gnu-4.9.2\nmodule load fftw/3.3.6-pl2/gnu-4.9.2\nmodule load mrtrix/3.0rc3/gnu-4.9.2/nogui\n</code></pre> <p>You must load these modules once from a login node before submitting a job. It copies a <code>.mrtrix.conf</code> to your home directory the first time you run this module from a login node, which sets:</p> <pre><code>  Analyse.LeftToRight: false\n  NumberOfThreads: 4\n</code></pre> <p>You need to alter <code>NumberOfThreads</code> to what you are using in your job script before you submit a job. </p> <p>The MRtrix GUI tools are unavailable: <code>mrview</code> and <code>shview</code> in MRtrix 3 cannot be run over a remote X11 connection so are not usable on our clusters. To use these tools you will need a local install on your own computer. </p>"},{"location":"Software_Guides/Other_Software/#mutect","title":"MuTect","text":"<p>MuTect is a tool developed at the Broad Institute for the reliable and accurate identification of somatic point mutations in next generation sequencing data of cancer genomes. It is built on top of the GenomeAnalysisToolkit (GATK), which is also developed at the Broad Institute, so it uses the same command-line conventions and (almost all) the same input and output file formats. </p> <p>MuTect requires you to agree to the GATK license before we can add you to the <code>lgmutect</code> group which gives you access: you can do this by downloading MuTect from The Broad Institute CGA page. You may need to create a gatkforums account before you can download.</p> <p>MuTect is currently not compatible with Java 1.8, so you need to use the system Java 1.7. Set up your modules as follows: </p> <pre><code>module load mutect/1.1.7\n</code></pre> <p>Then to run MuTect, you should either prefix the .jar you want to run with <code>$MUTECTPATH</code>:</p> <pre><code>java -Xmx2g -jar $MUTECTPATH/mutect-1.1.7.jar OPTION1=value1 OPTION2=value2...\n</code></pre> <p>Or we provide wrappers, so you can run it this way instead: </p> <pre><code>mutect OPTION1=value1 OPTION2=value2...\n</code></pre>"},{"location":"Software_Guides/Other_Software/#namd","title":"NAMD","text":"<p>NAMD is a parallel molecular dynamics code designed for high-performance simulation of  large biomolecular systems.</p> <p>We have several different types of install, some of them suited to particular clusters only. To see all the versions, type <code>module avail namd</code>.</p> <p>These examples are running the <code>apoa1</code> benchmark, available from the NAMD website.</p>"},{"location":"Software_Guides/Other_Software/#multicore-gpu","title":"Multicore GPU","text":"<p>This version of NAMD runs within one GPU node. It can run on multiple GPUs on that node,  but not across multiple different nodes. NAMD uses the CPUs and GPUs together so it is  recommended you request all the cores on the node if you are requesting all the GPUs.</p> <p>For best performance of simulations it is recommended that you use an entire node,  all the CPUs and all the available GPUs.</p> <pre><code># request a number of CPU cores and GPUs\n#$ -pe smp 10\n#$ -l gpu=1\n\nmodule load namd/2.14/multicore-gpu\n\n# ${NSLOTS} will get the number of cores you asked for with -pe smp.\n# +setcpuaffinity is recommended to make sure threads are assigned to specific CPUs.\n\nnamd2 +p${NSLOTS} +setcpuaffinity apoa1_nve_cuda.namd\n</code></pre>"},{"location":"Software_Guides/Other_Software/#ofi","title":"OFI","text":"<p>This version of NAMD is for clusters with OmniPath interconnects (not Myriad). It can run across multiple nodes. The OFI versions should use significantly less  memory than the older MPI-based installs.</p> <pre><code>module unload -f compilers mpi\nmodule load compilers/intel/2019/update5\nmodule load mpi/intel/2019/update5/intel\nmodule load namd/2.14/ofi/intel-2019\n\n# ${NSLOTS} will get the number of cores you asked for with -pe.\n\ncharmrun +p${NSLOTS} namd2 apoa1.namd\n</code></pre>"},{"location":"Software_Guides/Other_Software/#ofi-smp","title":"OFI-SMP","text":"<p>This version of NAMD runs with threads (smp) and processes and is for clusters  with OmniPath interconnects (not Myriad). It can run across multiple nodes. The OFI versions should use significantly less memory than the older MPI-based installs.</p> <pre><code>module unload -f compilers mpi\nmodule load compilers/intel/2019/update5\nmodule load mpi/intel/2019/update5/intel\nmodule load namd/2.14/ofi-smp/intel-2019\n\n# ${NSLOTS} will get the number of cores you asked for with -pe.\n# +setcpuaffinity is recommended to make sure threads are assigned to specific CPUs.\n# ++ppn is the number of PEs (or worker threads) to create for each process.\n\ncharmrun +p${NSLOTS} namd2 apoa1.namd ++ppn2 +setcpuaffinity\n</code></pre>"},{"location":"Software_Guides/Other_Software/#ofi-smp-gpu","title":"OFI-SMP-GPU","text":"<p>This version of NAMD runs with threads (smp) and processes and is for clusters with OmniPath interconnects as well as GPUs (not Myriad). It can run across multiple nodes.</p> <pre><code># request a number of CPU cores and GPUs\n#$ -pe smp 24\n#$ -l gpu=2\n\nmodule unload compilers mpi gcc-libs\nmodule load gcc-libs/7.3.0\nmodule load compilers/intel/2019/update5\nmodule load mpi/intel/2019/update5/intel\nmodule load cuda/11.3.1/gnu-10.2.0\nmodule load namd/2.14/ofi-smp-gpu/intel-2019\n\n# ${NSLOTS} will get the number of cores you asked for with -pe.\n# +setcpuaffinity is recommended to make sure threads are assigned to specific CPUs.\n# ++ppn is the number of PEs (or worker threads) to create for each process.\n\n# The number of GPU devices must be a multiple of the number of NAMD processes\n# since processes cannot share GPUs.\n# Here we have ++ppn12 for 12 threads, and charmrun works out we have 2 NAMD processes \n# available for the 2 GPUs.\n\ncharmrun +p${NSLOTS} namd2 apoa1_nve_cuda.namd ++ppn12 +setcpuaffinity \n</code></pre>"},{"location":"Software_Guides/Other_Software/#mpi","title":"MPI","text":"<p>These are older versions. It is recommended to run the OFI versions above instead if possible.</p> <pre><code>module load fftw/2.1.5/intel-2015-update2\nmodule load namd/2.13/intel-2018-update3\n\n# GErun is our mpirun wrapper that gets $NSLOTS and the machinefile for you.\n\ngerun namd2 apoa1.namd \n</code></pre>"},{"location":"Software_Guides/Other_Software/#nextflow","title":"Nextflow","text":"<p>We do not currently have central installs of Nextflow, but a group of UCL researchers have  contributed a config file and instructions for Myriad at the nf-core/configs repository</p> <p>Nextflow containers can be run using Singularity.</p>"},{"location":"Software_Guides/Other_Software/#nonmem","title":"NONMEM","text":"<p>NONMEM\u00ae is a nonlinear mixed effects modelling tool used in population pharmacokinetic / pharmacodynamic analysis. </p> <p>We have one build that uses the GNU compiler and ATLAS and an Intel build using MKL. Both use Intel MPI. </p> <p>This example uses the Intel build.</p> <pre><code>jobDir=example1_parallel_$JOB_ID\nmkdir $jobDir\n\n# Copy control and datafiles to jobDir\ncp /shared/ucl/apps/NONMEM/examples/foce_parallel.ctl $jobDir\ncp /shared/ucl/apps/NONMEM/examples/example1b.csv $jobDir\ncd $jobDir\n\nmodule unload compilers mpi\nmodule load compilers/intel/2015/update2\nmodule load mpi/intel/2015/update3/intel\nmodule load nonmem/7.3.0/intel-2015-update2\n\n# Create parafile for job using $TMPDIR/machines\nparafile.sh $TMPDIR/machines &gt; example1.pnm\n\nnmfe73 foce_parallel.ctl example1.res -parafile=example1.pnm -background -maxlim=1 &gt; example1.log\n</code></pre>"},{"location":"Software_Guides/Other_Software/#nwchem","title":"NWChem","text":"<p>NWChem applies theoretical techniques to predict the structure, properties, and reactivity of chemical and biological species ranging in size from tens to millions of atoms.</p> <p>You should load the NWChem module you wish to use once from a login node, as it will create a symlinked <code>.nwchemrc</code> in your home. </p> <pre><code>module unload compilers mpi\nmodule load compilers/intel/2017/update4\nmodule load mpi/intel/2017/update3/intel\nmodule load python/2.7.12\nmodule load nwchem/6.8-47-gdf6c956/intel-2017\n\n# $NSLOTS will get the number of processes you asked for with -pe mpi.\nmpirun -np $NSLOTS -machinefile $TMPDIR/machines nwchem hpcvl_sample.nw\n</code></pre>"},{"location":"Software_Guides/Other_Software/#nwchem-troubleshooting","title":"NWChem troubleshooting","text":"<p>If you get errors like this</p> <pre><code>{    0,    3}:  On entry to PDSTEDC parameter number   10 had an illegal value\n</code></pre> <p>then you are coming across an error in Intel MKL 2018, and should make sure you change  to the Intel 2017 compiler module as shown above. (MKL versions are bundled with the  corresponding Intel compiler modules).</p> <p>If your run terminates with an error saying</p> <pre><code>ARMCI supports block process mapping only\n</code></pre> <p>then you are probably trying to use round-robin MPI process placement, which ARMCI does not like. <code>gerun</code> uses round-robin for Intel MPI by default as it works better in most cases. Use <code>mpirun</code> instead of <code>gerun</code>:</p> <pre><code>mpirun -np $NSLOTS -machinefile $TMPDIR/machines nwchem input.nw\n</code></pre> <p>If you get an error complaining about <code>$NWCHEM_NWPW_LIBRARY</code> similar to this:</p> <pre><code>warning:::::::::::::: from_compile\nNWCHEM_NWPW_LIBRARY is: &lt;\n/dev/shm/tmp.VB3DpmjULc/nwchem-6.6/src/nwpw/libraryps/&gt;\nbut file does not exist or you do not have access to it !\n------------------------------------------------------------------------\nnwpwlibfile: no nwpw library found 0\n</code></pre> <p>then your <code>~/.nwchemrc</code> symlink is likely pointing to a different version that you used previously. Deleting the symlink and loading the module you want to use will recreate it correctly. </p>"},{"location":"Software_Guides/Other_Software/#orca","title":"ORCA","text":"<p>ORCA is an ab initio, DFT, and semi-empirical SCF-MO package.</p> <pre><code>module unload compilers \nmodule unload mpi\nmodule load compilers/gnu/4.9.2\nmodule load mpi/openmpi/3.1.4/gnu-4.9.2\nmodule load orca/4.2.1-bindist/gnu-4.9.2\n\norca input.inp &gt; output.out\n</code></pre> <p>If you want to run ORCA in parallel using MPI, the jobscript will be the same but you will need to add the <code>!PAL</code> keyword to your input file to tell it how many processes to use. (You do not use <code>mpirun</code> or <code>gerun</code> with ORCA).</p>"},{"location":"Software_Guides/Other_Software/#picard","title":"Picard","text":"<p>Picard comprises Java-based command-line utilities that manipulate SAM files, and a Java API (SAM-JDK) for creating new programs that read and write SAM files. Both SAM text format and SAM binary (BAM) format are supported. </p> <p>Picard requires a Java 1.8 module to be loaded.</p> <pre><code>module load java/1.8.0_92\nmodule load picard-tools/2.18.9\n</code></pre> <p>To run Picard you can prefix the .jar you want to run with <code>$PICARDPATH</code> and give the full command, or we have wrappers:</p> <pre><code>java -Xmx2g -jar $PICARDPATH/picard.jar PicardCommand TMP_DIR=$TMPDIR OPTION1=value1 OPTION2=value2...\n</code></pre> <p>The wrappers allow you to run commands like this - in this case our wrapper sets <code>TMP_DIR</code> for you as well: </p> <pre><code>PicardCommand OPTION1=value1 OPTION2=value2...\n</code></pre> <p>Temporary files: by default, Picard writes temporary files into <code>/tmp</code> rather than into <code>$TMPDIR</code>. These are not cleaned up after your job ends, and means future runs can fail because <code>/tmp</code> is full (and requesting more tmpfs in your job doesn't make it larger). If you run Picard with the full <code>java -jar</code> command then give Picard the <code>TMP_DIR=$TMPDIR</code> option as our example above does to get it to write in the correct place. </p>"},{"location":"Software_Guides/Other_Software/#quantum-espresso","title":"Quantum Espresso","text":"<p>Quantum Espresso is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modelling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials. </p> <p>The most recent version we have available on the clusters is 7.3.1. For  this version we have both the normal CPU/MPI variant and a GPU one. The GPU variant is only  available on the Myriad and Young clusters.</p> <p>For the CPU/MPI variant:</p> <pre><code>module unload compilers mpi gcc-libs\nmodule load gcc-libs/10.2.0\nmodule load compilers/gnu/10.2.0\nmodule load mpi/openmpi/4.0.5/gnu-10.2.0\nmodule load fftw/3.3.9/gnu-10.2.0\nmodule load quantum-espresso/7.3.1-cpu/gnu-10.2.0\n\n# Set the path here to where ever you keep your pseudopotentials.\nexport ESPRESSO_PSEUDO=$HOME/qe-psp\n\n# Gerun is our mpirun wrapper which sets the machinefile and number of\n# processes to the amount you requested with -pe mpi.\ngerun pw.x -in input.in &gt;output.out\n</code></pre> <p>For the GPU variant:</p> <pre><code>module unload compilers mpi gcc-libs\nmodule load gcc-libs/10.2.0\nmodule load compilers/nvidia/hpc-sdk/22.9\nmodule load quantum-espresso/7.3.1-gpu/nvidia-22.9\n\n# Set the path here to where ever you keep your pseudopotentials.\nexport ESPRESSO_PSEUDO=$HOME/qe-psp\n\n# Gerun is our mpirun wrapper which sets the machinefile and number of\n# processes to the amount you requested with -pe mpi.\ngerun pw.x -in input.in &gt;output.out\n</code></pre> <p>Older versions are still available, for 6.1:</p> <pre><code>module load xorg-utils\nmodule load quantum-espresso/6.1-impi/intel2017\n\n# Set the path here to where ever you keep your pseudopotentials.\nexport ESPRESSO_PSEUDO=$HOME/qe-psp\n\n# Gerun is our mpirun wrapper which sets the machinefile and number of\n# processes to the amount you requested with -pe mpi.\ngerun pw.x -in input.in &gt;output.out\n</code></pre>"},{"location":"Software_Guides/Other_Software/#repast-hpc","title":"Repast HPC","text":"<p>Repast for High Performance Computing (Repast HPC) is a next generation agent-based modelling system intended for large-scale distributed computing platforms. It implements the core Repast Simphony concepts (e.g. contexts and projections), modifying them to work in a parallel distributed environment. </p> <pre><code>module unload compilers\nmodule unload mpi\nmodule load compilers/gnu/4.9.2\nmodule load hdf/5-1.8.15/gnu-4.9.2\nmodule load netcdf/4.3.3.1/gnu-4.9.2\nmodule load netcdf-fortran/4.4.1/gnu-4.9.2\nmodule load mpi/openmpi/1.8.4/gnu-4.9.2\nmodule load python/2.7.9\nmodule load boost/1_54_0/mpi/gnu-4.9.2\nmodule load netcdf-c++/4.2/gnu-4.9.2\nmodule load repast-hpc/2.1/gnu-4.9.2\n</code></pre> <p>The module sets the environment variables <code>$REPAST_HPC_INCLUDE</code>, <code>$REPAST_HPC_LIB_DIR</code> and <code>$REPAST_HPC_LIB</code>.</p>"},{"location":"Software_Guides/Other_Software/#root","title":"ROOT","text":"<p>ROOT provides a set of OO frameworks for handling, analysing, and visualising large amounts of data. Included are specialised storage methods, methods for histograming, curve fitting, function evaluation, minimization etc. ROOT includes a built-in CINT C++ interpreter. </p> <pre><code>module unload compilers mpi\nmodule load compilers/gnu/4.9.2\nmodule load fftw/3.3.4/gnu-4.9.2\nmodule load gsl/1.16/gnu-4.9.2\nmodule load root/6.04.00/gnu-4.9.2\n\n# run root in batch mode\nroot -b -q myMacro.C &gt; myMacro.out\n</code></pre>"},{"location":"Software_Guides/Other_Software/#sas","title":"SAS","text":"<p>SAS is a statistics package providing a wide range of tools for data management, analysis and presentation. </p> <pre><code>cd $TMPDIR\n\nmodule load sas/9.4/64\n\n# copy all your input files into $TMPDIR\ncp ~/Scratch/sas_input/example1/* $TMPDIR\n\nsas example1.in\n\n# tar up all contents of $TMPDIR back into your space\ntar cvzf $HOME/Scratch/SAS_output/files_from_job_$JOB_ID.tgz $TMPDIR\n</code></pre>"},{"location":"Software_Guides/Other_Software/#starccm","title":"StarCCM+","text":"<p>StarCCM+ is a commercial CFD package that handles fluid flows, heat transfer,  stress simulations, and other common applications of such. </p> <p>Before running any StarCCM+ jobs on the clusters you must load the StarCCM+  module on a login node. This is so the module can set up two symbolic links  in your home directory to directories created in your Scratch area so that user  settings etc can be written by running jobs.</p> <pre><code>module load star-ccm+/13.06.012\n</code></pre> <p>Here is the jobscript example.</p> <pre><code># Request one license per core - makes sure your job doesn't start \n# running until sufficient licenses are free.\n#$ -l ccmpsuite=1\n\nmodule load star-ccm+/13.06.012\n\nstarccm+ -np $NSLOTS -machinefile $TMPDIR/machines -rsh ssh -batch my_input.sim\n</code></pre>"},{"location":"Software_Guides/Other_Software/#hfi-error","title":"hfi error","text":"<p>If you get an error like this:</p> <pre><code>hfi_wait_for_device: The /dev/hfi1_0 device failed to appear after 15.0 seconds: Connection timed out\n</code></pre> <p>then you need to add <code>-fabric ibv</code> to your options as shown in the example script.</p> <p>It is trying to use an OmniPath device on a cluster that has InfiniBand, so the  fabric needs to be changed. If you have this left over in jobscripts from Grace, you need to remove it on Kathleen.</p>"},{"location":"Software_Guides/Other_Software/#starcd","title":"StarCD","text":"<p>StarCD is a commercial package for modelling and simulating combustion and engine dynamics. </p> <p>You must request access to the group controlling StarCD access (legstarc) to use it. The license is owned by the Department of Mechanical Engineering who will need to approve your access request. </p> <pre><code># Request one license per core - makes sure your job doesn't start \n# running until sufficient licenses are free.\n#$ -l starsuite=1\n\nmodule load star-cd/4.28.050\n\n# run star without its tracker process as this causes multicore jobs to die early\nstar -notracker\n</code></pre> <p>StarCD uses IBM Platform MPI by default. You can also run StarCD simulations using Intel MPI by changing the command line to:</p> <pre><code>star -notracker -mpi=intel\n</code></pre> <p>Simulations run using Intel MPI may run faster than they do when using IBM Platform MPI. </p> <p>If being run on a diskless cluster without available <code>$TMPDIR</code> like Kathleen,  then StarCD will create a <code>$HPC_SCRATCH</code> location to store its temporary files when the module is loaded. In a job this is set to <code>$HOME/Scratch/STAR_ScrDirs/[randomLabel]</code> and it will make this directory and notify that it did this in your .e file.  You can delete the randomly-named directory after the job ends. To set the location yourself, after you load the module you can set it to any  other existing directory instead:</p> <pre><code>export HPC_SCRATCH=/path/to/desired/location\n</code></pre>"},{"location":"Software_Guides/Other_Software/#statamp","title":"Stata/MP","text":"<p>Stata is a statistics, data management, and graphics system. Stata/MP is the version of the package that runs on multiple cores. </p> <p>We have a sixteen user license of Stata/MP. Our license supports Stata running on up to four cores per job. </p> <pre><code># Select 4 OpenMP threads (the most possible)\n#$ -pe smp 4\n\ncd $TMPDIR\nmodule load stata/15\n\n# copy files to $TMPDIR\ncp myfile.do $TMPDIR\n\nstata-mp -b do myfile.do\n\n# tar up all contents of $TMPDIR back into your space\ntar zcvf $HOME/Scratch/Stata_output/files_from_job_$JOB_ID.tar.gz $TMPDIR\n</code></pre>"},{"location":"Software_Guides/Other_Software/#torch","title":"Torch","text":"<p>Torch is a scientific computing framework with wide support for machine learning algorithms that puts GPUs first. </p> <p>We provide a <code>torch-deps</code> module that contains the main Torch dependencies and creates a quick-install alias, <code>do-torch-install</code>. This uses Torch's installation script to git clone the current distribution and install LuaJIT, LuaRocks and Torch in <code>~/torch</code>. </p> <pre><code>module unload compilers mpi\nmodule load torch-deps\n\ndo-torch-install\n</code></pre> <p>You should load these same modules in your jobscript when using the version of torch this installs.</p>"},{"location":"Software_Guides/Other_Software/#turbomole","title":"Turbomole","text":"<p>Turbomole is an ab initio computational chemistry program that implements various quantum chemistry methods.  Turbomole has a Chemistry-wide license. Reserved application group <code>legtmole</code> for Chemistry users only.</p> <p>There are scripts you can use to generate Turbomole jobs for you:</p> <pre><code>/shared/ucl/apps/turbomole/turbomole-mpi.submit\n/shared/ucl/apps/turbomole/turbomole-smp.submit\n</code></pre> <p>They will ask you which version you want to use, how much memory, how many cores etc and set up and submit the job for you.</p> <p>Use the first for MPI jobs and the second for single-node shared memory threaded jobs.</p>"},{"location":"Software_Guides/Other_Software/#varscan","title":"VarScan","text":"<p>VarScan is a platform-independent mutation caller for targeted, exome, and whole-genome resequencing data generated on Illumina, SOLiD, Life/PGM, Roche/454, and similar instruments. </p> <pre><code>module load java/1.8.0_45\nmodule load varscan/2.3.9\n</code></pre> <p>Then to run VarScan, you should either prefix the .jar you want to run with $VARSCANPATH:</p> <pre><code>java -Xmx2g -jar $VARSCANPATH/VarScan.v2.3.9.jar OPTION1=value1 OPTION2=value2...\n</code></pre> <p>Or we provide wrappers, so you can run it this way instead:</p> <pre><code>varscan OPTION1=value1 OPTION2=value2...\n</code></pre>"},{"location":"Software_Guides/Other_Software/#vasp","title":"VASP","text":"<p>The Vienna Ab initio Simulation Package (VASP) is a computer program for atomic scale materials modelling, e.g. electronic structure calculations and quantum-mechanical molecular dynamics, from first principles.</p> <p>VASP is licensed software. To gain access, you need to email us letting us know  what email address you are named on a VASP license using. You can also mention  the name and email of the main VASP license holder and the license number if you  have it, though this is not necessary. We will then check in the VASP portal if  we can give you access. We will add you to the <code>legvasp5</code> or <code>legvasp6</code> reserved  application groups depending on which versions you are licensed for, and remove  you when VASP tell us you no longer have access.</p> <p>The VASP executables for current versions are named like this:</p> <ul> <li><code>vasp_gam</code> - optimised for gamma point calculations only</li> <li><code>vasp_std</code> - standard version</li> <li><code>vasp_ncl</code> - for non-collinear magnetic structure and/or spin-orbit coupling calculations </li> </ul>"},{"location":"Software_Guides/Other_Software/#vasp-5","title":"VASP 5","text":"<pre><code># vasp 5\nmodule unload -f compilers mpi\nmodule load compilers/intel/2017/update1\nmodule load mpi/intel/2017/update1/intel\nmodule load vasp/5.4.4-18apr2017/intel-2017-update1\n\n# Gerun is our mpirun wrapper which sets the machinefile and number of\n# processes to the amount you requested with -pe mpi.\ngerun vasp_std &gt; vasp_output.$JOB_ID\n</code></pre> <p>Note: although you can run VASP using the default Intel 2018 compiler this can lead to numerical errors in some types of simulation. In those cases we recommend switching to the specific compiler and MPI version used to build that install (mentioned at the end of the module name). We do this in the example above.</p> <p>Building your own VASP: You may also install your own copy of VASP in your home if you have access to the source, and we provide a simple VASP individual install script (tested with VASP 5.4.4, no patches). You need to download the VASP source code into your home directory and then you can run the script following the instructions at the top.</p>"},{"location":"Software_Guides/Other_Software/#vasp-6","title":"VASP 6","text":"<pre><code># vasp 6\nmodule unload -f compilers mpi\nmodule load compilers/intel/2019/update5\nmodule load mpi/intel/2019/update5/intel\nmodule load vasp/6.3.0-24Jan2022/intel-2019-update5\n\ngerun vasp_std &gt; vasp_output.$JOB_ID\n</code></pre>"},{"location":"Software_Guides/Other_Software/#vasp-6-gpu","title":"VASP 6 GPU","text":"<p>This is the OpenACC GPU port of VASP. The VASP documentation has some information about suitable numbers of MPI processes vs GPUs.</p> <pre><code># vasp 6 GPU\n\n# request a gpu\n#$ -l gpu=1\n\nmodule unload -f compilers mpi\nmodule load compilers/nvidia/hpc-sdk/22.1\nmodule load fftw/3.3.10/nvidia-22.1\nmodule load vasp/6.3.0-24Jan2022/nvidia-22.1-gpu\n\ngerun vasp_std &gt; vasp_output.$JOB_ID\n</code></pre>"},{"location":"Software_Guides/Other_Software/#xmds","title":"XMDS","text":"<p>XMDS allows the fast and easy solution of sets of ordinary, partial and stochastic differential equations, using a variety of efficient numerical algorithms. </p> <p>We have XMDS 3 and XMDS 2 installed.</p> <p>For XMDS 3.0.0 you will need to load the modules on a login node and run <code>xmds3-setup</code>  to set up XMDS.</p> <pre><code>module unload compilers\nmodule unload mpi\nmodule load compilers/gnu/4.9.2\nmodule load mpi/intel/2015/update3/gnu-4.9.2\nmodule load python3/3.7\nmodule load fftw/3.3.4-impi/gnu-4.9.2\nmodule load hdf/5-1.8.15/gnu-4.9.2\nmodule load xmds/3.0.0\n\n# run this on a login node to set up XMDS\nxmds3-setup\n</code></pre> <p>You can also build the current developmental version from SVN in your space by running  <code>create-svn-xmds3-inst</code>.</p> <p>For XMDS 2.2.2 you will need to load the modules on a login node and run <code>xmds2-setup</code>  to set up XMDS. </p> <pre><code>module unload compilers\nmodule unload mpi\nmodule load compilers/gnu/4.9.2\nmodule load mpi/intel/2015/update3/gnu-4.9.2\nmodule load python2/recommended\nmodule load fftw/3.3.4-impi/gnu-4.9.2\nmodule load hdf/5-1.8.15/gnu-4.9.2\nmodule load xmds/2.2.2\n\n# run this on a login node to set up XMDS\nxmds2-setup\n</code></pre> <p>Note that the <code>create-svn-xmds-inst</code> SVN install using the 2.2.2 modules will  no longer work since the release of XMDS 3.0.0 (see above to use that).</p>"},{"location":"Software_Guides/R/","title":"R","text":"<p>Type <code>module avail r</code> to see the currently available versions of R.</p> <p>The current version will always also exist as <code>r/recommended</code> - this is a module bundle and loading it will also load its many dependencies.</p> <p><code>module show r/recommended</code> shows you exactly which versions loading this module will give you.</p> <p>R can be run on a single core or multithreaded using many cores (some commands can run threaded automatically, otherwise you may wish to look at R's <code>parallel</code> package). </p> <p><code>doMPI</code>, <code>Rmpi</code> and <code>snow</code> allow multi-node parallel jobs using MPI to be run.</p> <p>List of additional R packages shows you what packages are installed and available for the current R version.</p>"},{"location":"Software_Guides/R/#setup","title":"Setup","text":"<p>Before you can use R interactively, you need to load the R module using: </p> <pre><code>module -f unload compilers mpi gcc-libs\nmodule load r/recommended\n</code></pre>"},{"location":"Software_Guides/R/#example-serial-jobscript","title":"Example serial jobscript","text":"<p>This script runs R using only one core.</p> <pre><code>#!/bin/bash -l\n\n# Example jobscript to run a single core R job\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n# Change this to suit your requirements.\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM. Change this to suit your requirements.\n#$ -l mem=1G\n\n# Set the name of the job. You can change this if you wish.\n#$ -N R_job_1\n\n# Set the working directory to somewhere in your scratch space.  This is\n# necessary because the compute nodes cannot write to your $HOME\n# NOTE: this directory must exist.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/R_output\n\n# Your work must be done in $TMPDIR (serial jobs particularly) \ncd $TMPDIR\n\n# Load the R module and run your R program\nmodule -f unload compilers mpi gcc-libs\nmodule load r/recommended\n\nR --no-save &lt; /home/username/Scratch/myR_job.R &gt; myR_job.out\n\n# Preferably, tar-up (archive) all output files to transfer them back \n# to your space. This will include the R_output file above.\ntar zcvf $HOME/Scratch/R_output/files_from_job_$JOB_ID.tgz $TMPDIR\n\n# Make sure you have given enough time for the copy to complete!\n</code></pre> <p>You will need to change the <code>-wd /home/&lt;your_UCL_id&gt;/Scratch/R_output</code> location and the location of your R input file, called <code>myR_job.R</code> here.  <code>myR_job.out</code> is the file we are redirecting the output into. The output file is saved in the tar archive produced by the last command in the runscript and will be in <code>$HOME/Scratch/R_output</code>. </p> <p>If your jobscript is called <code>run-R.sh</code> then your job submission command would be:</p> <pre><code>qsub run-R.sh\n</code></pre>"},{"location":"Software_Guides/R/#example-shared-memory-threaded-parallel-job","title":"Example shared memory threaded parallel job","text":"<p>This script uses multiple cores on the same node. It cannot run across multiple nodes.</p> <pre><code>#!/bin/bash -l\n\n# Example jobscript to run an OpenMP threaded R job across multiple cores on one node.\n# This may be using the foreach packages foreach(...) %dopar% for example.\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n# Change this to suit your requirements.\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM per core. Change this to suit your requirements.\n#$ -l mem=1G\n\n# Set the name of the job. You can change this if you wish.\n#$ -N R_jobMC_2\n\n# Select 12 threads. The number of threads here must equal the number of worker \n# processes in the registerDoMC call in your R program.\n#$ -pe smp 12\n\n# Set the working directory to somewhere in your scratch space.  This is\n# necessary because the compute nodes cannot write to your $HOME\n# NOTE: this directory must exist.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/R_output\n\n# Your work must be done in $TMPDIR\ncd $TMPDIR\n\n# Load the R module and run your R program\nmodule -f unload compilers mpi gcc-libs\nmodule load r/recommended\n\nR --no-save &lt; /home/username/Scratch/myR_job.R &gt; myR_job.out\n\n# Preferably, tar-up (archive) all output files to transfer them back \n# to your space. This will include the R_output file above.\ntar zcvf $HOME/Scratch/R_output/files_from_job_$JOB_ID.tgz $TMPDIR\n\n# Make sure you have given enough time for the copy to complete!\n</code></pre> <p>You will need to change the <code>-wd /home/&lt;your_UCL_id&gt;/Scratch/R_output</code> location and the location of your R input file, called <code>myR_job.R</code> here.  <code>myR_job.out</code> is the file we are redirecting the output into. The output file is saved in the tar archive produced by the last command in the runscript and will be in <code>$HOME/Scratch/R_output</code>.</p> <p>If your jobscript is called <code>run-R.sh</code> then your job submission command would be:</p> <pre><code>qsub run-R.sh\n</code></pre>"},{"location":"Software_Guides/R/#example-multi-node-parallel-job-using-rmpi-and-snow","title":"Example multi-node parallel job using Rmpi and snow","text":"<p>This script uses Rmpi and snow to allow it to run across multiple nodes using MPI.</p> <pre><code>#!/bin/bash -l\n\n# Example jobscript to run an R MPI parallel job\n\n# Request ten minutes of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=0:10:0\n\n# Request 1 gigabyte of RAM per process.\n#$ -l mem=1G\n\n# Request 15 gigabytes of TMPDIR space per node (default is 10 GB)\n#$ -l tmpfs=15G\n\n# Set the name of the job.\n#$ -N snow_monte_carlo\n\n# Select the MPI parallel environment with 32 processes\n#$ -pe mpi 32\n\n# Set the working directory to somewhere in your scratch space.  This is\n# necessary because the compute nodes cannot write to your $HOME\n# NOTE: this directory must exist.\n# Replace \"&lt;your_UCL_id&gt;\" with your UCL user ID\n#$ -wd /home/&lt;your_UCL_id&gt;/Scratch/R_output\n\n# Load the R module\nmodule -f unload compilers mpi gcc-libs\nmodule load r/recommended\n\n# Copy example files in to the working directory (not necessary if already there)\ncp ~/R/Examples/snow_example.R .\ncp ~/R/Examples/monte_carlo.R .\n\n# Run our MPI job. GERun is our wrapper for mpirun, which launches MPI jobs  \ngerun RMPISNOW &lt; snow_example.R &gt; snow.out.${JOB_ID}\n</code></pre> <p>The output file is saved in <code>$HOME/Scratch/R_examples/snow/snow.out.${JOB_ID}</code>.</p> <p>If your jobscript is called <code>run-R-snow.sh</code> then your job submission command would be:</p> <pre><code>qsub run-R-snow.sh\n</code></pre>"},{"location":"Software_Guides/R/#example-r-script-using-rmpi-and-snow","title":"Example R script using Rmpi and snow","text":"<p>This R script has been written to use Rmpi and snow and can be used with the above jobscript. It is <code>snow_example.R</code> above.</p> <pre><code>#Load the snow and random number package.\nlibrary(snow)\nlibrary(Rmpi)\n\n# This example uses the already installed LEcuyers RNG library(rlecuyer)\nlibrary(rlecuyer)\n\n# Set up our input/output\nsource('./monte_carlo.R')\nsink('./monte_carlo_output.txt')\n\n# Get a reference to our snow cluster that has been set up by the RMPISNOW\n# script.\ncl &lt;- getMPIcluster ()\n\n# Display info about each process in the cluster\nprint(clusterCall(cl, function() Sys.info()))\n\n# Load the random number package on each R process\nclusterEvalQ (cl, library (rlecuyer))\n\n# Generate a seed for the pseudorandom number generator, unique to each\n# processor in the cluster.\n\n#Uncomment below line for default (unchanging) random number seed.\n#clusterSetupRNG(cl, type = 'RNGstream')\n\n#The lines below set up a time-based random number seed.  Note that \n#this only demonstrates the virtues of changing the seed; no guarantee\n#is made that this seed is at all useful.  Comment out if you uncomment\n#the above line.\ns &lt;- sum(strtoi(charToRaw(date()), base = 32))\nclusterSetupRNGstream(cl, seed=rep(s,6))\n\n#Choose which of the following blocks best fit your own needs.\n\n# BLOCK 1\n# Set up the input to our Monte Carlo function.\n# Input is identical across the batch, only RNG seed has changed. \n# For this example, both clusters will roll one die. \n\nnrolls &lt;- 2\nprint(\"Roll the dice once...\")\noutput &lt;- clusterCall(cl, monte_carlo, nrolls)\noutput\nprint(\"Roll the dice again...\")\noutput &lt;- clusterCall(cl, monte_carlo, nrolls)\noutput\n\n# Output should show the results of two rolls of a six-sided die.\n\n#BLOCK 2\n# Input is different for each processor\nprint(\"Second example: coin flip plus 3 dice\")\ninput &lt;- array(1:2)  # Set up array of inputs, with each entry\ninput[1] &lt;- 1        #   corresponding to one processor.\ninput[2] &lt;- 3\nparameters &lt;- array(1:2)  # Set up inputs that will be used by each cluster.\nparameters[1] &lt;- 2        #   These will be passed to monte_carlo as its\nparameters[2] &lt;- 6        #   second argument.\noutput &lt;- clusterApply(cl, input, monte_carlo, parameters)\n\n# Output should show the results of a coin flip and the roll of three \n# six-sided die.\n\n# Output the output.\noutput\n\ninputStrings &lt;- array(1:2)\ninputStrings[1] &lt;- 'abc'\ninputStrings[2] &lt;- 'def'\noutput &lt;- clusterApply(cl, inputStrings, paste, 'foo')\noutput\n\n#clusterEvalQ(cl, sinkWorkerOutput(\"snow_monte_carlo.out\"))\n\n# Clean up the cluster and release the relevant resources.\nstopCluster(cl)\nsink()\n\nmpi.quit()\n</code></pre> <p>This is <code>monte_carlo.R</code> which is called by <code>snow_example.R</code>:</p> <pre><code>monte_carlo &lt;- function(x, numsides=6){\n  streamname &lt;- .lec.GetStreams ()\n  dice &lt;- .lec.uniform.int(streamname[1], n = 1, a=1, b=numsides)\n  outp &lt;- sum(dice)\n  return(outp)\n}\n</code></pre> <p>This example was based on SHARCNET's Using R and MPI.</p>"},{"location":"Software_Guides/R/#using-your-own-r-packages","title":"Using your own R packages","text":"<p>If we do not have R packages installed centrally that you wish to use, you can  install them in your space on the cluster and tell R where to find them.</p> <p>First you need to tell R where to install your package to and where to look for user-installed packages, using the R library path.</p>"},{"location":"Software_Guides/R/#set-your-r-library-path","title":"Set your R library path","text":"<p>There are several ways to modify your R library path so you can pick up packages that you have installed in your own space.</p> <p>The easiest way is to add them to the <code>R_LIBS</code> environment variable (insert the correct path):</p> <pre><code>export R_LIBS=/your/local/R/library/path:$R_LIBS\n</code></pre> <p>This is a colon-separated list of directories that R will search through. </p> <p>Setting that in your terminal will let you install to that path from inside R and  should also be put in your jobscript (or your <code>.bashrc</code>) when you submit a job  using those libraries. This appends your directory to the existing value of  <code>$R_LIBS</code> rather than overwriting it so the centrally-installed libraries can still be found.</p> <p>You can also change the library path for a session from within R:</p> <pre><code>.libPaths(c('~/MyRlibs',.libPaths()))\n</code></pre> <p>This puts your directory at the beginning of R's search path, and means that <code>install.packages()</code> will automatically put packages there and the <code>library()</code> function will find libraries in your local directory.</p>"},{"location":"Software_Guides/R/#install-an-r-package","title":"Install an R package","text":"<p>To install, after setting your library path:</p> <p>From inside R, you can do</p> <pre><code>install.packages('package_name', repos=\"http://cran.r-project.org\")\n</code></pre> <p>Or if you have downloaded the tar file, you can do</p> <pre><code>R CMD INSTALL -l /home/username/your_R_libs_directory package.tar.gz\n</code></pre> <p>If you want to keep some libraries separate, you can have multiple colon-separated paths in your <code>$R_LIBS</code> and specify which one you want to install into with <code>R CMD INSTALL</code>.</p>"},{"location":"Software_Guides/R/#bioconductor","title":"BioConductor","text":"<p>If you are installing extra packages for BioConductor, check that you are using the same version that the R module you have loaded is using.</p> <p>Eg. you can find the BioConductor 3.15 package downloads here.</p>"},{"location":"Software_Guides/RFAA/","title":"RoseTTAFold All-Atom","text":"<p>The RoseTTAFold All-Atom (RFAA) developed by Baker Lab is an inference pipeline for protein structure prediction.</p> <p>The version of it that is current on the 4th of March 2025 is installed on Myriad, available in environment modules as <code>rfaa/20250304</code>.</p> <p>Important</p> <p>RFAA depends on a piece of software called SignalP 6.0h which is licensed under an academic licence that explicitly forbids commercial use. You therefore may not under any circumstances use the RFAA pipeline as it is presently installed for commercial workloads. Where any ambiguity exists about the non-commercial nature of your work you must cease using RFAA immediately and contact rc-support to discuss paths forward. Violation of software licenses is a violation of the UCL Computer terms of use which may result in removal of access to all IT systems.</p>"},{"location":"Software_Guides/RFAA/#using-rfaa","title":"Using RFAA","text":"<p>Important</p> <p>RFAA contains an embedded miniforge3 install and so conflicts with other versions of Python.</p> <p>When run, RFAA relies on some very large databases (about 2.5TiB) as well as the model weights from the original paper. These, and other files are expected to be in a predictable structure in the input directory which you run the code in. To facilitate this, we have placed those reference sets in a central location and provided a shell script <code>prepare_rfaa_input_directory</code> with which will either make appropriate file-system links in the current working directory, or, if given an argument will do so there so:</p> <pre><code>$ module load rfaa/20250304\n\nPlease note that the license of SignalP 6.0h which is used\nin RFAA ONLY allows its use for non-commercial work.\nThis means you may only use RFAA for non-commercial work.\n\n$ cd ~/Scratch/\n$ mkdir rfaa_input\n$ prepare_rfaa_input_directory rfaa_input/\nPreparing rfaa_input/ as an RFAA input directory.\n\nLinking databases to rfaa_input/ directory...\n/shared/ucl/apps/RoseTTAFold-All-Atom_db/bfd &lt;- rfaa_input//bfd\n/shared/ucl/apps/RoseTTAFold-All-Atom_db/pdb100_2021Mar03 &lt;- rfaa_input//pdb100_2021Mar03\n/shared/ucl/apps/RoseTTAFold-All-Atom_db/UniRef30_2020_06 &lt;- rfaa_input//UniRef30_2020_06\n\nLinking weights to rfaa_input/ directory...\n/shared/ucl/apps/RoseTTAFold-All-Atom_db/RFAA_paper_weights.pt &lt;- rfaa_input//RFAA_paper_weights.pt\n\nDone.\n\nAssuming you have the environment module correctly loaded, you can now run RFAA from inside this directory.\n\n$ cd rfaa_input/\n$ ls -lah\ntotal 20K\ndrwx------  2 uccaoke uccapc3 4.0K Mar 13 16:15 .\ndrwxr-xr-x 16 uccaoke uccapc3  12K Mar 13 16:15 ..\nlrwxrwxrwx  1 uccaoke uccapc3   44 Mar 13 16:15 bfd -&gt; /shared/ucl/apps/RoseTTAFold-All-Atom_db/bfd\nlrwxrwxrwx  1 uccaoke uccapc3   57 Mar 13 16:15 pdb100_2021Mar03 -&gt; /shared/ucl/apps/RoseTTAFold-All-Atom_db/pdb100_2021Mar03\nlrwxrwxrwx  1 uccaoke uccapc3   62 Mar 13 16:15 RFAA_paper_weights.pt -&gt; /shared/ucl/apps/RoseTTAFold-All-Atom_db/RFAA_paper_weights.pt\nlrwxrwxrwx  1 uccaoke uccapc3   57 Mar 13 16:15 UniRef30_2020_06 -&gt; /shared/ucl/apps/RoseTTAFold-All-Atom_db/UniRef30_2020_06\n$ \n</code></pre> <p>You then need to prepare your input files which are YAML files. We will take the <code>protein</code> example.</p> <pre><code>cp /shared/ucl/apps/rfaa/20250304/RoseTTAFold-All-Atom/rf2aa/config/inference/protein.yaml .\ncp /shared/ucl/apps/rfaa/20250304/RoseTTAFold-All-Atom/rf2aa/config/inference/base.yaml .\n</code></pre> <p>If we look at <code>protein.yaml</code>, it depends on <code>base.yaml</code>:</p> <pre><code>defaults:\n  - base\n\njob_name: \"7u7w_protein\"\nprotein_inputs: \n  A:\n    fasta_file: examples/protein/7u7w_A.fasta\n</code></pre> <p>We also need the <code>fasta</code> file, so we can copy this to our current directory and modify <code>protein.yaml</code> so it can find it.</p> <pre><code>$ cp /shared/ucl/apps/rfaa/20250304/RoseTTAFold-All-Atom/examples/protein/7u7w_A.fasta .\n</code></pre> <pre><code>defaults:\n  - base\n\njob_name: \"7u7w_protein\"\nprotein_inputs: \n  A:\n    fasta_file: 7u7w_A.fasta\n</code></pre> <p>If we are on a compute node (preferrably with a GPU), booked interactively with <code>qrsh</code> we can then directly run the pipeline:</p> <pre><code>python3 -m rf2aa.run_inference --config-path=$(pwd) --config-name protein\n</code></pre> <p>Note that we specify where to find the config files with the <code>--config-path</code> option and give it a configuration name to run with <code>--config-name</code>.</p> <pre><code>$ python3 -m rf2aa.run_inference --config-path=$(pwd) --config-name protein\n/shared/ucl/apps/rfaa/20250304/miniforge3/envs/RFAA/lib/python3.10/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'protein': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n  warnings.warn(msg, UserWarning)\nUsing the cif atom ordering for TRP.\nmake_msa.sh 7u7w_A.fasta 7u7w_protein/A 4 64  pdb100_2021Mar03/pdb100_2021Mar03\nPredicting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:04&lt;00:00,  4.52s/sequences]\nRunning HHblits against UniRef30 with E-value cutoff 1e-10\n- 16:46:14.500 INFO: Input file = 7u7w_protein/A/hhblits/t000_.1e-10.a3m\n\n- 16:46:14.500 INFO: Output file = 7u7w_protein/A/hhblits/t000_.1e-10.id90cov75.a3m\n\n- 16:46:14.727 WARNING: Maximum number 100000 of sequences exceeded in file 7u7w_protein/A/hhblits/t000_.1e-10.a3m\n\n- 16:46:48.226 INFO: Input file = 7u7w_protein/A/hhblits/t000_.1e-10.a3m\n\n- 16:46:48.226 INFO: Output file = 7u7w_protein/A/hhblits/t000_.1e-10.id90cov50.a3m\n\n- 16:46:48.451 WARNING: Maximum number 100000 of sequences exceeded in file 7u7w_protein/A/hhblits/t000_.1e-10.a3m\n\nRunning PSIPRED\nRunning hhsearch\n$ ls\nu7w_A.fasta  7u7w_protein  7u7w_protein_aux.pt  7u7w_protein.pdb  base.yaml  bfd  outputs  pdb100_2021Mar03  protein.yaml  RFAA_paper_weights.pt  UniRef30_2020_06\n</code></pre> <p>When the process has run, we should find in our current working directory both a PDB file (<code>7u7w_protein.pdb</code>)  with the structure and a PyTorch file (<code>7u7w_protein_aux.pt</code>) with some statistical information about the run, as per the documentation.</p>"},{"location":"Software_Guides/RFAA/#writing-a-job-script","title":"Writing a job script","text":"<p>Assuming we have our input YAML and FASTA files in a directory inside our home directory called <code>rfaa_input</code>, and the configuration is <code>input.yaml</code>, a job script for RFAA looks like this:</p> <pre><code>#!/bin/bash -l\n\n# Batch script to run RoseTTAFold All-Atom on Myriad.\n\n# Request one GPU\n#$ -l gpu=1\n\n# Request 18 cores (half a node)\n#$ -pe smp 18\n\n# Request two hours of wallclock time (format hours:minutes:seconds).\n#$ -l h_rt=2:0:0\n\n# Request 5 gigabyte of RAM per core.\n#$ -l mem=5G\n\n# Set the name of the job.\n#$ -N RFAA\n\n# Set the working directory to the current working directory.\n#$ -cwd # You may wish to change this to a specific directory\n\nmodule load rfaa/20250304\n\nprepare_rfaa_input_directory # sets up current directory.\n\n# Copy input files into our directory.\n# Assume we have our input.yaml and fasta files in ~/rfaa_input - amend.\ncp ${HOME}/rfaa_input/*.yaml ${HOME}/rfaa_input/*.fasta .\n\npython3 -m rf2aa.run_inference --config-path=$(pwd) --config-name input\n</code></pre>"},{"location":"Software_Guides/Singularity/","title":"Using Apptainer (Singularity) on our clusters","text":"<p>Myriad has Apptainer installed, which will be rolled out to our other clusters at a later date.  The other clusters still have Singularity. You can use containers you have downloaded in your space.</p>"},{"location":"Software_Guides/Singularity/#apptainer","title":"Apptainer","text":"<p>Run <code>apptainer --version</code> to see which version we currently have installed.</p>"},{"location":"Software_Guides/Singularity/#set-up-cache-locations","title":"Set up cache locations","text":"<p>Use this command to load the <code>apptainer</code> module to set up a suitable environment:</p> <pre><code>module load apptainer\n</code></pre> <p>Or set the locations manually:</p> <pre><code># Create a build directory to build your own containers.\nmkdir -p $XDG_RUNTIME_DIR/$USER_apptainerbuild\n\n# Set $APPTAINER_TMPDIR to this local build location.\nexport APPTAINER_TMPDIR=$XDG_RUNTIME_DIR/$USER_apptainerbuild\n\n# Create a .apptainer directory in your Scratch\nmkdir $HOME/Scratch/.apptainer\n\n# Set the cache directory to your Scratch.\nexport APPTAINER_CACHEDIR=$HOME/Scratch/.apptainer\n</code></pre> <p>You probably want to add those <code>export</code> statements to your <code>.bashrc</code> under <code># User specific aliases and functions</code> so those environment variables are always set when you log in.</p>"},{"location":"Software_Guides/Singularity/#bind-locations","title":"Bind locations","text":"<p>Your $HOME and Scratch directories are bound automatically so they are available from inside your containers.</p> <p>For more information on these options, have a look at the Apptainer documentation:</p> <ul> <li>Apptainer user guide</li> <li>Apptainer bind paths and mounts</li> <li>Apptainer build environment</li> </ul>"},{"location":"Software_Guides/Singularity/#backwards-compatibility","title":"Backwards compatibility","text":"<p>The <code>singularity</code> command still exists as a symbolic link and will run <code>apptainer</code>.</p> <p>Apptainer will use the older <code>$SINGULARITY_</code> environment variables if the <code>$APPTAINER_</code> versions are not set.</p>"},{"location":"Software_Guides/Singularity/#build-remote-is-no-longer-available","title":"<code>build --remote</code> is no longer available","text":"<p>If you were building remote containers on Sylabs using</p> <pre><code>singularity build --remote\n</code></pre> <p>this functionality is no longer available in Apptainer. You can still log in to Sylabs via the web interface, build containers on it and then pull them down with Apptainer.</p>"},{"location":"Software_Guides/Singularity/#building-your-own-containers","title":"Building your own containers","text":"<p>With Apptainer you can build containers directly on our clusters without additional permissions,  as long as they use a local filesystem and not home or Scratch. </p> <p>Our default setup uses <code>$XDG_RUNTIME_DIR</code> on the local disk of the login nodes, or <code>$TMPDIR</code> on a  compute node (local disk on the node, on clusters that are not diskless).</p> <p>If you try to build a container on a parallel filesystem, it will fail with a number of permissions errors.</p>"},{"location":"Software_Guides/Singularity/#singularity","title":"Singularity","text":"<p>Run <code>singularity --version</code> to see which version we currently have installed.</p> <p>Singularity update to Apptainer</p> <p>On Myriad, we are updating to Singularity to Apptainer. This update will occur on 14th  November during a planned outage</p> <p>This update may affect any containers that are currently downloaded, so users will have to test them to check their workflow still functions correctly after the update. We expect most to work  as before, but cannot confirm this.</p> <p>A Singularity command that will no longer be available in Apptainer is  <code>singularity build --remote</code>. If any of you have workflows that depend on this,  please email rc-support@ucl.ac.uk. We are currently looking into how we would provide  equivalent functionality.</p> <p>Updates to the other clusters will follow, dates tbc.</p>"},{"location":"Software_Guides/Singularity/#set-up-cache-locations-and-bind-directories","title":"Set up cache locations and bind directories","text":"<p>The cache directories should be set to somewhere in your space so they don't fill up <code>/tmp</code> on  the login nodes.</p> <p>The bindpath mentioned below specifies what directories are made available inside the container -  only your home is bound by default so you need to add Scratch.</p> <p>You can either use the <code>singularity-env</code> environment module for this, or run the commands manually.</p> <pre><code>module load singularity-env\n</code></pre> <p>or:</p> <pre><code># Create a .singularity directory in your Scratch\nmkdir $HOME/Scratch/.singularity\n\n# Create cache subdirectories we will use / export\nmkdir $HOME/Scratch/.singularity/tmp\nmkdir $HOME/Scratch/.singularity/localcache\nmkdir $HOME/Scratch/.singularity/pull\n\n# Set all the Singularity cache dirs to Scratch\nexport SINGULARITY_CACHEDIR=$HOME/Scratch/.singularity\nexport SINGULARITY_TMPDIR=$SINGULARITY_CACHEDIR/tmp\nexport SINGULARITY_LOCALCACHEDIR=$SINGULARITY_CACHEDIR/localcache\nexport SINGULARITY_PULLFOLDER=$SINGULARITY_CACHEDIR/pull\n\n# Bind your Scratch directory so it is accessible from inside the container\n#      and the temporary storage jobs are allocated\nexport SINGULARITY_BINDPATH=/scratch/scratch/$USER,/tmpdir\n</code></pre> <p>Different subdirectories are being set for each cache so you can tell which files came from where.</p> <p>You probably want to add those <code>export</code> statements to your <code>.bashrc</code> under <code># User specific aliases and functions</code> so those environment variables are always set when you log in.</p> <p>For more information on these options, have a look at the Singularity documentation:</p> <ul> <li>Singularity user guide</li> <li>Singularity Bind Paths and Mounts</li> <li>Singularity Build Environment</li> </ul>"},{"location":"Software_Guides/Singularity/#downloading-and-running-a-container","title":"Downloading and running a container","text":"<p>Assuming you want to run an existing container, first you need to pull it from somewhere online that provides it:</p> <pre><code># make sure we set up apptainer as above\nmodule load apptainer\n\n# get image from location and call it hello-world.sif in our current directory\napptainer pull hello-world.sif shub://vsoch/hello-world\n</code></pre> <p>Run the container.</p> <pre><code>apptainer run hello-world.sif\n</code></pre> <p>Run a specific command within our container.</p> <pre><code>apptainer exec hello-world.sif /bin/echo Hello World!\n</code></pre> <p>You can run containers inside jobscripts in the same way.</p> <p>Useful links:</p> <ul> <li>Carpentries Incubator lesson: Reproducible computational environments using containers: Introduction to Singularity</li> <li>Using NVIDIA Grid Cloud Containers on our clusters</li> </ul>"},{"location":"Software_Guides/Singularity/#docker-containers","title":"Docker containers","text":"<p>You can use Singularity to run Docker containers. Docker itself is not suitable for use on a multi-user HPC system, but Singularity can convert and run Docker containers for you.</p> <pre><code>apptainer pull python-3.9.6.sif docker://python:3.9.6-slim-buster\n</code></pre> <p>In this case, <code>apptainer pull</code> is downloading a Docker image, and also converting it into a format that Apptainer uses. You then use <code>apptainer run</code> or <code>apptainer exec</code> on the .sif image as above.</p>"},{"location":"Software_Guides/Singularity/#graphical-containers-in-interactive-jobs","title":"Graphical containers in interactive jobs","text":"<p>If you are trying to run a graphical application from inside a container in an  interactive job and it is failing with errors about not being able to open a  display, you will need to: </p> <ul> <li>ssh in to the cluster with X-forwarding on  as normal </li> <li>request an interactive job using <code>qrsh</code> </li> <li>bind mount your <code>$HOME/.Xauthority</code> file inside the container</li> </ul> <p>To do the bind mount, you could add it to your <code>$APPTAINER_BINDPATH</code></p> <pre><code>export APPTAINER_BINDPATH=$APPTAINER_BINDPATH,$HOME/.Xauthority\n</code></pre> <p>or you can pass it in with the <code>\u2013-bind=$HOME/.Xauthority</code> option to <code>singularity shell</code> or  <code>singularity exec</code>.</p>"},{"location":"Software_Guides/Singularity/#building-containers-with-apptainer","title":"Building containers with Apptainer","text":"<p>Requires Apptainer</p> <p>This requires Apptainer and will not work in clusters which still use Singularity.</p> <p>One of the nice new features of Apptainer is it is now possible to build containers on the login/compute nodes of our clusters and you no longer need to have a separate Linux machine (or the Singularity build service) to do so.</p>"},{"location":"Software_Guides/Singularity/#set-up-your-environment","title":"Set up your environment","text":"<p>Load the <code>apptainer</code> module:</p> <pre><code>module load apptainer\n</code></pre>"},{"location":"Software_Guides/Singularity/#write-a-definition-file","title":"Write a definition file","text":"<p>Unfortunately Apptainer doesn't (yet) support Dockerfiles and uses its own syntax. </p> <p>There are example files in this repository and we will use the \"AlmaLinux Python\" example here.</p> <p>The two files you need for this example are <code>alma_python.def</code> and <code>pi.py</code>.</p> <p>This example builds an image based on the latest AlmaLinux (at time of writing 9.3) container in Dockerhub:</p> <pre><code>Bootstrap: docker\nFrom: almalinux:latest\nStage: build\n\n%files\n    ./pi.py /pi.py\n\n%post\n    chmod 644 /pi.py\n    dnf update -y\n    dnf install -y 'dnf-command(config-manager)'\n    dnf config-manager --set-enabled crb\n    dnf install -y epel-release\n    dnf install -y python3 python3-pip python3-virtualenv\n    virtualenv /runtime\n    source /runtime/bin/activate\n    pip install --upgrade pip\n    pip install numba\n%runscript\n    source /runtime/bin/activate\n    python3 /pi.py\n</code></pre> <p>It copys the <code>pi.py</code> file into the image (the <code>%files</code> section), updates the packages in the container, installs the Code Ready Builder and EPEL repositories, and uses them to install <code>pip</code> and <code>virtualenv</code> (the <code>%post</code> section).  Finally it uses those two tools to build a virtual environment with Numba to run the code in. </p> <p>Finally it sets the container's run command to activate the virtual environment with the dependencies and execute <code>pi.py</code> (the <code>%runscript</code> section).</p>"},{"location":"Software_Guides/Singularity/#build-container","title":"Build container","text":"<p>To the build the container simply run:</p> <pre><code>apptainer build --fakeroot alma_python.sif alma_python.def\n</code></pre> <p>The <code>--fakeroot</code> option is important because of course as a normal user on our HPC systems you are not <code>root</code>.</p>"},{"location":"Software_Guides/Singularity/#running-the-container","title":"Running the container","text":"<p>To run the container:</p> <pre><code>apptainer run alma_python.sif \n</code></pre>"},{"location":"Supplementary/Compiling_Your_Code/","title":"Compiling Your Code","text":""},{"location":"Supplementary/Compiling_Your_Code/#download-your-code","title":"Download your code","text":"<p>Use wget or curl to download the source code for the software you want to install to your account. There might be binaries available, but they often won't work on our clusters because they were compiled for other machines with other library versions available. Use tar or unzip or similar depending on archive type to uncompress your source code.</p> <pre><code>wget\u00a0https://www.example.com/program.tar.gz\ntar\u00a0-xvf\u00a0program.tar.gz\n</code></pre> <p>You won't be able to use a package manager like yum, you'll need to follow the manual installation instructions for a user-space install (not using sudo).</p>"},{"location":"Supplementary/Compiling_Your_Code/#set-up-your-modules","title":"Set up your modules","text":"<p>Before you start compiling, you need to make sure you have the right compilers, libraries and other tools available for your software. If you haven't changed anything, you will have the default modules loaded.</p> <p>Check what the instructions for your software tell you about compiling it. If the website doesn't say much, the source code will hopefully have a <code>README</code> or <code>INSTALL</code> file.</p> <p>You may want to use a different compiler - the default is the Intel compiler.</p> <p><code>module avail compilers</code> will show you all the compiler modules available. Most Open Source software tends to assume you're using GCC and OpenMPI (if it uses MPI) and is most tested with that combination, so if it doesn't specify you may want to begin there (do check what the newest modules available are):</p> <pre><code>module\u00a0unload\u00a0compilers\u00a0mpi\u00a0mkl\nmodule\u00a0load\u00a0compilers/gnu/4.9.2  \nmodule\u00a0load\u00a0mpi/openmpi/1.10.1/gnu-4.9.2\n</code></pre> <p>Useful resources:</p> <ul> <li>Modules pt 1 (moodle) (UCL users)</li> <li>Modules pt 2 (moodle) (UCL users)</li> <li>Modules pt 1 (mediacentral) (non-UCL users)</li> <li>Modules pt 2 (mediacentral) (non-UCL users)</li> </ul>"},{"location":"Supplementary/Compiling_Your_Code/#available-compilers","title":"Available compilers","text":"<p>The following compilers are available and supported on our clusters:</p> <ul> <li>Intel C, C++ and Fortran</li> <li>GNU C, C++ and Fortran</li> </ul> <p>We currently have a limited number of licenses for the Intel compilers so only a certain number of users can use them simultaneously. This means that your compilation may fail with an error complaining about not being able to obtain a valid license. If this happens, simply wait for a few minutes and try again.</p> <p>In addition to the supported tools, there are a number of tools installed on our clusters which are not supported (for example the PGI compilers) which were installed to build certain supported packages. Users who use the unsupported packages do so at their own risk.</p>"},{"location":"Supplementary/Compiling_Your_Code/#build-systems","title":"Build systems","text":"<p>Most software will use some kind of build system to manage how files are compiled and linked and in what order. Here are a few common ones.</p>"},{"location":"Supplementary/Compiling_Your_Code/#automake-configure","title":"Automake configure","text":"<p>Automake will generate the Makefile for you and hopefully pick up sensible options through configuration. You can give it an install prefix to tell it where to install (or you can build it in place and not use make install at all).</p> <pre><code>./configure\u00a0--prefix=/home/username/place/you/want/to/install  \nmake  \n#\u00a0if\u00a0it\u00a0has\u00a0a\u00a0test\u00a0suite,\u00a0good\u00a0idea\u00a0to\u00a0use\u00a0it  \nmake\u00a0test\u00a0  \nmake\u00a0install\n</code></pre> <p>If it has more configuration flags, you can use <code>./configure --help</code> to view them.</p> <p>Usually configure will create a config.log: you can look in there to find if any tests have failed or things you think should have been picked up haven't.</p>"},{"location":"Supplementary/Compiling_Your_Code/#cmake","title":"CMake","text":"<p>CMake is another build system. It will have a CMakeFile or the instructions will ask you to use cmake or ccmake rather than make. It also generates Makefiles for you. <code>ccmake</code> is a terminal-based interactive interface where you can see what variables are set to and change them, then repeatedly configure until everything is correct, generate the Makefile and quit. <code>cmake</code> is the commandline version. The process tends to go like this:</p> <pre><code>ccmake\u00a0CMakeLists.txt  \n#\u00a0press\u00a0c\u00a0to\u00a0configure\u00a0-\u00a0will\u00a0pick\u00a0up\u00a0some\u00a0options  \n#\u00a0press\u00a0t\u00a0to\u00a0toggle\u00a0advanced\u00a0options  \n#\u00a0keep\u00a0making\u00a0changes\u00a0and\u00a0configuring\u00a0until\u00a0no\u00a0more\u00a0errors\u00a0or\u00a0changes  \n#\u00a0press\u00a0g\u00a0to\u00a0generate\u00a0and\u00a0exit  \nmake  \n#\u00a0if\u00a0it\u00a0has\u00a0a\u00a0test\u00a0suite,\u00a0good\u00a0idea\u00a0to\u00a0use\u00a0it  \nmake\u00a0test\u00a0  \nmake\u00a0install\n</code></pre> <p>If you need to rerun ccmake and reconfigure, remember to delete the <code>CMakeCache.txt</code> file, or you'll be wondering why your changes haven't taken. Turning on verbose Makefiles in ccmake is also useful if your code didn't compile first time - you'll be able to see what flags the compiler or linker is actually being given when it fails.</p>"},{"location":"Supplementary/Compiling_Your_Code/#make","title":"Make","text":"<p>Your code may just come with a Makefile and have no configure, in which case the generic way to compile it is as follows:</p> <pre><code>make\u00a0targetname\n</code></pre> <p>There's usually a default target, which <code>make</code> on its own will use. If you need to change any configuration options, you'll need to edit those sections of the Makefile (at the top, where the variables/flags are defined).</p> <p>Here are some typical variables you may want to change in a Makefile.</p> <p>These are what compilers/mpi wrappers to use - these are also defined by the compiler modules, so you can see what they should be. Intel would be <code>icc</code>, <code>icpc</code>, <code>ifort</code>, for example. If it's a program that can be compiled using MPI and only has a variable for <code>CC</code>, then set that to <code>mpicc</code>.</p> <pre><code>CC=gcc  \nCXX=g++  \nFC=gfortran  \nMPICC=mpicc  \nMPICXX=mpicxx  \nMPIF90=mpif90\n</code></pre> <p>CFLAGS and LDFLAGS are flags for the compiler and linker respectively, and there might be LIBS or INCLUDE as well. When linking a library with the name libfoo, use <code>-lfoo</code>.</p> <pre><code>CFLAGS=\"-I/path/to/include\"  \nLDFLAGS=\"-L/path/to/foo/lib\u00a0-L/path/to/bar/lib\"  \nLDLIBS=\"-lfoo\u00a0-lbar\"\n</code></pre> <p>Remember to <code>make clean</code> first if you are recompiling with new options!</p>"},{"location":"Supplementary/Compiling_Your_Code/#blas-and-lapack","title":"BLAS and LAPACK","text":"<p>BLAS and LAPACK are provided as part of MKL, OpenBLAS or ATLAS. There are several different OpenBLAS and ATLAS modules on Legion for different compilers. MKL is available in the Intel compiler module.</p> <p>Your code may try to link <code>-lblas -llapack</code>: this isn't the right way to use BLAS and LAPACK with MKL or ATLAS (our OpenBLAS now has symlinks that allow you to do this).</p>"},{"location":"Supplementary/Compiling_Your_Code/#set-your-path-and-other-environment-variables","title":"Set your PATH and other environment variables","text":"<p>After you have installed your software, you'll need to add it to your <code>PATH</code> environment variable so you can run it without having to give the full path to its location.</p> <p>Put this in your <code>~/.bashrc</code> file so it will set this with every new session you create. Replace username with your username and point to the directory your binary was built in (frequently <code>program/bin</code>). This adds it to the front of your PATH, so if you install a newer version of something, it will be found before the system one.</p> <pre><code>export\u00a0PATH=/home/username/location/of/software/binary:$PATH\n</code></pre> <p>If you built a library that you'll go on to compile other software with, you probably want to also add the lib directory to your LD_LIBRARY_PATH and LIBRARY_PATH, and the include directory to CPATH (add export statements as above). This may mean your configure step will pick your library up correctly without any further effort on your part.</p> <p>To make these changes to your .bashrc take effect in your current session:</p> <pre><code>source\u00a0~/.bashrc\n</code></pre>"},{"location":"Supplementary/Compiling_Your_Code/#python","title":"Python","text":"<p>There are <code>python2/recommended</code> and <code>python3/recommended</code> bundles. These use a virtualenv and have pip set up for you. They both have numpy and scipy available.</p>"},{"location":"Supplementary/Compiling_Your_Code/#set-compiler-module","title":"Set compiler module","text":"<p>The Python versions on Myriad were built with GCC. You can run them with the default Intel compilers loaded because everything depends on the gcc-libs/4.9.2 module. When you are building your own Python packages you should have the GCC compiler module loaded however, to avoid the situation where you build a package with the Intel compiler and then try to run it with GCC, in which case it will be unable to find Intel-specific instructions.</p> <pre><code>module\u00a0unload\u00a0compilers\nmodule\u00a0load\u00a0compilers/gnu/4.9.2\n</code></pre> <p>If you get an error like this when trying to run something, recheck what compiler you used.</p> <pre><code>undefined\u00a0symbol:\u00a0__intel_sse2_strrchr\n</code></pre>"},{"location":"Supplementary/Compiling_Your_Code/#install-your-own-packages-in-the-same-virtualenv","title":"Install your own packages in the same virtualenv","text":"<p>This will use our central virtualenv, which contains a number of packages already installed.</p> <pre><code>pip\u00a0install\u00a0--user\u00a0&lt;python2pkg&gt;  \npip3\u00a0install\u00a0--user\u00a0&lt;python3pkg&gt;\n</code></pre> <p>These will install into <code>.python2local</code> or <code>.python3local</code> directories in your home directory, respectively.</p> <p>To see what is already installed, the Python-shared list shows what is installed for both Python2 and 3, while the Python2 list and Python3 list show what is only installed for one or the other. (There may also be prereqs that aren't listed explicitly - pip will tell you if something is already installed as long as you have the recommended module bundle loaded).</p>"},{"location":"Supplementary/Compiling_Your_Code/#use-your-own-virtualenvs","title":"Use your own virtualenvs","text":"<p>If you need different packages that are not compatible with the central installs, you can create a new virtualenv and only yours will be available.</p> <pre><code>virtualenv\u00a0&lt;DIR&gt; \nsource\u00a0&lt;DIR&gt;/bin/activate\n</code></pre> <p>Your bash prompt will show you that a different virtualenv is active.</p>"},{"location":"Supplementary/Compiling_Your_Code/#installing-via-setuppy","title":"Installing via setup.py","text":"<p>If you need to install using setup.py, you can use the <code>--user</code> flag and as long as one of the python bundles is loaded, it will install into the same <code>.python2local</code> or <code>.python3local</code> as pip and you won't need to add any new paths to your environment.</p> <pre><code>python\u00a0setup.py\u00a0install\u00a0--user\n</code></pre> <p>You can alternatively use <code>--prefix</code> in which case you will have to set the install prefix to somewhere in your space, and also set PYTHONPATH and PATH to include your install location. Some installs won't create the prefix directory for you, in which case create it first. This is useful if you want to keep this package entirely separate and only in your paths on demand.</p> <pre><code>export\u00a0PYTHONPATH=/home/username/your/path/lib/python2.7/site-packages:$PYTHONPATH  \n#\u00a0if\u00a0necessary,\u00a0create\u00a0install\u00a0path  \nmkdir\u00a0-p\u00a0home/username/your/path/lib/python2.7/site-packages  \npython\u00a0setup.py\u00a0install\u00a0--prefix=/home/username/your/path\n\n#\u00a0add\u00a0these\u00a0to\u00a0your\u00a0.bashrc\u00a0or\u00a0jobscript  \nexport\u00a0PYTHONPATH=/home/username/your/path/lib/python2.7/site-packages:$PYTHONPATH  \nexport\u00a0PATH=/home/username/your/path/bin:$PATH\n</code></pre> <p>Check that the PATH is where your Python executables were installed, and the PYTHONPATH is correct. It will tend to tell you at install time if you need to change or create the PYTHONPATH directory.</p>"},{"location":"Supplementary/Compiling_Your_Code/#python-script-executable-paths","title":"Python script executable paths","text":"<p>If you have an executable python script giving the location of python like this, and it fails because that python doesn't exist in that location or isn't the one that has the additional packages installed:</p> <pre><code>#!/usr/bin/python2.7\n</code></pre> <p>You should change it so it uses the first python found in your environment. </p> <pre><code>#!/usr/bin/env\u00a0python \n</code></pre>"},{"location":"Supplementary/Compiling_Your_Code/#perl","title":"Perl","text":"<p>Perl modules will freqently have a Makefile.PL (especially if you download the tar files from CPAN.org yourself). You can install manually as:</p> <pre><code>perl\u00a0Makefile.PL\u00a0PREFIX=/home/username/your/perl/location\nmake\nmake\u00a0install\n</code></pre>"},{"location":"Supplementary/Compiling_Your_Code/#cpan","title":"CPAN","text":"<p>You can use CPAN to download and install modules locally for you. The first time you run the <code>cpan</code> command, it will create a <code>.cpan</code> directory for you and ask you to give it configuration settings or allow it to set them automatically.</p> <p>You need to tell it where you want your install prefix to be.</p> <p>If it is automatically configured, you need to edit these lines in your <code>.cpan/CPAN/MyConfig.pm</code>, for example if you want it to be in a lib directory in your home (change username to your own username):</p> <pre><code>'make_install_arg'\u00a0=&gt;\u00a0q[PREFIX=/home/username/lib],\n\u00a0#\u00a0other\u00a0lines\u00a0in\u00a0here  \n'makepl_arg'\u00a0=&gt;\u00a0q[PREFIX=/home/username/lib],\n'mbuild_install_arg'\u00a0=&gt;\u00a0q[PREFIX=/home/username/lib],\n'mbuildpl_arg'\u00a0=&gt;\u00a0q[--install_base\u00a0/home/username/lib],\n</code></pre> <p>It will download and build modules inside .cpan and install them where you specified.</p>"},{"location":"Supplementary/Compiling_Your_Code/#set-perl5lib-paths","title":"Set PERL5LIB paths","text":"<p>If you install your own Perl or Perl modules, you will need to append them to your PERL5LIB:</p> <pre><code>export\u00a0PERL5LIB=/home/username/your/perl/location:$PERL5LIB\n</code></pre> <p>If you installed with CPAN, you may need to add several paths to this based on the layout it creates inside your nominated Perl directory.</p>"},{"location":"Supplementary/Compiling_Your_Code/#errors-when-using-non-default-perl-versions","title":"Errors when using non-default Perl versions","text":""},{"location":"Supplementary/Compiling_Your_Code/#warningspm","title":"warnings.pm","text":"<p>If you are using a version of Perl that is not the default system Perl and get strange errors when trying to run a Perl script, particularly ones about warnings.pm:</p> <pre><code>Search\u00a0pattern\u00a0not\u00a0terminated\u00a0at\u00a0/shared/ucl/apps/perl/5.20.0/lib/5.20.0/warnings.pm\u00a0line\u00a01099\n</code></pre> <p>then you need to edit the script so that instead of beginning with <code>#!/usr/bin/perl</code>, it begins with <code>#!/usr/bin/env perl</code>. Otherwise it will try to use the old system Perl libraries with your newer Perl executable, which won't work.</p>"},{"location":"Supplementary/Compiling_Your_Code/#libperlso-not-found","title":"libperl.so not found","text":"<p>You probably built perl without telling it to build the shared library too. Add <code>-Duseshrplib</code> to your build flags.</p>"},{"location":"Supplementary/Compiling_Your_Code/#r","title":"R","text":"<p>There are instructions on installing and using local R packages in Using your own R packages.</p>"},{"location":"Supplementary/Compiling_Your_Code/#compiling-with-mpi","title":"Compiling with MPI","text":"<p>OpenMPI and Intel MPI are available. Certain programs do not work well with one or the other, so if you are having problems try the other one. Intel MPI is based on MPICH, so if the program you are compiling mentions that, try Intel MPI first.</p> <p>The Intel MPI is threadsafe; some versions of OpenMPI aren't.</p> <p>Note that OpenMPI 1.8.4 had a segv bug in non-blocking collectives that is fixed in OpenMPI 1.10.1.</p>"},{"location":"Supplementary/Compiling_Your_Code/#enabling-openmp","title":"Enabling OpenMP","text":"<p>To enable OpenMP with the Intel compilers, you simply need to add <code>-openmp</code> to your compile line. With the GNU compilers you need to add <code>-fopenmp</code>.</p>"},{"location":"Supplementary/Compiling_Your_Code/#problems","title":"Problems","text":"<p>If you experience problems building your applications, please contact your local IT support in the first instance. We are available at rc-support AT ucl.ac.uk to help you if you still cannot build your app or if you need to report a problem with our software stack.</p>"},{"location":"Supplementary/Compiling_and_Running_Matlab_Programs/","title":"Compiling and Running Matlab Programs","text":"<p>Although full Matlab is available on Myriad, you can also compile Matlab programs on an external machine and then run them on Myriad using the Matlab runtime.</p> <p>There are some caveats, however: </p> <ul> <li> <p>Your Matlab program must be compiled using a 64bit Linux version of the Matlab compiler; the compiled code is not cross-platform compatible, so for example,  it cannot be built on macOS and then transferred to Myriad. </p> </li> <li> <p>Piping code into the Matlab compiler will not work, and the main routine being executed must be converted into a proper Matlab function.</p> </li> <li> <p>When arguments are passed into compiled Matlab executable, the compiled code does not automatically convert them to the required type (i.e. float or integer) as Matlab does from the command line. In this case the arguments, where necessary, must be converted to numbers using the <code>str2num()</code> function.</p> </li> </ul>"},{"location":"Supplementary/Compiling_and_Running_Matlab_Programs/#compiling-your-program","title":"Compiling your program:","text":"<p>The Matlab code is must be compiled using the mcc tool; this must be initially run as <code>mcc -setup</code> before anything is built. The mcc tool can actually be invoked from the interpreter command prompt and executing <code>help mcc</code> will give you quite a lot of information about how to use the tool, along with examples.</p> <p>All <code>.m</code> files must be built into the compiled code with the first .m referenced in the build line acting as the main entry point for the built code. It may be useful to include data files in the built code which are handled in the build line using the <code>-a &lt;datafile&gt;</code> option. Please remember to make the <code>.m</code> file an actual function and all other dependencies sub-functions, otherwise the compiled code will not execute.</p>"},{"location":"Supplementary/Compiling_and_Running_Matlab_Programs/#some-important-mcc-options","title":"Some important mcc options:","text":"<ul> <li><code>-m</code>: this is option which runs the macro to generate a C stand-alone     application. <code>-R</code>: specify runtime options for the Matlab compiler runtime.</li> </ul>"},{"location":"Supplementary/Compiling_and_Running_Matlab_Programs/#some-important-runtime-options","title":"Some important runtime options:","text":"<ul> <li><code>-nojvm</code>: disables the java virtual machine, which may speed-up     certain codes. This option cannot be used if you are planning to     have, for example pdf files or any other plots produced as output of     your run. <code>-nodisplay</code>: prevents anything being displayed on the screen, can be     useful if this happens with the application as this would not work     correctly in batch mode. <code>--singleCompThread</code>: use only a single computational thread,     otherwise Matlab will try to use more than one thread when the     operation being performed supports multi threading.</li> </ul> <p>Once the application has been built, there should be an executable named after the prefix of the <code>.m</code> file, generally <code>&lt;app name&gt;.m</code>, and a shell script with the name <code>run\\_&lt;app name&gt;.sh</code> - both these files need to be transferred to Myriad.</p> <p>If you have been given pre-compiled code by someone else, the application may not work as the Matlab runtime version must reasonably match that of the Matlab compiler that was used to build the application. The runtime is freely distributable and can be found in the installation directory of Matlab. The runtime has a GUI install interface and it can be installed at any location in your home directory.</p> <p>For more information, please read the Matlab documentation.</p>"},{"location":"Supplementary/Compiling_and_Running_Matlab_Programs/#job-submission-scripts","title":"Job submission scripts:","text":"<p>There are three things that you must take into account:</p> <ol> <li>The location of the Matlab compiler runtime needs to be passed to     the script used to run the compiled Matlab code as the first     argument.</li> <li>The compiler runtime needs a directory (cache) to unpack files to     when it is running. By default this directory is in the home folder.     Since the Matlab runs will be single     node jobs, the cache location should be in the storage on the     compute nodes which is stored in <code>TMPDIR</code>.</li> </ol> <p>For example, a multi-threaded serial script should look something like:</p> <pre><code>#!/bin/bash\u00a0-l\n#\u00a0Batch\u00a0script\u00a0to\u00a0run\u00a0a\u00a0serial\u00a0job\u00a0on\u00a0Legion\u00a0under\u00a0SGE.\n\n#\u00a0Force\u00a0bash\u00a0as\u00a0the\u00a0executing\u00a0shell.\n#$\u00a0-S\u00a0/bin/bash\n\n#\u00a0Request\u00a0ten\u00a0minutes\u00a0of\u00a0wallclock\u00a0time\u00a0(format\u00a0hours:minutes:seconds).\n#$\u00a0-l\u00a0h_rt=0:10:0\n\n#\u00a0Request\u00a01\u00a0gigabyte\u00a0of\u00a0RAM\u00a0\n#$\u00a0-l\u00a0mem=1G\n\n#\u00a0Request 36 cores\n#$\u00a0-pe smp 36\n\n#\u00a0Set\u00a0the\u00a0name\u00a0of\u00a0the\u00a0job.\n#$\u00a0-N\u00a0Matlab_Job_1\n\n#\u00a0Set\u00a0the\u00a0working\u00a0directory\u00a0to\u00a0somewhere\u00a0in\u00a0your\u00a0scratch\u00a0space.\n#\u00a0For\u00a0example:\n##$\u00a0-wd\u00a0/home//Scratch\n#\u00a0Alternatively,\u00a0you can automatically use the current working directory \n#  if you launch\u00a0your\u00a0job\u00a0from\u00a0anywhere\u00a0*within\u00a0~/Scratch*\n#$\u00a0-cwd\n\n#\u00a0store\u00a0the\u00a0MATLAB\u00a0runtime\u00a0path\u00a0in\u00a0a\u00a0global\u00a0environment\u00a0variable\u00a0(MCR_HOME)\nexport\u00a0MCR_HOME=/shared/ucl/apps/Matlab/R2011a/Runtime7.15/v715/\n\n#\u00a0the\u00a0path\u00a0to\u00a0the\u00a0Matlab\u00a0cache\u00a0is\u00a0stored\u00a0in\u00a0the\u00a0global\u00a0variable\u00a0MCR_CACHE_ROOT\u00a0\nexport\u00a0MCR_CACHE_ROOT=$TMPDIR/mcr_cache\n\n#\u00a0make\u00a0sure\u00a0the\u00a0directory\u00a0in\u00a0MCR_CACHE_ROOT\u00a0exists\nmkdir\u00a0-p\u00a0$MCR_CACHE_ROOT\n\n#\u00a0Run\u00a0the\u00a0executable,\u00a0passing\u00a0the\u00a0path\u00a0stored\u00a0in\u00a0MCR_HOME\u00a0as\u00a0the\u00a0first\u00a0argument.\n#\u00a0There\u00a0is\u00a0no\u00a0need\u00a0to\u00a0pass\u00a0the\u00a0content\u00a0of\u00a0MCR_CACHE_ROOT\u00a0as\u00a0an\u00a0argument\u00a0to\u00a0the\n#\u00a0to\u00a0the\u00a0run_appname.sh\u00a0script\u00a0since\u00a0it\u00a0is\u00a0a\u00a0variable\u00a0that\u00a0the\u00a0Matlab\u00a0runtime\u00a0is\u00a0aware\u00a0of.\n./run_appname.sh\u00a0$MCR_HOME\u00a0[arguments\u00a0list]\n\n#\u00a0Preferably,\u00a0tar-up\u00a0(archive)\u00a0all\u00a0output\u00a0files\u00a0onto\u00a0the\u00a0shared\u00a0scratch\u00a0area\ntar\u00a0zcvf\u00a0$HOME/Scratch/files_from_job_${JOB_ID}.tgz\u00a0$TMPDIR\n\n#\u00a0Make\u00a0sure\u00a0you\u00a0have\u00a0given\u00a0enough\u00a0time\u00a0for\u00a0the\u00a0copy\u00a0to\u00a0complete!\n</code></pre> <p>For any queries and problem reports, please contact rc-support@ucl.ac.uk.</p>"},{"location":"Supplementary/Connecting_to_RDSS/","title":"Connecting to the Research Data Storage Service","text":"<p>The Research Data Storage Service (RDSS) is a system run by the Research Data group in the Advanced Research Computing department, and is designed to help with data storage during and after a project. Several solutions for copying data between RDSS and the central UCL research computing platforms are presented below. Sections of the example code surrounded by angle brackets (\\&lt;&gt;) should  be replaced by the information indicated (do not keep the angle brackets in).</p>"},{"location":"Supplementary/Connecting_to_RDSS/#between-myriad-and-rdss","title":"Between Myriad and RDSS","text":"<p>If you already have an account with the Research Data Storage Service, you can transfer data directly between Legion and Research Data Storage using the Secure Copy (<code>scp</code>) command.</p>"},{"location":"Supplementary/Connecting_to_RDSS/#from-rds-to-myriad","title":"From RDS to Myriad","text":"<p>If you are on an RDSS login node, you can transfer data to Myriad\u2019s Scratch area at the highest rate currently possible by running the command: </p> <pre><code>scp\u00a0data_file.tgz\u00a0myriad.rc.ucl.ac.uk:~/Scratch/\n</code></pre> <p>Or from somewhere within Myriad (including compute nodes in running jobs) running the command: </p> <pre><code>scp\u00a0ssh.rd.ucl.ac.uk:~/data_file.tgz\u00a0~/Scratch/\n</code></pre>"},{"location":"Supplementary/Connecting_to_RDSS/#from-myriad-to-rdss","title":"From Myriad to RDSS","text":"<p>From Myriad, send data to your project space on RDSS by running the command:</p> <pre><code>scp\u00a0data_file.tgz\u00a0ccaaxyz@ssh.rd.ucl.ac.uk:&lt;path_to_project_space&gt;\n</code></pre> <p>The RDSS support pages provide more information:</p> <p>https://www.ucl.ac.uk/advanced-research-computing/rdss-myriad-data-storage-transfer-service</p>"},{"location":"Supplementary/GPU_Clusters/","title":"GPU clusters","text":"<p>UCL users may be able to access the following GPU clusters.</p>"},{"location":"Supplementary/GPU_Clusters/#national-gpu-clusters","title":"National GPU clusters","text":"<p>There are two nationally-accessible EPSRC Tier 2 HPC centres with GPUs.</p> <p>Access is generally managed through calls to an EPSRC Resource Allocation Panel</p> <ul> <li>Tier 2 RAP calls</li> <li>Filterable page for all open EPSRC calls</li> </ul> <p>There may also be pump-priming/proof of concept access available.</p> <p>General information about machines with external access is available at HPC-UK.</p>"},{"location":"Supplementary/GPU_Clusters/#csd3","title":"CSD3","text":"<ul> <li>Suitable for workloads spanning multiple compute nodes using GPUs     and MPI</li> <li>NVIDIA Tesla P100</li> <li>Access to CSD3</li> </ul>"},{"location":"Supplementary/GPU_Clusters/#jade","title":"JADE","text":"<ul> <li>NVIDIA DGX-1 (Tesla P100)</li> <li>Access to JADE</li> </ul>"},{"location":"Supplementary/GPU_Nodes/","title":"GPU Nodes","text":""},{"location":"Supplementary/GPU_Nodes/#node-types","title":"Node Types","text":"<p>You can view the hardware specifications for GPU node types in Myriad.</p> <p>There are several types of GPU nodes available in Myriad.</p>"},{"location":"Supplementary/GPU_Nodes/#available-modules","title":"Available modules","text":"<p>You can see all the available CUDA modules by typing</p> <pre><code>module\u00a0avail\u00a0cuda\n</code></pre>"},{"location":"Supplementary/GPU_Nodes/#sample-cuda-code","title":"Sample CUDA code","text":"<p>There are samples in some CUDA install locations, e.g. </p> <pre><code>/shared/ucl/apps/cuda/7.5.18/gnu-4.9.2/samples\n/shared/ucl/apps/cuda/8.0.61/gnu-4.9.2/samples\n</code></pre> <p>which are further documented by NVIDIA here. In general, you should look at their CUDA docs: http://docs.nvidia.com/cuda/</p>"},{"location":"Supplementary/GPU_Nodes/#sample-jobscripts","title":"Sample jobscripts","text":"<p>You can see sample jobscripts here.</p> <p>Use this in your script to request up to 2 GPUs. </p> <pre><code>#$\u00a0-l\u00a0gpu=2\n</code></pre> <p>Load GCC and the relevant CUDA module. </p> <pre><code>module\u00a0unload\u00a0compilers\u00a0mpi\nmodule\u00a0load\u00a0compilers/gnu/4.9.2\nmodule\u00a0load\u00a0cuda/7.5.18/gnu-4.9.2\n</code></pre>"},{"location":"Supplementary/GPU_Nodes/#running-the-sample-code","title":"Running the sample code","text":"<p>To get started, here's how you would compile one of the CUDA samples and run it in an interactive session on a GPU node.</p> <p>You can compile CUDA code on the login nodes like this (which do not have GPUs) if they do not require all the CUDA libraries to be present at compile time. If they do, you'll get an error saying it cannot link the CUDA libraries, and <code>ERROR: CUDA could not be found on your system</code>  and you will need tro do your compiling on the GPU node as well.</p> <p>1. Load the cuda module </p> <pre><code>module\u00a0unload\u00a0compilers\u00a0mpi\nmodule\u00a0load\u00a0compilers/gnu/4.9.2 \nmodule\u00a0load\u00a0cuda/7.5.18/gnu-4.9.2\n</code></pre> <p>2. Copy the samples directory to somewhere in your home (or to Scratch if you're building on the GPU node or are going to want a job to write anything in the same directory).</p> <pre><code>cp\u00a0-r\u00a0/shared/ucl/apps/cuda/7.5.18/gnu-4.9.2/NVIDIA_CUDA-7.5_Samples/\u00a0~/cuda\n</code></pre> <p>3. Choose an example: eigenvalues in this case, and build using the provided makefile - if you have a look at it you can see it is using nvcc and g++. </p> <pre><code>cd\u00a0NVIDIA_CUDA-7.5_Samples/6_Advanced/eigenvalues/\nmake\n</code></pre> <p>4. Request an interactive job with a GPU and wait to be given access to the node. You will see your prompt change to indicate that you are on a different node than the login node once your qrsh request has been scheduled, and you can then continue. Load the cuda module on the node and run the program. </p> <pre><code>qrsh\u00a0-l\u00a0mem=1G,h_rt=0:30:0,gpu=1\u00a0-now\u00a0no\n\n# wait for interactive job to start\n\nmodule\u00a0unload\u00a0compilers\u00a0mpi\nmodule\u00a0load\u00a0compilers/gnu/4.9.2\nmodule\u00a0load\u00a0cuda/7.5.18\ncd\u00a0~/cuda/NVIDIA_CUDA-7.5_Samples/6_Advanced/eigenvalues/\n./eigenvalues\n</code></pre> <p>5. Your output should look something like this: </p> <pre><code>Starting\u00a0eigenvalues  \nGPU\u00a0Device\u00a00:\u00a0\"Tesla\u00a0M2070\"\u00a0with\u00a0compute\u00a0capability\u00a02.0\n\nMatrix\u00a0size:\u00a02048\u00a0x\u00a02048\u00a0  \nPrecision:\u00a00.000010  \nIterations\u00a0to\u00a0be\u00a0timed:\u00a0100  \nResult\u00a0filename:\u00a0'eigenvalues.dat'  \nGerschgorin\u00a0interval:\u00a0-2.894310\u00a0/\u00a02.923303  \nAverage\u00a0time\u00a0step\u00a01:\u00a026.739325\u00a0ms  \nAverage\u00a0time\u00a0step\u00a02,\u00a0one\u00a0intervals:\u00a09.031162\u00a0ms  \nAverage\u00a0time\u00a0step\u00a02,\u00a0mult\u00a0intervals:\u00a00.004330\u00a0ms  \nAverage\u00a0time\u00a0TOTAL:\u00a035.806992\u00a0ms  \nTest\u00a0Succeeded!\n</code></pre>"},{"location":"Supplementary/GPU_Nodes/#building-your-own-code","title":"Building your own code","text":"<p>As above, if the code you are trying to compile needs to link against libcuda, it must also be built on a GPU node because only the GPU nodes have the correct libraries.</p> <p>The NVIDIA examples don't require this, but things like Tensorflow do.</p>"},{"location":"Supplementary/GPU_Nodes/#tensorflow","title":"Tensorflow","text":"<p>Tensorflow is installed: type <code>module avail tensorflow</code> to see the available versions.</p> <p>Modules to load for the non-MKL GPU version: </p> <pre><code>module\u00a0unload\u00a0compilers\u00a0mpi \nmodule\u00a0load\u00a0compilers/gnu/4.9.2  \nmodule\u00a0load\u00a0python3/3.7\nmodule\u00a0load\u00a0cuda/10.0.130/gnu-4.9.2  \nmodule\u00a0load\u00a0cudnn/7.4.2.24/cuda-10.0\nmodule\u00a0load\u00a0tensorflow/2.0.0/gpu-py37\n</code></pre> <p>Modules to load the most recent version we have installed with GPU support (2.11.0):</p> <pre><code>module unload compilers mpi gcc-libs\nmodule load gcc-libs/10.2.0\nmodule load python/3.9.6-gnu-10.2.0\nmodule load cuda/11.2.0/gnu-10.2.0\nmodule load cudnn/8.1.0.77/cuda-11.2\nmodule load tensorflow/2.11.0/gpu\n</code></pre>"},{"location":"Supplementary/GPU_Nodes/#pytorch","title":"PyTorch","text":"<p>PyTorch is installed: type <code>module avail pytorch</code> to see the versions available.</p> <p>Modules to load the most recent release we have installed (May 2022) are:</p> <pre><code>module unload compilers mpi gcc-libs\nmodule load gcc-libs/10.2.0\nmodule load python3/3.9-gnu-10.2.0\nmodule load cuda/11.3.1/gnu-10.2.0\nmodule load cudnn/8.2.1.32/cuda-11.3\nmodule load pytorch/1.11.0/gpu\n</code></pre> <p>If you want the CPU only version then use:</p> <pre><code>module unload compilers mpi gcc-libs\nmodule load gcc-libs/10.2.0\nmodule load python3/3.9-gnu-10.2.0\nmodule load pytorch/1.11.0/cpu\n</code></pre>"},{"location":"Supplementary/GPU_Nodes/#using-mpi-and-gpus","title":"Using MPI and GPUs","text":"<p>It is possible to run MPI programs that use GPUs but only within a single node, so you can request up to 4 GPUs and 36 cores on Myriad.</p>"},{"location":"Supplementary/GPU_Nodes/#looking-for-more-gpus","title":"Looking for more GPUs?","text":"<ul> <li>GPU clusters available to UCL users.</li> </ul>"},{"location":"Supplementary/Glossary/","title":"Research Computing Glossary","text":"Bash A shell and scripting language, which is the default   command processor on most Linux operating systems. Cluster A cluster consists of a set of computer nodes connected together over a fast local area network. A message passing protocol such as MPI allows individual nodes to work together as a single system. Core A core refers to a processing unit within a node. A node may have multiple cores which can work in parallel on a single task, operating on the same data in memory. This kind of parallelism is coordinated using the OpenMP library. Alternatively, cores may work independently on different tasks. Cores may or may not also share cache. Interconnect The interconnect is the network which is used to transfer data between nodes in a cluster. Different types of interconnect operate at different bandwidths and with different amounts of latency, which affects the suitability of a collection of nodes for jobs which use message passing (MPI). Batch Processing A workflow in which tasks are collected as produced, and then processed as capacity becomes available. This usually involves ensuring that the task can be completed without user intervention, so that the user does not have to remain present or available. Job In the context of Batch Processing, a job refers to a computational task to be performed such as a single simulation or analysis. Job Script A job script is essentially a special kind of script used to specify the parameters of a job. Users can specify the data to input, program to use, and the computing resources required. The job script is specified when a job is submitted to SGE, which reads lines starting with <code>#$</code>. MPI The Message Passing Interface (MPI) system is a set of portable libraries which can be incorporated into programs in order to control parallel computation. Specifically it coordinates effort between nodes which do not share the same memory address space cf. OpenMP. Node In cluster computing, a node refers to a computational unit which is capable of operating independently of other parts of the cluster. As a minimum it consists of one (or more) processing cores, has its own memory, and runs its own operating system. OpenMP Open Multi-Processing. OpenMP supports multithreading, a process whereby a master thread generates a number of slave threads to run a task which is divided among them. OpenMP applies to processes running on shared memory platforms, i.e.  jobs running on a single node. Hybrid applications may make use of both OpenMP and MPI. Process A process is a single instance of a program that is running on a computer. A single process may consist of many threads acting concurrently, and there may multiple instances of a program running as separate processes. Script A shell script enables users to list commands to be run consecutively by typing them into a text file instead of typing them out live. The first line of the script uses the shebang notation <code>#!</code> to designate the scripting language interpreter program to be used to interpret the commands, e.g. bash. Shebang \"Shebang\" is a common abbreviation for \"hash-bang\" \u2014 the character sequence <code>#!</code> \u2014 which is placed at the start of a script to specify the interpreter that should be used. When the shebang is found in the first line of a script, the program loader reads the rest of the line as the path to the required interpreter (e.g. <code>/bin/bash</code> is the usual path to the bash shell). The specified interpreter is then run with the path to the script passed as an argument to it. Shell A command line interpreter which provides an interface for users to type instructions to be interpreted by the operating system and display output via the monitor. Users type specific shell commands in order to run processes, e.g. <code>ls</code> to list directory contents. Son of Grid Engine (SGE or SoGE) The queuing system used by many cluster computing systems (including, currently, all the ones we run) to organise and schedule jobs. Once jobs are submitted to SGE, it takes care of executing them when the required resources become available. Job priority is subject to the local fair use policy. Sun Grid Engine (SGE) The original software written by Sun Microsystems that was later modified to make Son of Grid Engine (among other products, like Univa Grid Engine). Documentation may refer to Sun Grid Engine instead of Son of Grid Engine, and for most user purposes, the terms are interchangeable. Thread A thread refers to a serial computational process which can run on a single core. The number of threads generated by a parallel job may exceed the number of cores available though, in which case cores may alternate between running different threads. Threads are a software concept whereas cores are physical hardware."},{"location":"Supplementary/Hostkeys/","title":"Hostkeys","text":"<p>These are the current hostkey fingerprints for our clusters. The are different types of hostkeys (ECDSA, ED25519, RSA) and which one is used depends on your ssh client and its configuration. The MD5 or SHA256  at the front is letting you know what type of fingerprint it is - your ssh client  may not include that part in its output.</p>"},{"location":"Supplementary/Hostkeys/#myriad","title":"Myriad","text":"<pre><code>ECDSA    MD5:db:06:ca:12:38:23:1f:12:ed:47:4f:51:0f:19:5d:23\nECDSA    SHA256:7FTryal3mIhWr9CqM3EPPeXsfezNk8Mm8HPCCAGXiIA\nED25519  MD5:92:6d:97:46:eb:8d:0a:4b:8a:65:cb:0d:65:79:bb:7f\nED25519  SHA256:waqBYWAb+g1lwUGWz8ku4M48McIBWGCdpMoU8l8j6tU\nRSA      MD5:29:a7:45:04:83:86:ec:95:fa:25:dc:7a:f4:93:78:c1\nRSA      SHA256:8H13PdcJGJJbV/F3oBYXX7W1/8q/7m3gLBc8uZP3wio\n</code></pre>"},{"location":"Supplementary/Hostkeys/#kathleen","title":"Kathleen","text":"<pre><code>ECDSA    MD5:6c:94:b2:01:c3:2f:58:5b:f3:03:02:cf:0f:ac:a0:d2\nECDSA    SHA256:rCKAb0yOWXK8+GClKy/pdbwrUbrGMvFkMciZLVcbaTA\nED25519  MD5:92:6d:97:46:eb:8d:0a:4b:8a:65:cb:0d:65:79:bb:7f\nED25519  SHA256:waqBYWAb+g1lwUGWz8ku4M48McIBWGCdpMoU8l8j6tU\nRSA      MD5:5a:cf:95:a2:e4:05:8a:36:46:dc:65:0a:f2:8b:ab:e1\nRSA      SHA256:4SMOyhe/MVQ8aUOZyPfHjrIU5zHh7ZhmVd4zxzY+ukI\n</code></pre>"},{"location":"Supplementary/Hostkeys/#young","title":"Young","text":"<pre><code>ECDSA    MD5:60:13:a6:4d:09:33:4d:67:1b:46:24:ee:44:66:71:17\nECDSA    SHA256:3zwMU9C8d9rgmYJ9qDElo15NnWyF2I4xy2X/VIAmFdo\nED25519  MD5:92:6d:97:46:eb:8d:0a:4b:8a:65:cb:0d:65:79:bb:7f\nED25519  SHA256:waqBYWAb+g1lwUGWz8ku4M48McIBWGCdpMoU8l8j6tU\nRSA      MD5:06:17:f3:f2:0c:3e:0d:df:1d:04:fb:53:dc:77:60:56\nRSA      SHA256:DPcjbsUTBq3LwRggu4N+q2WQR0rkoM42jRdYXJtB86M\n</code></pre>"},{"location":"Supplementary/Hostkeys/#michael","title":"Michael","text":"<pre><code>ECDSA    MD5:11:98:2e:c2:da:14:0c:d3:4e:a3:70:11:e1:59:72:7e\nECDSA    SHA256:3PMLXp6ny0dECycvx4D7+t0sNgsSsLvSO5QUYmzkbhs\nED25519  MD5:92:6d:97:46:eb:8d:0a:4b:8a:65:cb:0d:65:79:bb:7f\nED25519  SHA256:waqBYWAb+g1lwUGWz8ku4M48McIBWGCdpMoU8l8j6tU\nRSA      MD5:85:31:4b:cf:1a:ec:64:e4:b2:98:28:4a:46:b2:c1:90\nRSA      SHA256:/qv4BAS9ga6C6iMwj8coEPQGg740CmazeDTFnXeGX+c\n</code></pre>"},{"location":"Supplementary/Installing_PGI/","title":"Installing PGI","text":""},{"location":"Supplementary/Installing_PGI/#pgi-compiler-suite-installation-at-ucl","title":"PGI Compiler Suite Installation at UCL","text":"<p>UCL has two floating licences for PGI Fortran/C/C++ Server for Linux, purchased primarily for building Gaussian 03 and Gaussian 09 on UCL computers. To install follow the procedure below. If you are installing on a system outside the Institutional Firewall, please connect to the UCL VPN service first.</p> <ul> <li>Download version 13.9 or 11.9 from the UCL Software Database     on your Linux system in a empty directory. You will need to login      with your UCL userid and password.</li> <li>Untar the installer files using:  </li> </ul> <pre><code>tar\u00a0xvzf\u00a0pgilinux-2013-139.tar.gz\n</code></pre> <ul> <li>Run the installer as root and follow the instructions:</li> </ul> <pre><code>./install`\n</code></pre> <ul> <li> <p>During the installation you will be asked: Do you wish to generate     license keys? (y/n) - Enter <code>n</code> as you will need to use the central     UCL licence server.</p> </li> <li> <p>Add the PGI Suite bin directory to your PATH. The default location     used by the installer is <code>/opt/pgi/linux86-64/13.9/bin</code>.</p> </li> <li>Open access to TCP ports 27000 and 27055 in your local firewall and     any departmental firewall.</li> <li>Setup access to the licence server by setting the <code>LM_LICENSE_FILE</code>     environment variable. Use either:</li> </ul> <pre><code>export\u00a0LM_LICENSE_FILE=27000@lic-pgi.ucl.ac.uk\n\nexport\u00a0LM_LICENSE_FILE=$LM_LICENSE_FILE:27000@lic-pgi.ucl.ac.uk\n</code></pre> <ul> <li>Use the second version if you have other licence managers already     defined.</li> </ul> <p>The PGI compilers should now be installed and working.</p>"},{"location":"Supplementary/NVIDIA_Containers/","title":"Using NVIDIA Grid Cloud Containers","text":"<p>NVIDIA's NGC Container Registry stores a lot of containers<sup>1</sup> with various applications pre-configured to be run with GPUs. Each container can be set up completely differently, so you'll need to read the instructions in the registry to work out how to use it.</p> <p>Many of the instructions for using these containers assume you are using the Docker toolset to run the container, but this is not installed on our services<sup>2</sup>. The Singularity container runtime can be used instead for most workloads, but there are some limitations, that mostly affect containers that try to run web services.</p> <p>In general, if a container's instructions do not have specific instructions for using Singularity, and would tell you to run, for example:</p> <pre><code>docker run --gpus all nvcr.io/some/container\n</code></pre> <p>You should instead use:</p> <pre><code>singularity run --nv https://nvcr.io/some/container\n</code></pre> <p>For jobs using MPI, this is more complicated, because the MPI traffic has to be passed between the container and the operating system underneath. Containers built to use MPI will usually contain instructions for using Singularity, however.</p>"},{"location":"Supplementary/NVIDIA_Containers/#worked-example-namd-3","title":"Worked Example: NAMD 3","text":"<p>The NAMD authors publish a NAMD container on the NGC Container Registry, and we should be able to download this and run it on our cluster inside a batch job. </p> <p>The page about how to use the container is here: https://catalog.ngc.nvidia.com/orgs/hpc/containers/namd</p> <p>Following it through, you can download the benchmark example to check later whether your container works:</p> <pre><code>mkdir ngc_namd_experiment\ncd ngc_namd_experiment\nwget -O - https://gitlab.com/NVHPC/ngc-examples/raw/master/namd/3.0/get_apoa1.sh | bash\n</code></pre> <p>There are a couple of typos in the instructions: you'll need to use the tag <code>3.0-alpha3-singlenode</code> instead of <code>3.0_alpha3-singlenode</code>:</p> <pre><code>export NAMD_TAG=\"3.0-alpha3-singlenode\"\n</code></pre>"},{"location":"Supplementary/NVIDIA_Containers/#creating-the-container-image","title":"Creating the Container Image","text":"<p>Before you use Singularity to create the container image, you should load the Singularity module to set up some directories where things are stored. Not doing this can cause you problems, because the default places often do not have space to store the large files needed.</p> <pre><code>module load singularity-env\n</code></pre> <p>Once you've done that, you can download the container's files and build them into the usable container:</p> <pre><code>singularity build ${NAMD_TAG}.sif docker://nvcr.io/hpc/namd:${NAMD_TAG}\n</code></pre> <p>This can take a while: Singularity has to download quite a few file collections and assemble them into a single usable set. You may see some of the following warnings:</p> <pre><code>WARNING: 'nodev' mount option set on /lustre, it could be a source of failure during build process\n2022/02/03 14:06:28  warn xattr{var/log/apt/term.log} ignoring ENOTSUP on setxattr \"user.rootlesscontainers\"\n2022/02/03 14:06:28  warn xattr{/home/uccaiki/Scratch/.singularity/tmp/rootfs-5ac43e37-84fa-11ec-8784-0894ef553d4e/var/log/apt/term.log} destination filesystem does not support xattrs, further warnings will be suppressed\n</code></pre> <p>These indicate that various capabilities are not available because of how we're building the container. For HPC use, they don't present a problem, but they could be problematic if you were building a web server into a container.</p> <p>When Singularity has finished, you should see the following message:</p> <pre><code>INFO:    Creating SIF file...\nINFO:    Build complete: 3.0-alpha3-singlenode.sif\n</code></pre> <p>This file is the container image, which contains the files needed to run NAMD. You can see what NAMD gets when running \"inside\" the container, by running <code>ls</code> with it:</p> <pre><code>singularity exec 3.0-alpha3-singlenode.sif ls /\n</code></pre> <p>gives:</p> <pre><code>WARNING: Bind mount '/home/uccaiki =&gt; /home/uccaiki' overlaps container CWD /home/uccaiki/ngc_namd_experiment, may not be available\nbin  boot  dev  environment  etc  home  host_pwd  lib  lib64  lustre  media  mnt  opt  proc  root  run  sbin  scratch  singularity  srv  sys  tmp  tmpdir  usr  var\n</code></pre> <p>The warning you get is telling you that your current working directory overlaps with a directory being \"bound\" into the container. Binding brings a directory into the container's view of the filesystem, so that, for example, programs can still access your home directory as usual. In this case it's not a problem, because it's warning you that your home directory is being bound into the container in the same place it would usually be, and that means the same files are visible.</p> <p>By default, the clusters have Singularity configured to bind your home and Scratch directories into the container, as well as the per-job temporary storage allocated to jobs under <code>$TMPDIR</code>. </p> <p>The NAMD instructions make an alternative suggestion when setting up this environment variable to use to run Singularity, binding your data directory into a fixed place in the container:</p> <pre><code>SINGULARITY=\"$(which singularity) exec --nv -B $(pwd):/host_pwd ${NAMD_TAG}.sif\"\n</code></pre> <p>The option <code>-B $(pwd):/host_pwd</code> handles this, binding wherever you run the command from to the fixed location <code>/host_pwd</code> inside the container.</p> <p>So, for example, if you run:</p> <pre><code>ls\n$SINGULARITY ls /host_pwd\n</code></pre> <p>In both cases, you should see the same files, because you're looking at the same underlying directory.</p>"},{"location":"Supplementary/NVIDIA_Containers/#running-on-a-single-node","title":"Running on a Single Node","text":"<p>At this point you're ready to run NAMD inside the container, but you need a job script to submit to the scheduler which can set up the number of cores and GPUs correctly.</p> <pre><code>#!/bin/bash -l\n\n# Start with our resource requirements:\n#  1 hour's maximum runtime\n#$ -l h_rt=1:00:00\n\n# 2 GPUs\n#$ -l gpu=2\n\n# 36 processor cores\n#$ -pe smp 36\n\n# Start with current working directory the same as where we submitted the job from\n#$ -cwd\n\n# Make sure Singularity looks for our stored container data in the right places\nmodule load singularity-env\n\n# Set the variables we need for this example to run\nNAMD_TAG=\"3.0-alpha3-singlenode\"\nSINGULARITY=\"$(which singularity) exec --nv -B $(pwd):/host_pwd ${NAMD_TAG}.sif\"\n\n# This is where the benchmark's data ends up inside the container\nINPUT=\"/host_pwd/apoa1/apoa1_nve_cuda.namd\"\n\n# Show us some info we can refer to later\nprintf \"Running NAMD using:\\n Cores: %d\\n GPUs: %d\\n Container image: %s\\nWorking directory: %s\\n Input: %s\\n\" \\\n  \"$NSLOTS\" \\\n  \"$GPU\" \\\n  \"${NAMD_TAG}.sif\" \\\n  \"$(pwd)\" \\\n  \"$INPUT\"\n\n# Run NAMD\n\"$SINGULARITY\" namd3  +ppn \"$NSLOTS\" +setcpuaffinity +idlepoll \"$INPUT\"\n</code></pre> <p>Copy this into a file, and submit it to the queue, e.g.:</p> <pre><code>qsub ngc_namd_experiment.sh\n</code></pre> <p>This should take about 5 minutes to run on a 36-core, 2-GPU node. </p> <ol> <li> <p>A container is a way of bundling up a collection of files and instructions to run as a kind of \"altered view\" of the computer's files and systems.\u00a0\u21a9</p> </li> <li> <p>This is for a variety of reasons, but primarily that Docker presents a much larger range of security and misuse risks when used by inexperienced or hostile users. Singularity represents a much better fit for common HPC workloads and use-cases.\u00a0\u21a9</p> </li> </ol>"},{"location":"Supplementary/OnDemand/","title":"Myriad Open OnDemand Pilot","text":"<p>Note</p> <p>This component of the Myriad service is currently in limited pilot.</p> <p>As a pilot service, details are subject to change.</p> <p>Open OnDemand (OOD) is a setup that allows you to connect to, and use, an existing HPC cluster via a web browser.</p> <p>We've set this up on Myriad as a pilot service.</p> <p>Click Here To Access the OnDemand Pilot Service</p> <p>It currently has the following capabilities:</p> <ul> <li>file browsing, downloading, and uploading</li> <li>basic text editing</li> <li>SSH access in a browser window</li> <li>creating and running jobs based on templates</li> <li>running interactive desktops on the compute nodes</li> <li>running interactive Jupyter servers on the compute nodes</li> </ul> <p>These may expand as we get better acquainted with this system, or add other integrated interactive components. Interactive R Studio/Posit sessions in particular seem possible but are not currently implemented -- we intend to investigate and develop this during the pilot.</p> <p>For the interactive components, we'll especially need feedback, and be looking for metrics on, whether they interfere with and are interfered with by the batch compute workloads -- it's much more useful to have an interactive desktop available 2 minutes from now than 3 hours from now.</p> <p>You'll only be able to access Myriad's OnDemand servers from inside the UCL network, just like the SSH access to Myriad. If you're attempting to access these from off-campus, you'll need to use the UCL VPN, a SOCKS proxy, or some other mechanism of routing through the UCL firewalls.</p> <p>The pilot service is not currently available to all Myriad users: there is a limited access group for it. If you need to request an addition to this group, please mail rc-support@ucl.ac.uk. If you are not in the group, Microsoft's authentication systems should refuse you access, and if you attempt to access any of the internal links, you should see a page saying \"Unauthorized\" (HTTP error 401).</p>"},{"location":"Supplementary/OnDemand/#logging-in-for-the-first-time","title":"Logging in for the First Time","text":"<p>The first time you connect, Microsoft's authentication systems will ask whether you're happy to let our Open OnDemand instance use Microsoft 365 to authorise you. This is expected and normal.</p>"},{"location":"Supplementary/OnDemand/#tools","title":"Tools","text":"<p>Ideally we'd like to keep documentation light, since this is supposed to be relatively discoverable, but we'll be looking for feedback on what needs explicitly describing.</p>"},{"location":"Supplementary/OnDemand/#the-file-browser","title":"The File Browser","text":"<p>Files \u27a1 Home Directory </p> <p>This should hopefully look pretty familiar if you've used, e.g. Dropbox or OneDrive's web interfaces.</p> <p>From here, you can view the files on Myriad, and download or upload files. You can upload files either by clicking the Upload button, or by dragging them onto the window.</p> <p>Warn</p> <p>The upload and download mechanisms here have a size limit of ~10 GiB.</p> <p>Warn</p> <p>The list of files can take a second to populate: if it shows no files at first, it's probably just being slow.</p> <p>We may add additional storage entries in the menu and side bar -- for example, shared project directories -- later and as these become available.  However, these will not include storage directly attached to other clusters, for technical reasons.</p>"},{"location":"Supplementary/OnDemand/#rclone-and-ucl-onedrive","title":"rclone and UCL OneDrive","text":"<p>If you have set up UCL OneDrive as a remote using <code>rclone</code> on the cluster, your OneDrive storage will show up as a location in the File Browser. You can then use the Copy and Move tools to transfer files between the cluster and OneDrive.</p> <p>Additional remotes may show up here, but we have not yet tested these.</p>"},{"location":"Supplementary/OnDemand/#active-jobs","title":"Active Jobs","text":"<p>Jobs \u27a1 Active Jobs</p> <p>This is essentially an interface to the <code>qstat</code> command, so it shows queued and running jobs, and not completed jobs. You can also delete/cancel a queued or running job from here.</p>"},{"location":"Supplementary/OnDemand/#job-composer","title":"Job Composer","text":"<p>Jobs \u27a1 Job Composer</p> <p>This lets you select from a set of job templates to copy and edit, then submit to the queue. We'll add to the job templates over time: at the moment there are some basic examples.</p>"},{"location":"Supplementary/OnDemand/#terminal-in-the-browser","title":"Terminal in the Browser","text":"<p>Clusters \u27a1 Myriad Shell Access</p> <p>This lets you log in to one of Myriad's login nodes using SSH, giving you a terminal in the browser window.</p>"},{"location":"Supplementary/OnDemand/#interactive-desktop","title":"Interactive Desktop","text":"<p>Interactive Apps \u27a1 Myriad Desktop</p> <p>This submits a job that launches a desktop GUI on a compute node. For ease of maintenance, this is a containerised Ubuntu 24.04 LTS desktop Apptainer container, but it still has access to all the applications and the shared filesystems. It's possible having Ubuntu inside the container and RHEL outside could cause problems, so please let us know if you see problems you're not expecting.</p> <p>We may replace or supplement this with other container images (e.g. RHEL or Fedora) at a later date if we think it gives a better experience.</p> <p>Once the job has been configured and submitted, the site sends you to the My Interactive Sessions page, where you can Cancel it, or open the remote desktop viewer.</p> <p>If you close the remote desktop viewer, the job will stay running and you can reconnect later. If you attempt to log out in the remote desktop viewer, the job will end.</p> <p>Please note that the Applications menu in the interactive desktop does not currently have icons or entries for any research applications -- these still have to be run from the command line, but can still present a graphical window in the remote desktop.</p>"},{"location":"Supplementary/OnDemand/#jupyter","title":"Jupyter","text":"<p>Interactive Apps \u27a1 Jupyter Notebook</p> <p>Similarly to the Interactive Desktop, this can submit a job that launches a Jupyter Notebook session on a compute node. It then acts as a proxy to let you access that server from your browser.</p> <p>This uses a Python virtualenv with only the packages required to run Jupyter -- you may wish to install additional kernels from the command line.</p> <p>To do this:</p>"},{"location":"Supplementary/OnDemand/#python","title":"Python","text":"<ol> <li>In a shell, load the appropriate Python modules, e.g. for Python 3.11.4:</li> </ol> <pre><code>module load openssl/1.1.1u python/3.11.4\n</code></pre> <ol> <li>Create a virtual environment and activate it:</li> </ol> <p>e.g. for a virtual environment called <code>my_env</code>:</p> <pre><code>virtualenv my_env\nsource my_env/bin/activate\n</code></pre> <ol> <li>Install the packages you need as well as the <code>ipykernel</code> package:</li> </ol> <p>e.g. to install <code>numpy</code>, <code>matplotlib</code>:</p> <pre><code>pip install numpy matplotlib ipykernel\n</code></pre> <ol> <li>Install the kernel, making sure to set the <code>LD_LIBRARY_PATH</code> as follows:</li> </ol> <pre><code>python3 -m ipykernel install --user --name my_env --env LD_LIBRARY_PATH ${LD_LIBRARY_PATH}\n</code></pre> <p>You should then be able to select the kernel from the drop-down list in Jupyter.</p>"},{"location":"Supplementary/OnDemand/#r-studio","title":"R Studio","text":"<p>Interactive Apps \u27a1 R Studio</p> <p>As with the Jupyter Notebook option, this can submit a job that launches an R Studio session on a compute node. It then acts as a proxy to let you access that server from your browser.</p> <p>The installation of R used for this is quite barebones, for technical reasons: it is expected that users will install their own packages using e.g. <code>install.packages(\"dplyr\")</code>.</p> <p>To keep package installations separate, packages installed from R Studio sessions will be put in your home directory under <code>~/oodR</code> by default, instead of <code>~/R</code>. This should avoid problems with compiled C, C++, or Fortran parts of R packages.</p>"},{"location":"Supplementary/Points_of_Contact/","title":"MMM Points of Contact","text":""},{"location":"Supplementary/Points_of_Contact/#user-management-tools","title":"User management tools","text":"<p>This page contains tools and information for the nominated Points of Contact.</p> <p>Other system-specific information is at  Michael or  Young.</p> <p>These commands can all be run as  <code>michael-command</code> or <code>young-command</code>: they run the same thing and the different names are for convenience.</p>"},{"location":"Supplementary/Points_of_Contact/#displaying-user-information","title":"Displaying user information","text":"<p><code>young-show</code>, <code>michael-show</code> or <code>young-show</code> is a tool that enables you to find a lot of information about users. Access to the database is given to points of contact individually, contact rc-support@ucl.ac.uk if you try to use this and get an access denied.</p> <p>At the top level, <code>--user</code> shows all information for one user, in multiple tables. <code>--contacts</code> shows all points of contact - useful for getting the IDs, and <code>--institutes</code> is the same. <code>--allusers</code> will show everyone's basic info. <code>--getmmm</code> will show the most recently used mmm username.</p> <pre><code>young-show\u00a0-h  \nusage:\u00a0young-show\u00a0[-h]\u00a0[--user\u00a0username]\u00a0[--contacts]\u00a0[--institutes]  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[--allusers]\u00a0[--getmmm]  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0{recentusers,getusers,whois}\u00a0...\n\nShow\u00a0data\u00a0from\u00a0the\u00a0Young\u00a0database.\u00a0Use\u00a0[positional\u00a0argument\u00a0-h]\u00a0for\u00a0more  \nhelp.\n\npositional\u00a0arguments:  \n\u00a0{recentusers,getusers,whois}  \n\u00a0\u00a0\u00a0recentusers\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Show\u00a0the\u00a0n\u00a0newest\u00a0users\u00a0(5\u00a0by\u00a0default)  \n\u00a0\u00a0\u00a0getusers\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Show\u00a0all\u00a0users\u00a0with\u00a0this\u00a0project,\u00a0institute,\u00a0contact  \n\u00a0\u00a0\u00a0whois\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Search\u00a0for\u00a0users\u00a0matching\u00a0the\u00a0given\u00a0requirements\n\noptional\u00a0arguments:  \n\u00a0-h,\u00a0--help\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0show\u00a0this\u00a0help\u00a0message\u00a0and\u00a0exit  \n\u00a0--user\u00a0username\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Show\u00a0all\u00a0current\u00a0info\u00a0for\u00a0this\u00a0user  \n\u00a0--contacts\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Show\u00a0all\u00a0allowed\u00a0values\u00a0for\u00a0contact  \n\u00a0--institutes\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Show\u00a0all\u00a0allowed\u00a0values\u00a0for\u00a0institute  \n\u00a0--allusers\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Show\u00a0all\u00a0current\u00a0users  \n\u00a0--getmmm\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Show\u00a0the\u00a0highest\u00a0mmm\u00a0username\u00a0used\n</code></pre>"},{"location":"Supplementary/Points_of_Contact/#show-recent-users","title":"Show recent users","text":"<p><code>young-show recentusers</code> shows you the most recently-added N users, default 5.</p> <pre><code>young-show\u00a0recentusers\u00a0-h  \nusage:\u00a0young-show\u00a0recentusers\u00a0[-h]\u00a0[-n\u00a0N]\n\noptional\u00a0arguments:  \n\u00a0-h,\u00a0--help\u00a0\u00a0show\u00a0this\u00a0help\u00a0message\u00a0and\u00a0exit  \n\u00a0-n\u00a0N\n</code></pre>"},{"location":"Supplementary/Points_of_Contact/#show-users-with-a-given-project-institute-contact","title":"Show users with a given project, institute, contact","text":"<p><code>young-show getusers</code> will search for exact matches to the given project, institute, contact combination.</p> <pre><code>young-show\u00a0getusers\u00a0-h  \nusage:\u00a0young-show\u00a0getusers\u00a0[-h]\u00a0[-p\u00a0PROJECT]\u00a0[-i\u00a0INST_ID]\u00a0[-c\u00a0POC_ID]\n\noptional\u00a0arguments:  \n\u00a0-h,\u00a0--help\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0show\u00a0this\u00a0help\u00a0message\u00a0and\u00a0exit  \n\u00a0-p\u00a0PROJECT,\u00a0--project\u00a0PROJECT  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Project\u00a0name  \n\u00a0-i\u00a0INST_ID,\u00a0--institute\u00a0INST_ID  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Institute\u00a0ID  \n\u00a0-c\u00a0POC_ID,\u00a0--contact\u00a0POC_ID  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Point\u00a0of\u00a0Contact\u00a0ID\n</code></pre>"},{"location":"Supplementary/Points_of_Contact/#search-for-users-based-on-partial-information","title":"Search for users based on partial information","text":"<p><code>young-show whois</code> can be used to search for partial matches to username, name, email fragments, including all of those in combination.</p> <pre><code>young-show\u00a0whois\u00a0-h  \nusage:\u00a0young-show\u00a0whois\u00a0[-h]\u00a0[-u\u00a0USERNAME]\u00a0[-e\u00a0EMAIL]\u00a0[-n\u00a0GIVEN_NAME]  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[-s\u00a0SURNAME]\n\noptional\u00a0arguments:  \n\u00a0-h,\u00a0--help\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0show\u00a0this\u00a0help\u00a0message\u00a0and\u00a0exit  \n\u00a0-u\u00a0USERNAME,\u00a0--user\u00a0USERNAME  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0UCL\u00a0username\u00a0of\u00a0user\u00a0contains  \n\u00a0-e\u00a0EMAIL,\u00a0--email\u00a0EMAIL  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Email\u00a0address\u00a0of\u00a0user\u00a0contains  \n\u00a0-n\u00a0GIVEN_NAME,\u00a0--name\u00a0GIVEN_NAME  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Given\u00a0name\u00a0of\u00a0user\u00a0contains  \n\u00a0-s\u00a0SURNAME,\u00a0--surname\u00a0SURNAME  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Surname\u00a0of\u00a0user\u00a0contains\n</code></pre>"},{"location":"Supplementary/Points_of_Contact/#adding-user-information-and-new-projects","title":"Adding user information and new projects","text":"<p><code>young-add</code> will add information to the database. Access to the database is given to points of contact individually, contact rc-support@ucl.ac.uk if you try to use this and get an access denied.</p> <p>Please note that all options have a <code>--debug</code> flag that will allow you to see the query generated without committing the changes to the database - double-check that the information you are adding is correct.</p> <pre><code>young-add\u00a0-h  \nusage:\u00a0young-add\u00a0[-h]\u00a0{user,project,projectuser,poc,institute}\u00a0...\n\nAdd\u00a0data\u00a0to\u00a0the\u00a0Young\u00a0database.\u00a0Use\u00a0[positional\u00a0argument\u00a0-h]\u00a0for\u00a0more\u00a0help.\n\npositional\u00a0arguments:  \n\u00a0{user,project,projectuser,poc,institute}  \n   csv                 Add all users from the provided CSV file\n\u00a0\u00a0\u00a0user\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Adding\u00a0a\u00a0new\u00a0user\u00a0with\u00a0their\u00a0initial\u00a0project  \n\u00a0\u00a0\u00a0project\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Adding\u00a0a\u00a0new\u00a0project  \n\u00a0\u00a0\u00a0projectuser\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Adding\u00a0a\u00a0new\u00a0user-project-contact\u00a0relationship  \n\u00a0\u00a0\u00a0poc\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Adding\u00a0a\u00a0new\u00a0Point\u00a0of\u00a0Contact  \n\u00a0\u00a0\u00a0institute\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Adding\u00a0a\u00a0new\u00a0institute/consortium\n\noptional\u00a0arguments:  \n\u00a0-h,\u00a0--help\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0show\u00a0this\u00a0help\u00a0message\u00a0and\u00a0exit\n</code></pre>"},{"location":"Supplementary/Points_of_Contact/#add-a-new-user","title":"Add a new user","text":"<p><code>young-add user</code> allows you to add a new user, with their initial project and point of contact. As of 27 June 2022 this now goes  ahead and creates their account automatically within 10 minutes - first prompting you  that the information you have entered is correct. You do not need to email us separately  about creating accounts unless something has gone wrong. The user's initial project must already exist (create with <code>young-add project</code> first).</p> <p>The user will be allocated the next free <code>mmmxxxx</code> username - you should  only specify username yourself if they are an existing UCL user, or on Young if they previously had a Michael account you should give them the same username. If they already have an account on this cluster with a different institution, just add them as a projectuser instead using their  existing username.</p> <p>You can get your <code>poc_id</code> by looking at <code>young-show --contacts</code>.</p> <pre><code>young-add\u00a0user\u00a0-h  \nusage:\u00a0young-add\u00a0user\u00a0[-h]\u00a0-u\u00a0USERNAME\u00a0-n\u00a0GIVEN_NAME\u00a0[-s\u00a0SURNAME]\u00a0-e  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0EMAIL_ADDRESS\u00a0-k\u00a0\"SSH_KEY\"\u00a0-p\u00a0PROJECT_ID\u00a0-c\u00a0POC_ID  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[--debug]\n\noptional\u00a0arguments:  \n\u00a0-h,\u00a0--help\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0show\u00a0this\u00a0help\u00a0message\u00a0and\u00a0exit  \n\u00a0-u\u00a0USERNAME,\u00a0--user\u00a0USERNAME  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0UCL\u00a0username\u00a0of\u00a0user  \n\u00a0-n\u00a0GIVEN_NAME,\u00a0--name\u00a0GIVEN_NAME  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Given\u00a0name\u00a0of\u00a0user  \n\u00a0-s\u00a0SURNAME,\u00a0--surname\u00a0SURNAME  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Surname\u00a0of\u00a0user\u00a0(optional)  \n\u00a0-e\u00a0EMAIL_ADDRESS,\u00a0--email\u00a0EMAIL_ADDRESS  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Institutional\u00a0email\u00a0address\u00a0of\u00a0user  \n\u00a0-k\u00a0\"SSH_KEY\",\u00a0--key\u00a0\"SSH_KEY\"  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0User's\u00a0public\u00a0ssh\u00a0key\u00a0(quotes\u00a0necessary)  \n\u00a0-p\u00a0PROJECT_ID,\u00a0--project\u00a0PROJECT_ID  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Initial\u00a0project\u00a0the\u00a0user\u00a0belongs\u00a0to  \n\u00a0-c\u00a0POC_ID,\u00a0--contact\u00a0POC_ID  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Short\u00a0ID\u00a0of\u00a0the\u00a0user's\u00a0Point\u00a0of\u00a0Contact  \n --noconfirm           Don't ask for confirmation on user account creation\n\u00a0--verbose\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Show\u00a0SQL\u00a0queries\u00a0that\u00a0are\u00a0being\u00a0submitted  \n\u00a0--debug\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Show\u00a0SQL\u00a0query\u00a0submitted\u00a0without\u00a0committing\u00a0the\u00a0change\n</code></pre>"},{"location":"Supplementary/Points_of_Contact/#ssh-key-formats","title":"SSH key formats","text":"<p>It will verify the provided ssh key by default. Note that it has to be in the form <code>ssh-xxx keystartshere</code>. If someone has sent in a key which has line breaks and header items, make it into this format by adding the key type to the start and removing the line breaks from the key body.</p> <p>This key:</p> <pre><code>----\u00a0BEGIN\u00a0SSH2\u00a0PUBLIC\u00a0KEY\u00a0----  \nComment:\u00a0\"comment\u00a0goes\u00a0here\"  \nAAAAB3NzaC1yc2EAAAABJQAAAQEAlLhFLr/4LGC3cM1xgRZVxfQ7JgoSvnVXly0K  \n7MNufZbUSUkKtVnBXAOIjtOYe7EPndyT/SAq1s9RGZ63qsaVc/05diLrgL0E0gW+  \n9VptTmiUh7OSsXkoKQn1RiACfH7sbKi6H373bmB5/TyXNZ5C5KVmdXxO+laT8IdW  \n7JdD/gwrBra9M9vAMfcxNYVCBcPQRhJ7vOeDZ+e30qapH4R/mfEyKorYxrvQerJW  \nOeLKjOH4rSnAAOLcEqPmJhkLL8k6nQAAK3P/E1PeOaB2xD7NNPqfIsjhAJLZ+2wV  \n3eUZATx9vnmVF0YafOjvzcoK2GqUrhNAvi7k0f+ihh8twkfthj==  \n----\u00a0END\u00a0SSH2\u00a0PUBLIC\u00a0KEY\u00a0----\n</code></pre> <p>should be converted into</p> <pre><code>ssh-rsa\u00a0AAAAB3NzaC1yc2EAAAABJQAAAQEAlLhFLr/4LGC3cM1xgRZVxfQ7JgoSvnVXly0K7MNufZbUSUkKtVnBXAOIjtOYe7EPndyT/SAq1s9RGZ63qsaVc/05diLrgL0E0gW+9VptTmiUh7OSsXkoKQn1RiACfH7sbKi6H373bmB5/TyXNZ5C5KVmdXxO+laT8IdW7JdD/gwrBra9M9vAMfcxNYVCBcPQRhJ7vOeDZ+e30qapH4R/mfEyKorYxrvQerJWOeLKjOH4rSnAAOLcEqPmJhkLL8k6nQAAK3P/E1PeOaB2xD7NNPqfIsjhAJLZ+2wV3eUZATx9vnmVF0YafOjvzcoK2GqUrhNAvi7k0f+ihh8twkfthj==\n</code></pre> <p>Other types of keys (ed25519 etc) will say what they are in the first line, and you should change the <code>ssh-rsa</code> appropriately. The guide linked at Creating an ssh key in Windows also shows where users can get the second format out of PuTTY.</p>"},{"location":"Supplementary/Points_of_Contact/#add-new-users-in-bulk-from-a-csv-file","title":"Add new users in bulk from a CSV file","text":"<p><code>young-add csv</code> allows you to add users in bulk using a CSV file of specific format  and headers. As of 27 June 2022 the accounts will be all created and activated  automatically within 10 minutes.</p> <p>The CSV is comma-separated with a header line of </p> <pre><code>email,given_name,surname,username,project_ID,ssh_key\n</code></pre> <p>You can leave username empty for it to allocate them a new username, but if  they have an existing mmm username you should fill it in.  It may be useful to show users with a given institute on Young if you are migrating users from one service to another.</p> <p>You can download a CSV template here. Replace the example data.</p> <p><code>young-add csv</code> will try to automatically get your Point of Contact ID based on your username. If it can't, or if you have more than one, it will give you a  list to choose from. (All users in one CSV upload will be added using the same Point of Contact ID). </p> <p>It will prompt you for confirmation on each user account creation unless you give the <code>--noconfirm</code> option.</p> <p>The project you are adding the user to must already exist.</p> <p>The SSH key must be formatted as shown in SSH key formats.</p> <p>If you check your CSV file on the cluster with <code>cat -v</code> and it shows that it is  beginning with <code>M-oM-;M-?</code> and ending with <code>^M</code> you probably need to run  <code>dos2unix</code> on it first.</p>"},{"location":"Supplementary/Points_of_Contact/#add-a-new-project","title":"Add a new project","text":"<p><code>young-add project</code> will create a new project, associated with an institution. It will not show in Gold until it also has a user in it.</p> <p>A project ID should begin with your institute ID, followed by an underscore and a project name.</p> <pre><code>young-add\u00a0project\u00a0-h  \nusage:\u00a0young-add\u00a0project\u00a0[-h]\u00a0-p\u00a0PROJECT_ID\u00a0-i\u00a0INST_ID\u00a0[--debug]\n\noptional\u00a0arguments:  \n\u00a0-h,\u00a0--help\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0show\u00a0this\u00a0help\u00a0message\u00a0and\u00a0exit  \n\u00a0-p\u00a0PROJECT_ID,\u00a0--project\u00a0PROJECT_ID  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0A\u00a0new\u00a0unique\u00a0project\u00a0ID  \n\u00a0-i\u00a0INST_ID,\u00a0--institute\u00a0INST_ID  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Institute\u00a0ID\u00a0this\u00a0project\u00a0belongs\u00a0to  \n\u00a0--debug\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Show\u00a0SQL\u00a0query\u00a0submitted\u00a0without\u00a0committing\u00a0the\u00a0change\n</code></pre>"},{"location":"Supplementary/Points_of_Contact/#add-a-new-projectuser-pairing","title":"Add a new project/user pairing","text":"<p><code>young-add projectuser</code> will add an existing user to an existing project. Creating a new user for an existing project also creates this relationship. After a new project-user relationship is added, a cron job will pick that up within 15 minutes and create that project for that user in Gold, with no allocation.</p> <pre><code>young-add\u00a0projectuser\u00a0-h  \nusage:\u00a0young-add\u00a0projectuser\u00a0[-h]\u00a0-u\u00a0USERNAME\u00a0-p\u00a0PROJECT_ID\u00a0-c\u00a0POC_ID  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0[--debug]\n\noptional\u00a0arguments:  \n\u00a0-h,\u00a0--help\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0show\u00a0this\u00a0help\u00a0message\u00a0and\u00a0exit  \n\u00a0-u\u00a0USERNAME,\u00a0--user\u00a0USERNAME  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0An\u00a0existing\u00a0UCL\u00a0username  \n\u00a0-p\u00a0PROJECT_ID,\u00a0--project\u00a0PROJECT_ID  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0An\u00a0existing\u00a0project\u00a0ID  \n\u00a0-c\u00a0POC_ID,\u00a0--contact\u00a0POC_ID  \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0An\u00a0existing\u00a0Point\u00a0of\u00a0Contact\u00a0ID  \n\u00a0--debug\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Show\u00a0SQL\u00a0query\u00a0submitted\u00a0without\u00a0committing\u00a0the\u00a0change\n</code></pre>"},{"location":"Supplementary/Points_of_Contact/#deactivating-information","title":"Deactivating information","text":"<p>This tool is only partly functional at present. It allows you to deactivate  (not delete) some entities that may no longer exist or may have been created in error.</p>"},{"location":"Supplementary/Points_of_Contact/#deactivate-a-projectuser","title":"Deactivate a projectuser","text":"<p>Use this when the user should no longer be a member of the given project. It does not deactivate the user account, just their membership in this project. You can confirm the change by looking at <code>young-show --user</code> - it will say  'deactivated' rather than 'active' next to their listing for this project.</p> <pre><code>young-deactivate projectuser -h\nusage: young_deactivate.py projectuser [-h] -u USERNAME -p PROJECT [--debug]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -u USERNAME, --user USERNAME\n                        An existing username\n  -p PROJECT, --project PROJECT\n                        An existing project ID\n  --debug               Show SQL query submitted without committing the change\n</code></pre>"},{"location":"Supplementary/Points_of_Contact/#gold-resource-allocation","title":"Gold resource allocation","text":"<p>We are currently using Gold to manage allocations. The Michael and Young clusters use separate databases for this, so projects on one will not appear on the other.</p>"},{"location":"Supplementary/Points_of_Contact/#reporting-from-gold","title":"Reporting from Gold","text":"<p>There are wrapper scripts for a number of Gold commands (these exist in the <code>userscripts</code> module, loaded by default).</p> <p>These are all set to report in cpu-hours with the <code>-h</code> flag, as that is our main unit. If you wish to change anything about the wrappers, they live in <code>/shared/ucl/apps/cluster-scripts/</code> so you can take a copy and add your preferred options.</p> <p>They all have a <code>--man</code> option to see the man pages for that command.</p> <p>Here are some basic useful options and what they do. They can all be given more options for more specific searches.</p> <pre><code>gusage\u00a0-p\u00a0project_name\u00a0[-s\u00a0start_time]\u00a0\u00a0#\u00a0Show\u00a0the\u00a0Gold\u00a0usage\u00a0per\u00a0user\u00a0in\u00a0this\u00a0project,\u00a0in\u00a0the\u00a0given\u00a0timeframe\u00a0if\u00a0specified.  \ngbalance\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0Show\u00a0the\u00a0balance\u00a0for\u00a0every\u00a0project,\u00a0split\u00a0into\u00a0total,\u00a0reserved\u00a0and\u00a0available.  \nglsuser\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0Shows\u00a0all\u00a0the\u00a0users\u00a0in\u00a0Gold.  \nglsproject\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0Shows\u00a0all\u00a0the\u00a0projects\u00a0and\u00a0which\u00a0users\u00a0are\u00a0in\u00a0them.  \nglsres\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0Show\u00a0all\u00a0the\u00a0current\u00a0reservatioms,\u00a0inc\u00a0user\u00a0and\u00a0project.\u00a0The\u00a0Name\u00a0column\u00a0is\u00a0the\u00a0SGE\u00a0job\u00a0ID.  \ngstatement\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0Produce\u00a0a\u00a0reporting\u00a0statement\u00a0showing\u00a0beginning\u00a0and\u00a0end\u00a0balances,\u00a0credits\u00a0and\u00a0debits.\n\n#\u00a0Less\u00a0useful\u00a0commands  \nglstxn\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0Show\u00a0all\u00a0Gold\u00a0transactions.\u00a0Filter\u00a0or\u00a0it\u00a0will\u00a0take\u00a0forever\u00a0to\u00a0run.  \nglsalloc\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0#\u00a0Show\u00a0all\u00a0the\u00a0allocations.\n</code></pre> <p>These can be run by any user. The date format is YYYY-MM-DD.</p> <p>Eg. <code>gstatement -p PROJECT -s 2017-08-01</code> will show all credits and debits for the given project since the given date, saying which user and job ID each charge was associated with.</p>"},{"location":"Supplementary/Points_of_Contact/#transferring-gold","title":"Transferring Gold","text":"<p>As the point of contact, you can transfer Gold from your allocation account into other project accounts. As before, we've put <code>-h</code> in the wrapper so it is always working in cpu-hours.</p> <pre><code>gtransfer\u00a0--fromProject\u00a0xxx_allocation\u00a0--toProject\u00a0xxx_subproject\u00a0cpu_hours\n</code></pre> <p>You can also transfer in the opposite direction, from the subproject back into your allocation account.</p> <p>Note that you are able to transfer your allocation into another institute's projects, but you cannot transfer it back again - only the other institute's point of contact (or rc-support) can give it back, so be careful which project you specify.</p>"},{"location":"Supplementary/Points_of_Contact/#when-two-allocations-are-active","title":"When two allocations are active","text":"<p>There is now an overlap period of a week when two allocations can be active. By default, <code>gtransfer</code> will transfer from active allocations in the order of earliest expiring first. To transfer from the new allocation only, you need to specify the allocation id.</p> <pre><code>gtransfer\u00a0-i\u00a0allocation_ID\u00a0--fromProject\u00a0xxx_allocation\u00a0--toProject\u00a0xxx_subproject\u00a0cpu_hours\n</code></pre> <p><code>glsalloc -p xxx_allocation</code> shows you all allocations that ever existed for your institute,  and the first column is the id.</p> <pre><code>Id\u00a0\u00a0Account\u00a0Projects\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0StartTime\u00a0\u00a0EndTime\u00a0\u00a0\u00a0\u00a0Amount\u00a0\u00a0\u00a0\u00a0\u00a0Deposited\u00a0\u00a0Description\u00a0\u00a0\u00a0\u00a0  \n---\u00a0-------\u00a0---------------------\u00a0----------\u00a0----------\u00a0----------\u00a0----------\u00a0--------------\u00a0  \n87\u00a0\u00a038\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0UKCP_allocation\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02017-08-07\u00a02017-11-05\u00a0212800.00\u00a03712800.00  \n97\u00a0\u00a038\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0UKCP_allocation\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a02017-10-30\u00a02018-02-04\u00a03712800.00\u00a03712800.00\n</code></pre>"},{"location":"Supplementary/Previous_Terms_and_Conditions/","title":"Terms and Conditions","text":"<ol> <li>All use of Research Computing Platforms is subject to the UCL Computing Regulations.</li> <li>All users will be required to renew their account once per year. Users will receive a reminder one month prior to suspension of their Myriad account sent to their Live@UCL e-mail address. Funding information will need to be provided upon application, and publication information upon renewal.</li> <li>Users are forbidden from performing production runs on the login nodes.</li> <li>The Research Computing Platform Services Team reserve the right to suspend or ban without prior warning any use of the system which impairs its operation.</li> <li>With the exception of in cases where there is imminent harm or risk to the service, the Research Computing Platform Services Team will not access your files without permission.<sup>1</sup></li> <li>Official service notifications are sent to the myriad-users (or the equivalent for other services) mailing list. Users are automatically subscribed to this list using their Live@UCL e-mail address and should read notices sent there.</li> <li>The Research Computing Platform Services Team reserve the right to suspend users' accounts, without notice, in the event of a user being the subject of any UCL disciplinary procedure, or where a user is found to be in breach of UCL\u2019s Computing Regulations or best practice guidelines regarding password management, as provided by Information Services Division.</li> <li>Users are required to acknowledge their use of Myriad and associated research computing services in any publications describing research that has been conducted, in any part, on Myriad. The following words should be used:     \"The authors acknowledge the use of the UCL Myriad High Performance Computing Facility (Myriad@UCL), and associated support services, in the completion of this work\". </li> <li>All support requests should be sent by e-mail to rc-support@ucl.ac.uk.</li> </ol> <ol> <li> <p>When you submit a job script, a copy is made, and that copy is public. This may appear as an exception to this term. Please do not include passwords or API keys in your job scripts.\u00a0\u21a9</p> </li> </ol>"},{"location":"Supplementary/Troubleshooting/","title":"Troubleshooting","text":"<p>This page lists some common problems encountered by users, with methods to investigate or solve them.</p>"},{"location":"Supplementary/Troubleshooting/#why-is-my-job-in-eqw-status","title":"Why is my job in Eqw status?","text":"<p>If your job goes straight into Eqw state, there was an error in your jobscript that meant your job couldn't be started. The standard <code>qstat</code> job information command will give you a truncated version of the error:</p> <pre><code>qstat\u00a0-j\u00a0&lt;job_ID&gt;\n</code></pre> <p>To see the full error instead:</p> <pre><code>qexplain\u00a0&lt;job_ID&gt;\n</code></pre> <p>The <code>qexplain</code> script is part of our <code>userscripts</code> set -- if you try to use it and get an error that it doesn't exist, load the <code>userscripts</code> module:</p> <pre><code>module load userscripts\n</code></pre> <p>The most common reason jobs go into this error state is that  a file or directory your job is trying to use doesn't exist. Creating it after the job is in the <code>Eqw</code> state won't make the job run: it'll still have to be deleted and re-submitted.</p>"},{"location":"Supplementary/Troubleshooting/#binbash-invalid-option-error","title":"\"/bin/bash: invalid option\" error","text":"<p>This is a sign that your jobscript is a DOS-formatted text file and not a Unix one - the line break characters are different. Type <code>dos2unix &lt;yourscriptname&gt;</code> in your terminal to convert it.</p> <p>Sometimes the offending characters will be visible in the error. You can see here it's trying to parse <code>^M</code> as an option.</p>"},{"location":"Supplementary/Troubleshooting/#i-think-i-deleted-my-scratch-space-how-do-i-restore-it","title":"I think I deleted my Scratch space, how do I restore it?","text":"<p>You may have accidentally deleted or replaced the link to your Scratch space. Do an <code>ls -al</code> in your home - if set up correctly, it should look like this:</p> <pre><code>lrwxrwxrwx\u00a0\u00a0\u00a01\u00a0username\u00a0group\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a024\u00a0Apr\u00a014\u00a0\u00a02022\u00a0Scratch\u00a0-&gt;\u00a0/scratch/scratch/username\n</code></pre> <p>where <code>username</code> is your UCL user ID and <code>group</code> is your primary group. </p> <p>If this link is not present, you can recreate it with</p> <pre><code>ln\u00a0-s\u00a0/scratch/scratch/$(whoami)\u00a0Scratch\n</code></pre> <p>If you have actually deleted the files stored in your Scratch space, there is unfortunately no way to restore them.</p>"},{"location":"Supplementary/Troubleshooting/#which-mkl-library-files-should-i-use-to-build-my-application","title":"Which MKL library files should I use to build my application?","text":"<p>Depending on which whether you wish to use BLAS/LAPACK/ScaLAPACK/etc... there is a specific set of libraries that you need to pass to your compilation command line. Fortunately, Intel have released a tool that allows you to determine which libraries to link and in which order for a number of compilers and operating systems:</p> <p>http://software.intel.com/en-us/articles/intel-mkl-link-line-advisor/</p>"},{"location":"Supplementary/Troubleshooting/#unable-to-run-job-jsv-stderr-perl-warning-setting-locale-failed","title":"Unable to run job: JSV stderr: perl: warning: Setting locale failed.","text":"<p>This error is generally because your SSH client is passing LANG through as part of the SSH command, and is passing something that conflicts with what Myriad has it set to. You may be more likely to come across this with newer versions of macOS - if your client is different, have a look for an equivalent option.</p> <p>In macOS Terminal, click Settings and under International untick the box that says \"Set locale environment variables on startup\".</p> <p>Per session, you can try <code>LANG=C ssh userid@myriad.rc.ucl.ac.uk</code></p>"},{"location":"Supplementary/Troubleshooting/#what-can-i-do-to-minimise-the-time-i-need-to-wait-for-my-jobs-to-run","title":"What can I do to minimise the time I need to wait for my job(s) to run?","text":"<ol> <li>Minimise the amount of wall clock time you request.</li> <li>Use job arrays instead of submitting large numbers of jobs (see our     job script examples).</li> <li>Plan your work so that you can do other things while your jobs are     being scheduled.</li> </ol>"},{"location":"Supplementary/Troubleshooting/#what-is-my-project-code-short-string-project-id","title":"What is my project code (short string) / project ID?","text":"<p>Prior to July 2014, every user had a project code. Now all users belong to the default project \"AllUsers\" and no longer have to specify this. If you see older job script examples mentioning a project ID, you can delete that section. Only projects with access to paid or specialised resources need to give a project code in order to use those resources. If you do not know yours, contact rc-support.</p>"},{"location":"Supplementary/X-Forwarding/","title":"X-Forwarding","text":"<p>X is a system and protocol that lets remote computers push interactive windows to your local computer over a network. We use a method known as X-Forwarding, together with an SSH client, to direct the network messages for X over the same connection you use for the command-line.</p> <p>The setup steps for getting this working on Windows, Linux, and macOS are each different, and we've put them below.</p>"},{"location":"Supplementary/X-Forwarding/#windows","title":"Windows","text":"<p>Windows doesn't natively have the ability to receive X windows, so you need to install an X server separately.</p> <p>There are a few choices; UCL has a site-wide license (covering UCL-owned and personal computers) for one called Exceed, which is pretty reliable and seems to handle 3D content well, but there's also Xming, which is free and open-source if that's a concern.</p> <p>Exceed is installed on all UCL's centrally-managed Windows computers.</p>"},{"location":"Supplementary/X-Forwarding/#installing-exceed","title":"Installing Exceed","text":"<p>Exceed comes in two parts: the main package, and the add-on that handles certain types of rendered 3D content.</p> <p>You can download both parts from the Exceed page in the UCL Software Database. First, install the main package, then the 3D package.</p>"},{"location":"Supplementary/X-Forwarding/#using-putty-with-exceed","title":"Using PuTTY with Exceed","text":"<p>When you run the Exceed program, it will automatically wait in the background until you run a remote application, so you only have to configure PuTTY to pass the data through.</p> <p>To do this, first fill in the details for your PuTTY connection as normal with the hostname and any other details you'd normally use to connect, and then, in the pane labelled \"Category\" on the left, click the \"+\" next to \"SSH\" and then select \"X11\".</p> <p>The main pane should change to one labelled \"Options controlling SSH X11 forwarding\". Check the box labelled \"Enable X11 Forwarding\".</p> <p>You can now click \"Open\" to start the connection, or you can return to the \"Session\" options to save this setup. </p>"},{"location":"Supplementary/X-Forwarding/#macos","title":"macOS","text":"<p>Like Windows, macOS doesn't come with an X server to receive X windows. The most commonly used X server for macOS is XQuartz. If you download and install that, you can follow the Linux instructions below. When you connect with X-Forwarding enabled, the XQuartz server program should start automatically, ready to present remote windows.</p> <p>If you need to use remote 3D rendering (e.g. for GaussView), you will also need to enable \"Indirect rendering\", which allows rendering instructions to be sent to your computer.</p> <p>If you install XQuartz from the link above, you will need to open a terminal and run:</p> <pre><code>defaults write org.macosforge.xquartz.X11 enable_iglx -bool true\n</code></pre> <p>It is also possible to install Xorg from MacPorts -- in that case the command is instead:</p> <pre><code>defaults write org.macports.X11 enable_iglx -bool true\n</code></pre> <p>In either case, you will need to restart any running X11/Xorg/XQuartz processes.</p>"},{"location":"Supplementary/X-Forwarding/#linux","title":"Linux","text":"<p>Almost all Linux versions that have a graphical desktop use an X server to provide it, so you don't have to install a separate one.</p> <p>You still have to set up your SSH client's connection to \"tunnel\" the X windows from the remote computer, though. You can do this by simply adding the <code>-X</code> option to your <code>ssh</code> command line, so for example to connect to Myriad with X-Forwarding:</p> <pre><code>ssh -X ccaaxyz@myriad.rc.ucl.ac.uk\n</code></pre> <p>To use X-Forwarding from outside UCL, you must either use the VPN, or use the SSH Gateway system, with the appropriate flags for both <code>ssh</code> steps, for example:</p> <pre><code>[me@my_computer ~]$ ssh\u00a0-X\u00a0ccaaxyz@ssh-gateway.ucl.ac.uk\n[...]\n[ccaaxyz@ejp-gateway-01 ~]$ ssh -X ccaaxyz@myriad.rc.ucl.ac.uk\n</code></pre> <p>Note</p> <p>This assumes you use a Linux distribution that uses Xorg as its display server. If your distribution uses Wayland instead, and you aren't sure how to make this work, please contact us, letting us know what version of which distribution you're using.</p>"},{"location":"Supplementary/X-Forwarding/#checking-your-setup","title":"Checking your Setup","text":"<p>There are some simple programs on the system that use X, which can be used to check whether your setup is working correctly.</p> <ul> <li> <p><code>xterm</code> is a terminal emulator -- it presents a terminal much like you would already be using.</p> </li> <li> <p><code>glxgears</code> is a small test/benchmark program for the 3D remote rendering capability. It presents a small set of animated gears.</p> </li> </ul> <p>If these work, you have a working X connection. If not, you should see an error when you try to run them that may look something like:</p> <pre><code>xterm: Xt error: Can't open display:\n</code></pre> <p>Or:</p> <pre><code>Error: couldn't open display (null)\n</code></pre> <p>If you see these, please check you have followed all the appropriate steps above, and if you still have problems, contact rc-support@ucl.ac.uk for assistance.</p>"},{"location":"Supplementary/Young_GPU_Nodes/","title":"Young GPU Nodes","text":""},{"location":"Supplementary/Young_GPU_Nodes/#pilot-access","title":"Pilot access","text":"<p>A group of nominated pilot users had access to these nodes during the pilot, which lasted from 15 July to 5 September 2022. They are now available to all.</p>"},{"location":"Supplementary/Young_GPU_Nodes/#gpu-specs","title":"GPU specs","text":"<p>The nodes are listed in Young's node types table </p> <p>There are 6 nodes which each have 64 AMD EPYC CPU cores and 8 Nvidia A100-SXM4-40GB GPUs. They have 1T RAM and 200G local disk is available to request as <code>tmpfs</code>  unlike the rest of Young.  The AMD equivalent of hyperthreading is not enabled.</p>"},{"location":"Supplementary/Young_GPU_Nodes/#request-gpus","title":"Request GPUs","text":"<pre><code># Submit a job to the GPU nodes by adding a request for a number of GPUs per node\n#$ -l gpu=8\n\n# Only Free jobs are available at present. Use your normal projects\n#$ -P Free\n#$ -A Inst_Project\n</code></pre> <p>At the start of the pilot, jobs did not share nodes and users always had access to all  GPUs on each node. This has since been altered and device  cgroups are implemented (as of 10 Aug 2022) so jobs can share nodes on the GPU  nodes and each only have access to the number of GPUs they requested.</p> <p>For example, 8 separate single-GPU jobs from different users can be running on one node,  or 2 4-GPU jobs. Multi-node parallel GPU jobs do not share nodes, so a job asking for  two nodes and some number of GPUs per node over those two nodes will block out the entire 16 GPUs even if using fewer than that.</p>"},{"location":"Supplementary/Young_GPU_Nodes/#exclusive-use-of-nodes","title":"Exclusive use of nodes","text":"<p>If you are using fewer than 8 GPUs but want to make sure nothing else is running on the same node as you, add this to your jobscript:</p> <pre><code># Exclusive use of node: no other jobs are allowed to run on the rest of this node\n#$ -ac exclusive\n</code></pre> <p>This would generally only be done if you are benchmarking or investigating speeds and want to rule out anything else running on the rest of the node as possibly affecting your timings.</p>"},{"location":"Supplementary/Young_GPU_Nodes/#cuda-versions","title":"CUDA versions","text":"<p>The newer CUDA installs are visible in <code>module avail</code> with a reference to a newer gnu compiler in their names but can be used alongside any other compiler.</p> <pre><code>module avail cuda\n\n# pick one of the 11.x CUDA installs\nmodule load cuda/11.3.1/gnu-10.2.0\n# or\nmodule load cuda/11.2.0/gnu-10.2.0\n</code></pre>"},{"location":"Supplementary/Young_GPU_Nodes/#choosing-a-cuda-version","title":"Choosing a CUDA version","text":"<p>The drivers we have installed on the GPU nodes are version 460.27.03 which is CUDA 11.2. CUDA 11 has minor version compatibility so in most cases you can use the 11.3.1 runtime, but not all functionality is available.</p> <p>If your code builds but when running it you get an error like this:</p> <pre><code>CUDA RUNTIME API error: Free failed with error cudaErrorUnsupportedPtxVersion \n</code></pre> <p>then use the <code>cuda/11.2.0/gnu-10.2.0</code> module to build and run your program instead.</p>"},{"location":"Supplementary/Young_GPU_Nodes/#building-with-cuda","title":"Building with CUDA","text":"<p>If the code you are trying to build only needs to link to the CUDA runtime libraries, <code>libcudart.so</code> then you can build it on the login nodes which do not have GPUs.</p> <p>If it needs the full <code>libcuda.so</code> to be available, you need to build it on a GPU node. You can submit it as a job or request an interactive session with qrsh.  Eg:</p> <pre><code>qrsh -l gpu=8,h_rt=2:0:0,tmpfs=10G,mem=1G -pe smp 4 -P Free -A Inst_Project -now no\n</code></pre>"},{"location":"Supplementary/Young_GPU_Nodes/#nvidia-documentation","title":"NVIDIA documentation","text":"<p>NVIDIA has some useful information at these locations:</p> <ul> <li>Building Ampere compatible apps using CUDA 11</li> <li>CUDA Toolkit release notes</li> </ul>"},{"location":"Supplementary/Young_GPU_Nodes/#run-on-a-specific-device-or-limit-the-number-visible","title":"Run on a specific device or limit the number visible","text":"<p>If you want to tell your code to run on a specific device or devices, you can set  <code>CUDA_VISIBLE_DEVICES</code> to the ids between 0 and 7. If the code only uses one GPU  it will usually default to running on device 0, but if it is running on all GPUs that belong to your job and you don't want it to, you can limit it.</p> <pre><code># run on gpu 1\nexport CUDA_VISIBLE_DEVICES=1\n\n# run on gpus 0 and 4\nexport CUDA_VISIBLE_DEVICES=0,4\n</code></pre>"},{"location":"Supplementary/Young_GPU_Nodes/#cuda-utility-devicequery","title":"CUDA utility deviceQuery","text":"<p>CUDA has a number of small utilities that come with its examples which can be useful:  you can take a copy of the samples directory from the corresponding CUDA version -  for example <code>/shared/ucl/apps/cuda/11.3.1/gnu-10.2.0/samples/</code> and build those utilities  with their corresponding CUDA module loaded.</p> <p><code>samples/1_Utilities/deviceQuery</code> will give you a small utility that will confirm that  setting <code>CUDA_VISIBLE_DEVICES</code> is working - you can run it before and after setting it.  The devices will have been renamed as 0 and 1 in its output, but the location IDs will  be the same as when you could see all of them.</p>"},{"location":"Supplementary/Young_GPU_Nodes/#setting-ppn","title":"Setting PPN","text":"<p>You will also be able to set the number of cpu slots per node that you want.  Instead of <code>-pe smp</code> or <code>-pe mpi</code>, you would request:</p> <pre><code>-pe ppn=&lt;slots per node&gt; &lt;total slots&gt;\n</code></pre> <pre><code># this would give you 8 slots per node and 16 slots total (so is using 2 nodes)\n# along with 8 GPUs per node (16 GPUs in total).\n#$ -pe ppn=8 16\n#$ -l gpu=8\n</code></pre> <p>Like <code>-pe mpi</code> this will also create a suitable machinefile for you so MPI will know  how many cores on which nodes it can use. <code>gerun</code> (our mpirun wrapper) will use it  automatically for Intel MPI as usual and our OpenMPI modules shouldn't need it since they  have scheduler integration, but you can find it in <code>$TMPDIR/machines</code> if you are using  mpirun and need it.</p>"},{"location":"Supplementary/Young_GPU_Nodes/#request-tmpfs","title":"Request tmpfs","text":"<p>The GPU nodes do have local disks and you can request an amount of tmpfs up to the maximum  200G like this:</p> <pre><code># Request a $TMPDIR of 20G\n#$ -l tmpfs=20G\n</code></pre> <p>In the job, you refer to this using <code>$TMPDIR</code>. Many programs will use this environment variable for temporary files automatically, or you may need to tell them to do it explicitly with a command line argument.</p> <p><code>$TMPDIR</code> is deleted at the end of the job, so if you need any data that is being written to there, copy it back to your Scratch at the end of the job.</p>"},{"location":"Supplementary/Young_GPU_Nodes/#software-of-interest","title":"Software of interest","text":"<p>GPU software we have installed that may be of particular interest to users of Young.</p> <ul> <li>VASP 6 GPU</li> <li>GROMACS 2021.5 GPU</li> <li>NAMD 2.14 GPU</li> <li>LAMMPS 29 Sep 21 Update 2 GPU</li> </ul> <p>You can also use NVIDIA Grid Cloud Containers via Singularity which provide pre-configured GPU applications. Our page gives an example of using the  NAMD 3 container.</p>"},{"location":"Walkthroughs/Giving_Files/","title":"Giving Files to Another User","text":"<p>This is a walkthrough for when:</p> <ul> <li>you need to give someone a file</li> <li>they're working on the same cluster as you</li> </ul> <p>We have a tool installed called \"pipe-gifts\" for handling this, that works quickly even for large files.</p>"},{"location":"Walkthroughs/Giving_Files/#the-sender","title":"The Sender","text":"<ol> <li>Log in to one of the cluster login nodes.</li> <li><code>cd</code> to where the files you want to transfer are.</li> <li>Load the \"pipe-gifts\" module:</li> </ol> <pre><code>$ module load pipe-gifts\n</code></pre> <ol> <li>Use <code>pipe-give</code> with the name of one or more files, to set up the transfer.</li> </ol> <p>For example, here we're transferring a file called <code>some_file</code>:</p> <pre><code>$ pipe-give some_file\n\nYour transfer ID: dzbGPpE3\n    and password: 688815576\n\nPlease enter this at receiver.\n\nRemember you must be on the same node.\n\nThis node is: login01\n\n    [Ctrl-C to cancel.]\n</code></pre> <ol> <li> <p>Give the login node name (<code>login01</code> here), transfer ID, and password to the person you want to receive the files. The password only applies to this single transfer, so it's fine to send it via email or an IM system.</p> </li> <li> <p>Wait for the receiver to accept the transfer, and <code>pipe-give</code> to report that the transfer is complete.</p> </li> </ol> <p>You should see something like this:</p> <pre><code>[2020-12-02 15:06:44] pipe-give: (info) received correct password\n[2020-12-02 15:06:44] pipe-give: (info) sending identity and number of files\n[2020-12-02 15:06:44] pipe-give: (info) sending file listing to receiver for approval\n[2020-12-02 15:06:55] pipe-give: (info) confirmation received\n[2020-12-02 15:06:55] pipe-give: (info) transferring files...\nsome_file\n1.00GiB 0:00:04 [ 234MiB/s] [                        &lt;=&gt;                                                                                                                                                                                                                                ]\n[2020-12-02 15:06:59] pipe-give: (info) transfer complete\n</code></pre>"},{"location":"Walkthroughs/Giving_Files/#the-receiver","title":"The Receiver","text":"<ol> <li>Log in to the cluster.</li> <li>Get the login node name, transfer ID, and password from the person sending you the files.</li> <li>Make sure you're logged in to the same node as the sender. If you aren't, use <code>ssh</code> to connect to the right one.</li> </ol> <p>For example, here we're logged into login01:</p> <pre><code>[ccaa002@login01 ~]$\n</code></pre> <p>But our friend tells us they're on login02, so we connect to that one instead:</p> <pre><code>[ccaa002@login01 ~]$ ssh login02\nLast login: Wed Dec 20 17:47:18 2023 from 10.36.142.98\n[intro message]\n\n[ccaa002@login02 ~]$ \n</code></pre> <ol> <li>Load the \"pipe-gifts\" module:</li> </ol> <pre><code>$ module load pipe-gifts\n</code></pre> <ol> <li>(Optional) Use <code>cd</code> to change directory to where you want to receive the files to, for example:</li> </ol> <pre><code>$ cd files_from_my_friend\n</code></pre> <ol> <li>Use <code>pipe-receive</code> to set up the transfer. Enter the ID and password when requested, check the list of files that are going to be sent, and wait for the transfer to complete.</li> </ol> <p>You should see something like this:</p> <pre><code>$ pipe-receive\nPlease enter your transfer ID: dzbGPpE3\nPlease enter transfer password: 688815576\n[2020-12-02 15:06:44] pipe-receive: (info) password correct, receiving identity and file count\n[2020-12-02 15:06:44] pipe-receive: (info) receiving list of files for approval\n\n-- File List --\nsome_file\n---------------\n\nUser ccaa001 wants to copy this file to you.\nWould you like to accept? [press y for yes, anything else to cancel]\n[2020-12-02 15:06:55] pipe-receive: (info) confirming with sender\n[2020-12-02 15:06:55] pipe-receive: (info) receiving files...\nsome_file\n1.00GiB 0:00:04 [ 234MiB/s] [                        &lt;=&gt;                                                                                                                                                                                                                                ]\n[2020-12-02 15:06:59] pipe-receive: (info) transfer complete\n</code></pre>"},{"location":"Walkthroughs/Logging_In/","title":"Logging In","text":"<p>Here are walkthroughs showing all the steps to get logged in to one of our clusters from computers with different operating systems.</p> <p>These instructions assume you have already applied for an account  and received the email saying it has been created and that you can log in.</p> <p>We use Myriad as the cluster we are logging in to. The same steps also apply to Kathleen.</p> <p>To log in, you need an SSH client to be installed on the computer you are logging in from. </p>"},{"location":"Walkthroughs/Logging_In/#logging-in-from-windows","title":"Logging in from Windows","text":"<p>There are several choices of SSH client for Windows, with one now included in the Windows Command Prompt on Windows 10 or later. You can also log in via  Desktop@UCL Anywhere, which provides a Windows environment inside the UCL network.</p>"},{"location":"Walkthroughs/Logging_In/#putty","title":"PuTTY","text":"<p>PuTTY will provide you with a graphical interface to configure your SSH connection and then open a terminal window you can type into and press return to submit.</p>"},{"location":"Walkthroughs/Logging_In/#windows-command-prompt","title":"Windows Command Prompt","text":"<p>Launch the Command Prompt from the Windows Start menu. It will give you a prompt that you can type commands into. </p> <p>Replace \"uccacxx\" with your own central UCL username.</p> <pre><code>ssh uccacxx@ssh-gateway.ucl.ac.uk\n</code></pre> <p></p> <p>If your computer has never connected to the Gateway before, it has no existing record of the  host fingerprint which identifies it, so it will ask if you want to accept it  and continue.</p> <p>Type \"yes\" to accept the fingerprint. It will save it and check it next time. If the  fingerprint is different, it can be an indication that something else is pretending to be the Gateway (or that it has changed after a major update). If concerned, contact  rc-support@ucl.ac.uk or the main Service Desk.</p> <p></p> <p>It now informs you that it has added the Gateway to your list of known hosts.</p> <p>You have now contacted the Gateway and it displays a small splash screen and asks for your UCL password. Nothing will show up when typing in this box - no placeholders or bullet point characters. Press return at the end to submit.</p> <p></p> <p>If you have a typo in your password or have changed it within the last couple of hours and the new one hasn't propagated yet, it will ask again.</p> <p>Once the correct password has been entered, it will show you a longer message about the system.</p> <p>From the Gateway, we want to ssh in to Myriad:</p> <pre><code>ssh uccacxx@myriad.rc.ucl.ac.uk\n</code></pre> <p></p> <p>Your user on the Gateway will also not have connected to Myriad before, so you will get a  similar prompt about Myriad's host fingerprint. You can check this against  our current key fingerprints.</p> <p>After saying \"yes\" you will be prompted for your password again, and after typing it in you will be logged in to Myriad and see Myriad's message containing information about the system and where to get help.</p> <p></p> <p>At the bottom you can see that the prompt on Myriad looks like</p> <pre><code>[uccacxx@login13 ~]$\n</code></pre> <p>It shows you your username, which Myriad login node you are on, and where you are  (<code>~</code> is a short way to reference your home directory). You can now look at what software is available and write jobscripts.</p> <p>Helpful resources:</p> <ul> <li>Logging in with a password (UCL users)</li> <li>Logging in with a password (non-UCL users)</li> </ul> <p>After you have successfully logged in for the first time, visit the remote access for further resources on  accessing UCL services from outside the UCL firewall and creating an SSH key pair to help with logging in.</p>"},{"location":"Walkthroughs/Logging_In/#desktopucl-anywhere","title":"Desktop@UCL Anywhere","text":"<p>You can log in to this from a web browser or download a Citrix client. Once logged in, Desktop@UCL is inside the UCL network so we can log straight into a cluster with no need for a gateway machine or VPN.</p> <p>You can use PuTTY or the Windows Command Prompt on Desktop@UCL Anywhere to log in to Myriad.  It has a version of the Command Prompt called \"SSH Command Prompt\" that may set up some  additional configuration for SSH usage - either should work.</p> <p>If you want to use SSH keys for your connection, you need to use PuTTY and PuTTYgen. The OpenSSH Authentication Agent is not running as a service on Desktop@UCL Anywhere or on UCL managed laptops, so you cannot use <code>ssh-add</code> in the Windows Command Prompt.</p>"},{"location":"Walkthroughs/Machine_Learning/","title":"Machine Learning","text":"<p>Myriad is a traditional HPC system with a batch scheduler. This means that there are some additional steps to think about when running Machine Learning toolkits. Here are some videos that walk through installing and using various machine learning toolkits on Myriad.</p> <ul> <li>PyTorch</li> <li>Tensorflow</li> <li>Flux</li> </ul>"}]}